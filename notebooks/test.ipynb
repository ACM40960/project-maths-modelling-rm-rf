{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e488add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading embedding model ‚Ä¶\n",
      "‚è≥ Loading LLM ‚Ä¶ (first time may take a while)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 8 files:   0%|          | 0/8 [00:00<?, ?it/s]Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from git import Repo\n",
    "\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Step 1: Clone GitHub Repository\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if not os.path.exists(clone_path):\n",
    "        print(f\"Cloning {repo_url} ...\")\n",
    "        Repo.clone_from(repo_url, clone_path)\n",
    "        print(\"‚úÖ Repo cloned.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Repo already exists.\")\n",
    "\n",
    "\n",
    "# Step 2: Extract .py, .md, .txt files\n",
    "def load_code_files(repo_path):\n",
    "    code_files = []\n",
    "    for ext in [\".py\", \".md\", \".txt\"]:\n",
    "        code_files.extend(Path(repo_path).rglob(f\"*{ext}\"))\n",
    "    return code_files\n",
    "\n",
    "\n",
    "# Step 3: Read files and split into chunks\n",
    "def read_and_split(files, chunk_size=500, chunk_overlap=50):\n",
    "    docs = []\n",
    "    for fpath in files:\n",
    "        try:\n",
    "            with open(fpath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            doc = Document(page_content=content, metadata={\"source\": str(fpath)})\n",
    "            docs.append(doc)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to read {fpath}: {e}\")\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "# Step 4: Generate embeddings and store in FAISS\n",
    "def build_vector_db(chunks, db_path=\"faiss_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    print(f\"‚úÖ Vector DB saved at `{db_path}`.\")\n",
    "\n",
    "\n",
    "# Main Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    repo_url = \"https://github.com/YOUR_USERNAME/YOUR_REPO.git\"  # ‚¨ÖÔ∏è Replace with your repo\n",
    "    repo_path = \"tmp_repo\"\n",
    "\n",
    "    clone_repo(repo_url, repo_path)\n",
    "    files = load_code_files(repo_path)\n",
    "    print(f\"üìÅ Loaded {len(files)} code files.\")\n",
    "\n",
    "    chunks = read_and_split(files)\n",
    "    print(f\"üìÑ Split into {len(chunks)} text chunks.\")\n",
    "\n",
    "    build_vector_db(chunks, db_path=\"faiss_index\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622d95bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "query = \"What are the key skills missing in this resume?\"\n",
    "docs = vectordb.similarity_search(query, k=5)\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You're a helpful career advisor.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion:\\n{query}\"}\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09f50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. embed & index  ----------------------------------------------------------\n",
    "from git import Repo\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from glob import glob, iglob\n",
    "import os, textwrap\n",
    "\n",
    "repo_path = \"repos/requests\"\n",
    "Repo.clone_from(\"https://github.com/psf/requests\", repo_path, depth=1)\n",
    "\n",
    "chunks, metadatas = [], []\n",
    "for path in iglob(f\"{repo_path}/**/*\", recursive=True):\n",
    "    if path.endswith(('.py', '.md', '.txt', '.json', '.ppt')):\n",
    "        with open(path, 'r', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        for part in textwrap.wrap(content, 800):           # simple chunk\n",
    "            chunks.append(part)\n",
    "            metadatas.append({\"path\": path})\n",
    "\n",
    "emb = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectordb = FAISS.from_texts(chunks, emb, metadatas=metadatas)\n",
    "vectordb.save_local(\"vector_db/requests.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35963f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo already exists.\n",
      "Loaded 3 code files.\n",
      "Split into 68 text chunks.\n",
      "\n",
      "üìä Total Chunks: 68\n",
      "üìè Longest Chunk Length: 483 characters\n",
      "üìÇ Source File: tmp_repo\\StepUpAI\\app.py\n",
      "üßæ Preview:\n",
      "def generate_skill_gap(resume_text, target_role, retrieved_examples, fallback_skills):\n",
      "    examples_prompt = \"\\n\\n\".join([\n",
      "    f\"Example for role {ex['target_role']}:\\nResume: {ex['resume_summary']}\\nSkill Gaps: tech={ex['technical_skill_gap']}, soft={ex['soft_skill_gap']}, transferable={ex['transfe...\n",
      "‚úÖ Vector DB saved at `faiss_index`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from git import Repo\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "# Step 1: Clone GitHub Repository\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if not os.path.exists(clone_path):\n",
    "        print(f\"Cloning {repo_url} ...\")\n",
    "        Repo.clone_from(repo_url, clone_path)\n",
    "        print(\"Repo cloned.\")\n",
    "    else:\n",
    "        print(\"Repo already exists.\")\n",
    "\n",
    "\n",
    "# Step 2: Extract .py, .md, .txt files\n",
    "def load_code_files(repo_path):\n",
    "    code_files = []\n",
    "    for ext in [\".py\", \".md\", \".txt\", \".html\", \".js\", \".json\"]:\n",
    "        code_files.extend(Path(repo_path).rglob(f\"*{ext}\"))\n",
    "    return code_files\n",
    "\n",
    "\n",
    "# Step 3: Read files and split into chunks\n",
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    PythonCodeTextSplitter,\n",
    "    MarkdownTextSplitter,\n",
    "    HTMLHeaderTextSplitter\n",
    ")\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def get_splitter(file_path):\n",
    "    ext = Path(file_path).suffix.lower()\n",
    "\n",
    "    if ext == \".py\":\n",
    "        return PythonCodeTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    elif ext == \".md\":\n",
    "        return MarkdownTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "    elif ext in [\".html\", \".htm\"]:\n",
    "        return HTMLHeaderTextSplitter(headers_to_split_on=[(\"h1\", \"Title\"), (\"h2\", \"Section\")])\n",
    "    else:\n",
    "        # Fallback for .js, .json, .txt, etc.\n",
    "        return RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "def read_and_split(files):\n",
    "    chunks = []\n",
    "    for fpath in files:\n",
    "        try:\n",
    "            with open(fpath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            doc = Document(page_content=content, metadata={\"source\": str(fpath)})\n",
    "            splitter = get_splitter(fpath)\n",
    "            split_chunks = splitter.split_documents([doc])\n",
    "            chunks.extend(split_chunks)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {fpath}: {e}\")\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Generate embeddings and store in FAISS\n",
    "def build_vector_db(chunks, db_path=\"faiss_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    print(f\"‚úÖ Vector DB saved at `{db_path}`.\")\n",
    "\n",
    "\n",
    "# Main Pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    repo_url = \"https://github.com/adarshlearnngrow/StepUpYourCareer.AI\"  # ‚¨ÖÔ∏è Replace with your repo\n",
    "    repo_path = \"tmp_repo\"\n",
    "\n",
    "    clone_repo(repo_url, repo_path)\n",
    "    files = load_code_files(repo_path)\n",
    "    print(f\"Loaded {len(files)} code files.\")\n",
    "\n",
    "    chunks = read_and_split(files)\n",
    "    print(f\"Split into {len(chunks)} text chunks.\")\n",
    "\n",
    "    chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "    longest_idx = chunk_lengths.index(max(chunk_lengths))\n",
    "\n",
    "    print(f\"\\nüìä Total Chunks: {len(chunks)}\")\n",
    "    print(f\"üìè Longest Chunk Length: {chunk_lengths[longest_idx]} characters\")\n",
    "    print(f\"üìÇ Source File: {chunks[longest_idx].metadata['source']}\")\n",
    "    print(f\"üßæ Preview:\\n{chunks[longest_idx].page_content[:300]}...\")\n",
    "\n",
    "    build_vector_db(chunks, db_path=\"faiss_index\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "752d3c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved chunk details to `chunk_inspection.csv`.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_data = [{\n",
    "    \"source\": chunk.metadata[\"source\"],\n",
    "    \"length\": len(chunk.page_content),\n",
    "    \"content\": chunk.page_content\n",
    "} for chunk in chunks]\n",
    "\n",
    "df = pd.DataFrame(chunk_data)\n",
    "df.to_csv(\"chunk_inspection.csv\", index=False, encoding='utf-8')\n",
    "print(\"‚úÖ Saved chunk details to `chunk_inspection.csv`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be996109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Result 1 ---\n",
      "![Notes_250516_042908_4](https://github.com/user-attachments/assets/ba3b34ec-57ec-4bf5-906a-84af82342b8b)\n",
      "\n",
      "![Notes_250516_042908_3](https://github.com/user-attachments/assets/4b7e5f11-7923-427c-be95-cbcd7d919c5b)\n",
      "Source: tmp_repo\\README.md\n",
      "\n",
      "--- Result 2 ---\n",
      "# StepUpYourCareer.ai: Elevate Your Future\n",
      "\n",
      "An AI-powered career assistant that helps students and job seekers identify **skill gaps**, receive **personalized learning roadmaps**, and connect with **industry mentors**‚Äîall from a single resume upload.\n",
      "\n",
      "### Link to the website: https://stepupyourcareer.streamlit.app/\n",
      "\n",
      "---\n",
      "\n",
      "## Problem\n",
      "Source: tmp_repo\\README.md\n",
      "\n",
      "--- Result 3 ---\n",
      "- **Skill Gap Analyzer**: Extracts skills from your resume and compares them to your target role\n",
      "- **Action Plan Generator**: Recommends curated online courses and resources for each missing skill\n",
      "- **Mentor Matching**: Clusters users and mentors using K-Means to connect you with experts in your domain\n",
      "\n",
      "---\n",
      "\n",
      "![Notes_250516_042908_1](https://github.com/user-attachments/assets/a42216a8-e04c-4335-aad7-d56a61cc873b)\n",
      "Source: tmp_repo\\README.md\n",
      "\n",
      "--- Result 4 ---\n",
      "# Action Plan Generator\n",
      "Source: tmp_repo\\StepUpAI\\app.py\n",
      "\n",
      "--- Result 5 ---\n",
      "# Prepare GPT prompt only for uncovered skills\n",
      "    if tech_out or soft_out or trans_out:\n",
      "        prompt = f\"\"\"\n",
      "        You are a career coach. Only generate resources for the following skills not found in our internal library.\n",
      "\n",
      "        Provide for each:\n",
      "        - One **top-rated course** with real working URL.\n",
      "        - One **real book** just name & author and AMAZON links for buying that book.\n",
      "        - For soft/transferable skills, one article or video (with URL).\n",
      "Source: tmp_repo\\StepUpAI\\app.py\n"
     ]
    }
   ],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "# 1. Load the vector DB\n",
    "db_path = \"faiss_index\"\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectordb = FAISS.load_local(db_path, embedder, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 2. Define your query\n",
    "query = \"Give me an technical overview of the project.\"\n",
    "\n",
    "# 3. Perform similarity search\n",
    "results = vectordb.similarity_search(query, k=5)  # k is number of results\n",
    "\n",
    "# 4. Print the top results\n",
    "for i, doc in enumerate(results, 1):\n",
    "    print(f\"\\n--- Result {i} ---\")\n",
    "    print(doc.page_content[:500])  # Show only first 500 characters\n",
    "    print(f\"Source: {doc.metadata.get('source')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d63ebb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "==================== Code Chunk 1 ====================\n",
      "\n",
      "üîπ Overview:\n",
      "The Action Plan Generator is a software application designed to create action plans for various tasks or projects. Users can input specific details about the task or project, such as objectives, deadlines, and resources, and the application will generate a detailed action plan outlining the steps needed to achieve the desired outcome. This tool is intended to streamline the planning process and help users stay organized and focused on their goals.\n",
      "\n",
      "üîπ Installation Instructions:\n",
      "To use the Action Plan Generator, you will need to have Python installed on your computer. You can download Python from the official website: https://www.python.org/downloads/\n",
      "\n",
      "1. Clone the repository or download the code files to your local machine.\n",
      "2. Open a terminal or command prompt and navigate to the directory where the code files are located.\n",
      "3. Run the following command to install the required dependencies:\n",
      "   \n",
      "   ```\n",
      "   pip install pandas\n",
      "   ```\n",
      "\n",
      "4. Once the dependencies are installed, you can run the code by executing the following command in the terminal:\n",
      "\n",
      "   ```\n",
      "   python action_plan_generator.py\n",
      "   ```\n",
      "\n",
      "5. Follow the on-screen instructions to generate an action plan based on your input.\n",
      "\n",
      "That's it! You have successfully installed and can now use the Action Plan Generator.\n",
      "\n",
      "üîπ Usage Instructions:\n",
      "1. Install the necessary dependencies by running `pip install -r requirements.txt` in your terminal.\n",
      "   \n",
      "2. Run the app by executing the command `python action_plan_generator.py` in your terminal.\n",
      "\n",
      "3. Follow the instructions prompted by the app to generate an action plan based on the inputs provided.\n",
      "\n",
      "4. Input the required information when prompted, such as the goal, obstacles, resources, and timeline for the action plan.\n",
      "\n",
      "5. Once all the information has been provided, the app will generate an action plan for you to follow.\n",
      "\n",
      "6. Review the generated action plan and make any necessary adjustments before implementing it.\n",
      "\n",
      "7. Repeat the process as needed for generating multiple action plans for different goals or projects.\n",
      "\n",
      "8. Exit the app by following the on-screen instructions or by pressing `Ctrl + C` in the terminal.\n",
      "\n",
      "üîπ File/Module Description:\n",
      "This specific file or module is responsible for generating an action plan. It likely contains functions or logic that prompts the user for input related to a specific goal or objective, and then generates a plan of action based on that input. The functions in this file may involve creating steps, timelines, and specific tasks to help the user achieve their goal.\n",
      "\n",
      "üîπ Core Logic Explanation:\n",
      "This code block generates an action plan by taking a list of tasks as input and assigning each task to a team member based on their availability. It first calculates the total number of tasks and the number of team members available. Then, it divides the tasks equally among the team members by assigning each member a subset of tasks to complete. If the number of tasks is not divisible by the number of team members, the remaining tasks are evenly distributed among the team members. This ensures that each team member has a fair share of tasks to complete. Overall, the core algorithm focuses on distributing tasks as evenly as possible among available team members to create a balanced action plan.\n",
      "\n",
      "\n",
      "==================== Code Chunk 2 ====================\n",
      "\n",
      "üîπ Overview:\n",
      "The application shown in the code chunk is a system that generates and displays notes. Users can add new notes with a title and content, view existing notes, delete notes, and edit notes. Each note is displayed with a title and its content. The purpose of this application is to provide a simple and user-friendly interface for users to manage and organize their notes effectively. It allows users to quickly add, edit, and delete notes, making it a convenient tool for keeping track of important information or reminders.\n",
      "\n",
      "üîπ Installation Instructions:\n",
      "To install and run the code, follow these steps:\n",
      "\n",
      "1. Clone the repository:\n",
      "```bash\n",
      "git clone https://github.com/user-attachments/assets\n",
      "```\n",
      "\n",
      "2. Navigate to the directory with the code:\n",
      "```bash\n",
      "cd assets\n",
      "```\n",
      "\n",
      "3. The code seems to be referencing images. Make sure to have the images accessible in the specified locations (`Notes_250516_042908_4` and `Notes_250516_042908_3`).\n",
      "\n",
      "4. Run the code using a Python interpreter or any suitable code execution environment.\n",
      "\n",
      "5. Ensure any necessary Python libraries are installed based on the code's requirements.\n",
      "\n",
      "6. If there are any specific instructions within the code files, follow them accordingly.\n",
      "\n",
      "7. If you encounter any issues, refer to the code comments or documentation provided with the code.\n",
      "\n",
      "By following these steps, you should be able to install and run the code successfully.\n",
      "\n",
      "üîπ Usage Instructions:\n",
      "To use the app or module shown in the code snippet, follow these steps:\n",
      "\n",
      "1. Ensure you have the necessary programming environment set up on your machine, such as Python or a compatible IDE.\n",
      "2. Download the code files `Notes_250516_042908_4` and `Notes_250516_042908_3` from the provided GitHub links.\n",
      "3. Place the code files in the same directory for easy access.\n",
      "4. Open the code files in your preferred text editor or IDE.\n",
      "5. Review the code to understand its functionality and any specific instructions or configurations that need to be set.\n",
      "6. Run the code either through the terminal or by executing it within your IDE.\n",
      "7. Follow any prompts or input requirements specified within the code.\n",
      "8. Interact with the app or module based on its intended purpose, such as taking notes, organizing information, etc.\n",
      "9. Save or export any data or results generated by the app/module as needed.\n",
      "10. Close the app/module when finished and ensure to save any changes made to the data.\n",
      "\n",
      "Remember to refer to the code comments or documentation for additional guidance on using the app/module effectively.\n",
      "\n",
      "üîπ File/Module Description:\n",
      "The specific file or module shown in the code snippets provided appears to be related to taking notes or storing information. It contains functions or logic for creating and managing notes.\n",
      "\n",
      "Functions or logic present in the file/module may include:\n",
      "1. Creating a new note\n",
      "2. Editing an existing note\n",
      "3. Deleting a note\n",
      "4. Displaying all notes\n",
      "5. Searching for a specific note\n",
      "6. Saving notes to a file\n",
      "7. Loading notes from a file\n",
      "\n",
      "Overall, this file/module is responsible for handling the functionality related to notes, such as creating, editing, deleting, and displaying them.\n",
      "\n",
      "üîπ Core Logic Explanation:\n",
      "This code block is implementing a simple algorithm to calculate the sum of all even numbers within a given range. \n",
      "\n",
      "1. Initialize a variable `sum` to 0 to keep track of the running total of even numbers.\n",
      "2. Iterate through each number in the range specified (from `start` to `end`).\n",
      "3. Check if the current number is even by using the modulo operator `%` to divide by 2 and checking if the remainder is 0.\n",
      "4. If the number is even, add it to the `sum` variable.\n",
      "5. Finally, return the total sum of all even numbers within the specified range.\n",
      "\n",
      "This algorithm efficiently calculates the sum of even numbers within a given range by iterating through the numbers and adding only the even ones to the total sum.\n",
      "\n",
      "\n",
      "==================== Code Chunk 3 ====================\n",
      "\n",
      "üîπ Overview:\n",
      "This application is designed to help users identify skill gaps in their resume compared to their target role, generate an action plan to acquire those missing skills, and connect them with mentors in their domain. The Skill Gap Analyzer extracts skills from the user's resume and compares them to the desired role. The Action Plan Generator then recommends online courses and resources to help the user acquire the missing skills. Finally, the Mentor Matching feature clusters users and mentors using K-Means algorithm to connect users with experts who can provide guidance and support in their field. Overall, the application aims to help users bridge the gap between their current skills and the requirements of their desired role through personalized recommendations and mentorship.\n",
      "\n",
      "üîπ Installation Instructions:\n",
      "To install and run the Skill Gap Analyzer tool, follow these steps:\n",
      "\n",
      "1. Clone the repository from GitHub:\n",
      "   ```\n",
      "   git clone https://github.com/user-attachments/assets/a42216a8-e04c-4335-aad7-d56a61cc873b\n",
      "   ```\n",
      "\n",
      "2. Navigate to the project directory:\n",
      "   ```\n",
      "   cd assets/a42216a8-e04c-4335-aad7-d56a61cc873b\n",
      "   ```\n",
      "\n",
      "3. Install the required dependencies using pip:\n",
      "   ```\n",
      "   pip install -r requirements.txt\n",
      "   ```\n",
      "\n",
      "4. Run the Skill Gap Analyzer tool:\n",
      "   ```\n",
      "   python skill_gap_analyzer.py\n",
      "   ```\n",
      "\n",
      "5. Follow the on-screen instructions to input your resume and target role information.\n",
      "\n",
      "6. The tool will extract skills from your resume, compare them to your target role, generate an action plan with recommended online courses and resources, and match you with mentors in your domain.\n",
      "\n",
      "7. You are now ready to analyze your skill gap, create an action plan, and connect with mentors to improve your skills and reach your career goals.\n",
      "\n",
      "üîπ Usage Instructions:\n",
      "1. Upload your resume: Start by uploading your resume to the Skill Gap Analyzer module. This will extract the skills mentioned in your resume.\n",
      "\n",
      "2. Define your target role: Specify the job title or role you are aiming for. The module will compare the skills from your resume to the required skills for your target role.\n",
      "\n",
      "3. Review skill gaps: The module will generate a report highlighting the skills you have and the ones you are missing for your target role. This will help you understand where you need to improve.\n",
      "\n",
      "4. Explore action plan: The Action Plan Generator will recommend online courses and resources to help you bridge the skill gaps identified in the previous step. Explore these recommendations to start enhancing your skills.\n",
      "\n",
      "5. Connect with mentors: The Mentor Matching feature uses K-Means clustering to connect you with mentors who are experts in your domain. Reach out to these mentors for guidance and advice on how to progress in your career.\n",
      "\n",
      "6. Track your progress: As you work on improving your skills and connecting with mentors, track your progress using the tools provided in the app. Regularly review your skill gap analysis to see how far you've come and what areas still need improvement.\n",
      "\n",
      "7. Repeat as needed: Continuously upload updated versions of your resume, set new target roles, and follow the recommended action plans to keep improving and advancing in your career. The more you engage with the app, the more personalized and beneficial the recommendations will become.\n",
      "\n",
      "üîπ File/Module Description:\n",
      "This file or module is responsible for analyzing the skills present in a user's resume and comparing them to the skills required for a target role. It also generates an action plan by recommending online courses and resources for each missing skill. Additionally, it includes functionality for matching users with mentors by clustering them using K-Means algorithm to connect users with experts in their domain. \n",
      "\n",
      "The file may contain functions for extracting skills from a resume, comparing skills, generating action plans, and clustering users and mentors. It may also include logic for recommending online courses and resources based on the identified skill gaps.\n",
      "\n",
      "üîπ Core Logic Explanation:\n",
      "This code block describes a system with three main functions:\n",
      "1. **Skill Gap Analyzer**: This function analyzes the skills listed on a user's resume and compares them to the skills required for a target job role. It identifies any missing skills that the user needs to acquire in order to qualify for the target role.\n",
      "\n",
      "2. **Action Plan Generator**: Once the missing skills are identified, this function generates a personalized action plan for the user. It recommends specific online courses and resources that the user can use to acquire the missing skills.\n",
      "\n",
      "3. **Mentor Matching**: This function uses the K-Means clustering algorithm to group users and mentors based on their skills and expertise. It then connects users with mentors who are experts in the user's domain, helping them to acquire the necessary skills and knowledge.\n",
      "\n",
      "Overall, this system helps users to identify and bridge the gap between their current skills and the skills required for their desired job role, providing them with a roadmap for skill development and access to mentorship opportunities.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Load vector DB\n",
    "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vectordb = FAISS.load_local(\"faiss_index\", embedder, allow_dangerous_deserialization=True)\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Define multiple prompt templates for different doc sections\n",
    "sections = {\n",
    "    \"Overview\": \"\"\"\n",
    "    You are a technical writer. Given this code chunk, write a high-level overview of the entire application.\n",
    "    Focus on what the system does and its purpose.\n",
    "    \n",
    "    ### Code:\n",
    "    {code_chunk}\n",
    "    \n",
    "    ### Overview:\n",
    "    \"\"\",\n",
    "    \n",
    "    \"Installation Instructions\": \"\"\"\n",
    "    From the following code, infer any setup steps or dependencies.\n",
    "    Generate clear installation instructions for the user, like a README install section.\n",
    "    \n",
    "    ### Code:\n",
    "    {code_chunk}\n",
    "    \n",
    "    ### Installation:\n",
    "    \"\"\",\n",
    "\n",
    "    \"Usage Instructions\": \"\"\"\n",
    "    Based on the code below, write a usage guide or how-to for running the app or module.\n",
    "    \n",
    "    ### Code:\n",
    "    {code_chunk}\n",
    "    \n",
    "    ### Usage:\n",
    "    \"\"\",\n",
    "\n",
    "    \"File/Module Description\": \"\"\"\n",
    "    Explain what this specific file or module is responsible for.\n",
    "    Mention any functions or logic it contains.\n",
    "    \n",
    "    ### Code:\n",
    "    {code_chunk}\n",
    "    \n",
    "    ### File Purpose:\n",
    "    \"\"\",\n",
    "\n",
    "    \"Core Logic Explanation\": \"\"\"\n",
    "    Explain the core logic or algorithm in this code block.\n",
    "    Keep it technical, but understandable to junior developers.\n",
    "    \n",
    "    ### Code:\n",
    "    {code_chunk}\n",
    "    \n",
    "    ### Logic Explanation:\n",
    "    \"\"\",\n",
    "}\n",
    "\n",
    "# Run similarity search\n",
    "query = \"project summary\"\n",
    "top_chunks = vectordb.similarity_search(query, k=3)\n",
    "\n",
    "# Loop through chunks and generate each doc section\n",
    "for i, doc in enumerate(top_chunks):\n",
    "    print(f\"\\n\\n==================== Code Chunk {i+1} ====================\")\n",
    "    for section_title, prompt_template in sections.items():\n",
    "        prompt = prompt_template.format(code_chunk=doc.page_content)\n",
    "        response = llm.invoke(prompt)\n",
    "        print(f\"\\nüîπ {section_title}:\\n{response.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eee1d99a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ace_tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Convert to DataFrame for inspection\u001b[39;00m\n\u001b[32m     33\u001b[39m df_blocks = pd.DataFrame(code_blocks)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mace_tools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtools\u001b[39;00m; tools.display_dataframe_to_user(name=\u001b[33m\"\u001b[39m\u001b[33mAST Code Blocks\u001b[39m\u001b[33m\"\u001b[39m, dataframe=df_blocks.head(\u001b[32m10\u001b[39m))\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Return metadata for confirmation\u001b[39;00m\n\u001b[32m     37\u001b[39m {\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtotal_blocks\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(code_blocks),\n\u001b[32m     39\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33munique_files\u001b[39m\u001b[33m\"\u001b[39m: df_blocks[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m].nunique()\n\u001b[32m     40\u001b[39m }\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ace_tools'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Extract function/class-based code blocks using AST\n",
    "def extract_code_blocks_from_repo(repo_path: str) -> List[Dict]:\n",
    "    blocks = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.py\"):\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                code = f.read()\n",
    "            tree = ast.parse(code)\n",
    "            for node in tree.body:\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                    block_code = ast.get_source_segment(code, node)\n",
    "                    blocks.append({\n",
    "                        \"name\": getattr(node, 'name', 'unknown'),\n",
    "                        \"type\": type(node).__name__,\n",
    "                        \"code\": block_code,\n",
    "                        \"source\": str(filepath)\n",
    "                    })\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error reading {filepath}: {e}\")\n",
    "    return blocks\n",
    "\n",
    "# Extract blocks from the user's cloned repo\n",
    "repo_path = \"tmp_repo\"\n",
    "code_blocks = extract_code_blocks_from_repo(repo_path)\n",
    "\n",
    "# Convert to DataFrame for inspection\n",
    "df_blocks = pd.DataFrame(code_blocks)\n",
    "import ace_tools as tools; tools.display_dataframe_to_user(name=\"AST Code Blocks\", dataframe=df_blocks.head(10))\n",
    "\n",
    "# Return metadata for confirmation\n",
    "{\n",
    "    \"total_blocks\": len(code_blocks),\n",
    "    \"unique_files\": df_blocks['source'].nunique()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dab0eb6",
   "metadata": {},
   "source": [
    "# AST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9549bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS vector DB created at: faiss_ast_index\n",
      "‚úÖ Saved summary to `ast_chunk_summary.csv`.\n"
     ]
    }
   ],
   "source": [
    "# Full pipeline for AST-based chunking + FAISS + LLM summarisation\n",
    "\n",
    "import os\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if not os.path.exists(clone_path):\n",
    "        print(f\"Cloning {repo_url} ...\")\n",
    "        Repo.clone_from(repo_url, clone_path)\n",
    "        print(\"‚úÖ Repo cloned.\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Repo already exists.\")\n",
    "\n",
    "\n",
    "# Step 2: Extract .py, .md, .txt files\n",
    "def load_code_files(repo_path):\n",
    "    code_files = []\n",
    "    for ext in [\".py\", \".md\", \".txt\"]:\n",
    "        code_files.extend(Path(repo_path).rglob(f\"*{ext}\"))\n",
    "    return code_files\n",
    "\n",
    "# Step 1: Extract code blocks using AST\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.py\"):\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                code = f.read()\n",
    "            tree = ast.parse(code)\n",
    "            for node in tree.body:\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                    block_code = ast.get_source_segment(code, node)\n",
    "                    if block_code:\n",
    "                        metadata = {\n",
    "                            \"source\": str(filepath),\n",
    "                            \"type\": type(node).__name__,\n",
    "                            \"name\": getattr(node, 'name', 'unknown')\n",
    "                        }\n",
    "                        documents.append(Document(page_content=block_code, metadata=metadata))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error parsing {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "# Step 2: Embed and store chunks in FAISS\n",
    "def build_faiss_from_ast_chunks(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    print(f\"‚úÖ FAISS vector DB created at: {db_path}\")\n",
    "    return vectordb\n",
    "\n",
    "# Step 3: Define prompt for LLM doc generation\n",
    "def generate_doc_for_code_chunk(chunk: Document, model_name=\"gpt-3.5-turbo\") -> str:\n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "    prompt = f\"\"\"\n",
    "You are a senior software engineer.\n",
    "\n",
    "Please generate a clean, technical, and concise docstring or explanation for the following {chunk.metadata.get('type')} named `{chunk.metadata.get('name')}`.\n",
    "\n",
    "### Code:\n",
    "{chunk.page_content}\n",
    "\n",
    "### Documentation:\n",
    "\"\"\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "# Run the pipeline\n",
    "repo_path = \"tmp_repo\"\n",
    "ast_chunks = extract_ast_chunks(repo_path)\n",
    "vectordb = build_faiss_from_ast_chunks(ast_chunks, db_path=\"faiss_ast_index\")\n",
    "\n",
    "# Return summary for user\n",
    "chunk_summaries = [{\n",
    "    \"name\": doc.metadata.get(\"name\"),\n",
    "    \"type\": doc.metadata.get(\"type\"),\n",
    "    \"source\": doc.metadata.get(\"source\"),\n",
    "    \"length\": len(doc.page_content)\n",
    "} for doc in ast_chunks]\n",
    "\n",
    "df_summary = pd.DataFrame(chunk_summaries)\n",
    "df_summary.to_csv(\"ast_chunk_summary.csv\", index=False)\n",
    "print(\"‚úÖ Saved summary to `ast_chunk_summary.csv`.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff855f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cloning https://github.com/VESIT-CMPN-Projects/2023-24-BE26 ...\n",
      "‚úÖ Repo cloned.\n",
      "üß© Extracted 9811 chunks (18 Python, 9793 others)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 128\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    127\u001b[39m     repo_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/VESIT-CMPN-Projects/2023-24-BE26\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# üîÅ Replace as needed\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 111\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(repo_url)\u001b[39m\n\u001b[32m    108\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müß© Extracted \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(all_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m chunks (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(ast_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Python, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(other_chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m others)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;66;03m# Save to FAISS\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m vectordb = \u001b[43mbuild_faiss_from_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfaiss_ast_index\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Save chunk summary\u001b[39;00m\n\u001b[32m    114\u001b[39m chunk_summaries = [{\n\u001b[32m    115\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: doc.metadata.get(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    116\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: doc.metadata.get(\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    117\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m: doc.metadata.get(\u001b[33m\"\u001b[39m\u001b[33msource\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    118\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mlen\u001b[39m(doc.page_content)\n\u001b[32m    119\u001b[39m } \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m all_chunks]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 78\u001b[39m, in \u001b[36mbuild_faiss_from_documents\u001b[39m\u001b[34m(chunks, db_path)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild_faiss_from_documents\u001b[39m(chunks: List[Document], db_path=\u001b[33m\"\u001b[39m\u001b[33mfaiss_ast_index\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     77\u001b[39m     embedder = HuggingFaceEmbeddings(model_name=\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     vectordb = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     79\u001b[39m     vectordb.save_local(db_path)\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ FAISS vector DB created at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:848\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    846\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:150\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    142\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    143\u001b[39m \n\u001b[32m    144\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    148\u001b[39m \u001b[33;03m        List of embeddings, one for each text.\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:127\u001b[39m, in \u001b[36mHuggingFaceEmbeddings._embed\u001b[39m\u001b[34m(self, texts, encode_kwargs)\u001b[39m\n\u001b[32m    125\u001b[39m     sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    130\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore\u001b[39;49;00m\n\u001b[32m    131\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    135\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected embeddings to be a Tensor or a numpy array, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    136\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgot a list instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    137\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:685\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[39m\n\u001b[32m    682\u001b[39m features.update(extra_features)\n\u001b[32m    684\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m685\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    687\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:758\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m    756\u001b[39m     module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m    757\u001b[39m     module_kwargs = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[32m--> \u001b[39m\u001b[32m758\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    759\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:442\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns token_embeddings, cls_token\"\"\"\u001b[39;00m\n\u001b[32m    436\u001b[39m trans_features = {\n\u001b[32m    437\u001b[39m     key: value\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m    439\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33minputs_embeds\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    440\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m442\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    443\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    444\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1016\u001b[39m, in \u001b[36mBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[32m   1012\u001b[39m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[32m   1013\u001b[39m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[32m   1014\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m-> \u001b[39m\u001b[32m1016\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1019\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1028\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1029\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:662\u001b[39m, in \u001b[36mBertEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    651\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m    652\u001b[39m         layer_module.\u001b[34m__call__\u001b[39m,\n\u001b[32m    653\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    659\u001b[39m         output_attentions,\n\u001b[32m    660\u001b[39m     )\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    665\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    669\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    672\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    673\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:552\u001b[39m, in \u001b[36mBertLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    541\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    542\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    549\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m    550\u001b[39m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[32m    551\u001b[39m     self_attn_past_key_value = past_key_value[:\u001b[32m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m552\u001b[39m     self_attention_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    559\u001b[39m     attention_output = self_attention_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    561\u001b[39m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:482\u001b[39m, in \u001b[36mBertAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    472\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    473\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    474\u001b[39m     hidden_states: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    480\u001b[39m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    481\u001b[39m ) -> Tuple[torch.Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m     self_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    491\u001b[39m     attention_output = \u001b[38;5;28mself\u001b[39m.output(self_outputs[\u001b[32m0\u001b[39m], hidden_states)\n\u001b[32m    492\u001b[39m     outputs = (attention_output,) + self_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:376\u001b[39m, in \u001b[36mBertSdpaSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[39m\n\u001b[32m    374\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    375\u001b[39m     key_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28mself\u001b[39m.key(current_states))\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m     value_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_states\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    377\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention:\n\u001b[32m    378\u001b[39m         key_layer = torch.cat([past_key_value[\u001b[32m0\u001b[39m], key_layer], dim=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "from git import Repo\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Step 1: Clone the repository (removes old copy)\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if os.path.exists(clone_path):\n",
    "        print(\"üßπ Removing existing repo...\")\n",
    "        import stat\n",
    "        def handle_remove_readonly(func, path, exc):\n",
    "            os.chmod(path, stat.S_IWRITE)\n",
    "            func(path)\n",
    "        shutil.rmtree(clone_path, onerror=handle_remove_readonly)\n",
    "    print(f\"üîÑ Cloning {repo_url} ...\")\n",
    "    Repo.clone_from(repo_url, clone_path)\n",
    "    print(\"‚úÖ Repo cloned.\")\n",
    "\n",
    "# Step 2a: Extract Python function/class chunks using AST\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.py\"):\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                code = f.read()\n",
    "            tree = ast.parse(code)\n",
    "            for node in tree.body:\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                    block_code = ast.get_source_segment(code, node)\n",
    "                    if block_code:\n",
    "                        metadata = {\n",
    "                            \"source\": str(filepath),\n",
    "                            \"type\": type(node).__name__,\n",
    "                            \"name\": getattr(node, 'name', 'unknown')\n",
    "                        }\n",
    "                        documents.append(Document(page_content=block_code, metadata=metadata))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error parsing {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "# Step 2b: Load full content of documentation and config files\n",
    "def extract_other_chunks(repo_path: str) -> List[Document]:\n",
    "    extensions = [\".md\", \".txt\", \".html\", \".css\", \".js\", \".json\", \".yml\", \".yaml\", \".ini\", \".cfg\"]\n",
    "    special_files = [\"README\", \"requirements.txt\", \"setup.py\"]\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for filepath in Path(repo_path).rglob(\"*\"):\n",
    "        if filepath.is_file() and (\n",
    "            filepath.suffix in extensions or\n",
    "            filepath.name in special_files or\n",
    "            filepath.name.lower().startswith(\"readme\")\n",
    "        ):\n",
    "            try:\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                metadata = {\n",
    "                    \"source\": str(filepath),\n",
    "                    \"type\": \"File\",\n",
    "                    \"name\": filepath.name\n",
    "                }\n",
    "                documents.append(Document(page_content=content, metadata=metadata))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error reading {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "# Step 3: Embed and store in FAISS\n",
    "def build_faiss_from_documents(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    print(f\"‚úÖ FAISS vector DB created at: {db_path}\")\n",
    "    return vectordb\n",
    "\n",
    "# Step 4 (Optional): Generate summary docstring using LLM\n",
    "def generate_doc_for_code_chunk(chunk: Document, model_name=\"gpt-3.5-turbo\") -> str:\n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "    prompt = f\"\"\"\n",
    "You are a senior software engineer.\n",
    "\n",
    "Please generate a clean, technical, and concise docstring or explanation for the following {chunk.metadata.get('type')} named `{chunk.metadata.get('name')}`.\n",
    "\n",
    "### Code:\n",
    "{chunk.page_content}\n",
    "\n",
    "### Documentation:\n",
    "\"\"\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "# Step 5: Pipeline Runner\n",
    "def main(repo_url: str):\n",
    "    repo_path = \"tmp_repo\"\n",
    "\n",
    "    # Clone and extract\n",
    "    clone_repo(repo_url, repo_path)\n",
    "    ast_chunks = extract_ast_chunks(repo_path)\n",
    "    other_chunks = extract_other_chunks(repo_path)\n",
    "    all_chunks = ast_chunks + other_chunks\n",
    "\n",
    "    print(f\"üß© Extracted {len(all_chunks)} chunks ({len(ast_chunks)} Python, {len(other_chunks)} others)\")\n",
    "\n",
    "    # Save to FAISS\n",
    "    vectordb = build_faiss_from_documents(all_chunks, db_path=\"faiss_ast_index\")\n",
    "\n",
    "    # Save chunk summary\n",
    "    chunk_summaries = [{\n",
    "        \"name\": doc.metadata.get(\"name\"),\n",
    "        \"type\": doc.metadata.get(\"type\"),\n",
    "        \"source\": doc.metadata.get(\"source\"),\n",
    "        \"length\": len(doc.page_content)\n",
    "    } for doc in all_chunks]\n",
    "\n",
    "    df_summary = pd.DataFrame(chunk_summaries)\n",
    "    df_summary.to_csv(\"ast_chunk_summary.csv\", index=False)\n",
    "    print(\"üìÑ Saved summary to `ast_chunk_summary.csv`\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    repo_url = \"https://github.com/VESIT-CMPN-Projects/2023-24-BE26\"  # üîÅ Replace as needed\n",
    "    main(repo_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6105fb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cloning https://github.com/VESIT-CMPN-Projects/2023-24-BE26 ...\n",
      "‚úÖ Repo cloned.\n",
      "üß© Extracted 18 Python AST chunks, 12 text chunks\n",
      "‚úÖ FAISS vector DB created at: faiss_ast_index\n",
      "üìÑ Saved summary to `ast_chunk_summary.csv`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "\n",
    "from git import Repo\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "# --------- Filter logic ---------\n",
    "low_value_extensions = [\n",
    "    \".class\", \".o\", \".exe\", \".dll\", \".jar\", \".zip\", \".png\", \".jpg\", \".ico\", \".log\", \".tmp\", \".min.js\", \".css\"\n",
    "]\n",
    "skip_dirs = [\"node_modules\", \"venv\", \".git\", \"__pycache__\"]\n",
    "\n",
    "def is_low_value(filepath: Path):\n",
    "    if any(skip in filepath.parts for skip in skip_dirs):\n",
    "        return True\n",
    "    if filepath.suffix.lower() in low_value_extensions:\n",
    "        return True\n",
    "    try:\n",
    "        size = os.path.getsize(filepath)\n",
    "        return size < 50 or size > 1_000_000  # Skip tiny or huge files\n",
    "    except:\n",
    "        return True\n",
    "\n",
    "\n",
    "# --------- Clone GitHub repo ---------\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if os.path.exists(clone_path):\n",
    "        print(\"üßπ Removing existing repo...\")\n",
    "        shutil.rmtree(clone_path)\n",
    "    print(f\"üîÑ Cloning {repo_url} ...\")\n",
    "    Repo.clone_from(repo_url, clone_path)\n",
    "    print(\"‚úÖ Repo cloned.\")\n",
    "\n",
    "\n",
    "# --------- AST-based chunking for .py ---------\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    documents = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.py\"):\n",
    "        if is_low_value(filepath):\n",
    "            continue\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                code = f.read()\n",
    "            tree = ast.parse(code)\n",
    "            for node in tree.body:\n",
    "                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                    block_code = ast.get_source_segment(code, node)\n",
    "                    if block_code:\n",
    "                        metadata = {\n",
    "                            \"source\": str(filepath),\n",
    "                            \"type\": type(node).__name__,\n",
    "                            \"name\": getattr(node, 'name', 'unknown')\n",
    "                        }\n",
    "                        documents.append(Document(page_content=block_code, metadata=metadata))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error parsing {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# --------- Text file chunking (.md, .txt, .html, .css, .js) ---------\n",
    "def extract_text_chunks(repo_path: str) -> List[Document]:\n",
    "    text_exts = [\".md\", \".txt\", \".html\", \".js\"]\n",
    "    documents = []\n",
    "    for ext in text_exts:\n",
    "        for filepath in Path(repo_path).rglob(f\"*{ext}\"):\n",
    "            if is_low_value(filepath):\n",
    "                continue\n",
    "            try:\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                metadata = {\n",
    "                    \"source\": str(filepath),\n",
    "                    \"type\": ext,\n",
    "                    \"name\": filepath.name\n",
    "                }\n",
    "                documents.append(Document(page_content=content, metadata=metadata))\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error reading {filepath}: {e}\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "# --------- Build FAISS vector store ---------\n",
    "def build_faiss_from_chunks(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    print(f\"‚úÖ FAISS vector DB created at: {db_path}\")\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "# --------- (Optional) Summarise a code chunk ---------\n",
    "def generate_doc_for_code_chunk(chunk: Document, model_name=\"gpt-3.5-turbo\") -> str:\n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "    prompt = f\"\"\"\n",
    "You are a senior software engineer.\n",
    "\n",
    "Please generate a clean, technical, and concise docstring or explanation for the following {chunk.metadata.get('type')} named `{chunk.metadata.get('name')}`.\n",
    "\n",
    "### Code:\n",
    "{chunk.page_content}\n",
    "\n",
    "### Documentation:\n",
    "\"\"\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "\n",
    "# --------- Main pipeline ---------\n",
    "def main(repo_url: str):\n",
    "    repo_path = \"tmp_repo\"\n",
    "    clone_repo(repo_url, repo_path)\n",
    "\n",
    "    # Collect AST and text chunks\n",
    "    ast_chunks = extract_ast_chunks(repo_path)\n",
    "    text_chunks = extract_text_chunks(repo_path)\n",
    "    all_chunks = ast_chunks + text_chunks\n",
    "\n",
    "    print(f\"üß© Extracted {len(ast_chunks)} Python AST chunks, {len(text_chunks)} text chunks\")\n",
    "\n",
    "    # Save FAISS\n",
    "    vectordb = build_faiss_from_chunks(all_chunks)\n",
    "\n",
    "    # Save metadata summary\n",
    "    chunk_summaries = [{\n",
    "        \"name\": doc.metadata.get(\"name\"),\n",
    "        \"type\": doc.metadata.get(\"type\"),\n",
    "        \"source\": doc.metadata.get(\"source\"),\n",
    "        \"length\": len(doc.page_content)\n",
    "    } for doc in all_chunks]\n",
    "\n",
    "    df_summary = pd.DataFrame(chunk_summaries)\n",
    "    df_summary.to_csv(\"ast_chunk_summary.csv\", index=False)\n",
    "    print(\"üìÑ Saved summary to `ast_chunk_summary.csv`\")\n",
    "\n",
    "\n",
    "# --------- Run ---------\n",
    "if __name__ == \"__main__\":\n",
    "    repo_url = \"https://github.com/VESIT-CMPN-Projects/2023-24-BE26\"  # üîÅ Change this\n",
    "    main(repo_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fe584b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cloning https://github.com/VESIT-CMPN-Projects/2023-24-BE26 ...\n",
      "‚úÖ Repo cloned.\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git: [Errno 13] Permission denied: 'tmp_repo\\\\.git'\n",
      "‚ö†Ô∏è Skipping tmp_repo\\26_Big Data Analytics and Machine Learning_Indu Dokare.docx: 'utf-8' codec can't decode byte 0x90 in position 11: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\26_Big Data Analytics and Machine Learning_Indu Dokare.pdf: 'utf-8' codec can't decode byte 0xa7 in position 10: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\BE Group 26 Video.zip: 'utf-8' codec can't decode byte 0xeb in position 10: invalid continuation byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\.ipynb_checkpoints: [Errno 13] Permission denied: 'tmp_repo\\\\backend\\\\.ipynb_checkpoints'\n",
      "‚ö†Ô∏è Skipping tmp_repo\\node_modules\\.bin: [Errno 13] Permission denied: 'tmp_repo\\\\node_modules\\\\.bin'\n",
      "‚ö†Ô∏è Skipping tmp_repo\\node_modules\\.cache: [Errno 13] Permission denied: 'tmp_repo\\\\node_modules\\\\.cache'\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\favicon.ico: 'utf-8' codec can't decode byte 0xe3 in position 14: invalid continuation byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\logo192.png: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\logo512.png: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\assets\\2.png: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\assets\\3.png: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\assets\\Aditi_bg.png: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\assets\\image_renewable.jpg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\assets\\image_renewable0.jpg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\assets\\image_renewable2.jpg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\assets\\image_renewable21.jpg: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\assets\\image_renewable22.jpg: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\assets\\image_renewable3.jpg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\public\\assets\\user.png: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\node_modules\\iconv-lite\\.idea: [Errno 13] Permission denied: 'tmp_repo\\\\node_modules\\\\iconv-lite\\\\.idea'\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\100PLSTM.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\100_011PLSTM.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\100_2PLSTM.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\bengaluru_mlp_model.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\delhi_mlp_model.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\hyderabad_mlp_model.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\jaipur_mlp_model.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\kanpur_mlp_model.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\linear_regression_model_new.pkl: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\mumbai_mlp_model.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\nagpur_mlp_model.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\mlp_models\\pune_mlp_model.h5: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\assets\\img\\avatars\\avatar1.jpeg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\assets\\img\\avatars\\avatar2.jpeg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\assets\\img\\avatars\\avatar3.jpeg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\assets\\img\\avatars\\avatar4.jpeg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\assets\\img\\avatars\\avatar5.jpeg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\assets\\img\\dogs\\image1.jpeg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\assets\\img\\dogs\\image2.jpeg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\static\\assets\\img\\dogs\\image3.jpeg: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\backend\\__pycache__\\settings.cpython-310.pyc: 'utf-8' codec can't decode byte 0xeb in position 10: invalid continuation byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\backend\\__pycache__\\urls.cpython-310.pyc: 'utf-8' codec can't decode byte 0xaa in position 9: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\backend\\__pycache__\\wsgi.cpython-310.pyc: 'utf-8' codec can't decode byte 0xa8 in position 9: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\backend\\__pycache__\\__init__.cpython-310.pyc: 'utf-8' codec can't decode byte 0xa8 in position 9: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\api\\__pycache__\\admin.cpython-310.pyc: 'utf-8' codec can't decode byte 0xa9 in position 9: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\api\\__pycache__\\apps.cpython-310.pyc: 'utf-8' codec can't decode byte 0xa9 in position 9: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\api\\__pycache__\\models.cpython-310.pyc: 'utf-8' codec can't decode byte 0xa9 in position 9: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\api\\__pycache__\\urls.cpython-310.pyc: 'utf-8' codec can't decode byte 0xb0 in position 8: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\api\\__pycache__\\views.cpython-310.pyc: 'utf-8' codec can't decode byte 0xfd in position 10: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\api\\__pycache__\\__init__.cpython-310.pyc: 'utf-8' codec can't decode byte 0xa9 in position 9: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\backend\\api\\migrations\\__pycache__\\__init__.cpython-310.pyc: 'utf-8' codec can't decode byte 0xa9 in position 9: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git\\objects\\pack\\pack-c9df0e33ab8e11e06a6a30e4de55dc95bbaa94dc.idx: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git\\objects\\pack\\pack-c9df0e33ab8e11e06a6a30e4de55dc95bbaa94dc.pack: 'utf-8' codec can't decode byte 0x91 in position 12: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git\\objects\\pack\\pack-c9df0e33ab8e11e06a6a30e4de55dc95bbaa94dc.rev: 'utf-8' codec can't decode byte 0xed in position 15: invalid continuation byte\n",
      "üß© Extracted 163 chunks\n",
      "üìÑ Saved summary to `ast_chunk_summary.csv`\n",
      "‚úÖ FAISS DB saved to `faiss_ast_index`\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 124\u001b[39m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    123\u001b[39m     repo_url = \u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/VESIT-CMPN-Projects/2023-24-BE26\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_url\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(repo_url)\u001b[39m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìÑ Saved summary to `ast_chunk_summary.csv`\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    118\u001b[39m build_faiss_from_ast_chunks(ast_chunks, db_path=\u001b[33m\"\u001b[39m\u001b[33mfaiss_ast_index\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m \u001b[43mgenerate_technical_doc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mast_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtechnical_documentation.docx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mgenerate_technical_doc\u001b[39m\u001b[34m(chunks, output_path)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks:\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.metadata.get(\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m'\u001b[39m\u001b[33mFunctionDef\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mClassDef\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m.py\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m         explanation = \u001b[43mgenerate_doc_for_code_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m         doc.add_heading(chunk.metadata.get(\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mUnnamed\u001b[39m\u001b[33m\"\u001b[39m), level=\u001b[32m2\u001b[39m)\n\u001b[32m     95\u001b[39m         doc.add_paragraph(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìÑ Source: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk.metadata.get(\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 73\u001b[39m, in \u001b[36mgenerate_doc_for_code_chunk\u001b[39m\u001b[34m(chunk, model_name)\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_doc_for_code_chunk\u001b[39m(chunk: Document, model_name=\u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     llm = \u001b[43mChatOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     prompt = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     75\u001b[39m \u001b[33mYou are a senior software engineer.\u001b[39m\n\u001b[32m     76\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33m### Documentation:\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m llm.invoke(prompt).content\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:222\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.finalize.<locals>.warn_if_direct_instance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    220\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    221\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[39m, in \u001b[36mSerializable.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\pydantic\\_internal\\_decorators_v1.py:148\u001b[39m, in \u001b[36mmake_v1_generic_root_validator.<locals>._wrapper1\u001b[39m\u001b[34m(values, _)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapper1\u001b[39m(values: RootValidatorValues, _: core_schema.ValidationInfo) -> RootValidatorValues:\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvalidator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_core\\utils\\pydantic.py:168\u001b[39m, in \u001b[36mpre_init.<locals>.wrapper\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m    165\u001b[39m             values[name] = field_info.default\n\u001b[32m    167\u001b[39m \u001b[38;5;66;03m# Call the decorated function\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_community\\chat_models\\openai.py:357\u001b[39m, in \u001b[36mChatOpenAI.validate_environment\u001b[39m\u001b[34m(cls, values)\u001b[39m\n\u001b[32m    355\u001b[39m         values[\u001b[33m\"\u001b[39m\u001b[33mclient\u001b[39m\u001b[33m\"\u001b[39m] = openai.OpenAI(**client_params).chat.completions\n\u001b[32m    356\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values.get(\u001b[33m\"\u001b[39m\u001b[33masync_client\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m         values[\u001b[33m\"\u001b[39m\u001b[33masync_client\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m.chat.completions\n\u001b[32m    360\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m values.get(\u001b[33m\"\u001b[39m\u001b[33mclient\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    361\u001b[39m     values[\u001b[33m\"\u001b[39m\u001b[33mclient\u001b[39m\u001b[33m\"\u001b[39m] = openai.ChatCompletion\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\openai\\_client.py:473\u001b[39m, in \u001b[36mAsyncOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    471\u001b[39m     base_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m473\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[38;5;28mself\u001b[39m._default_stream_cls = AsyncStream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\openai\\_base_client.py:1399\u001b[39m, in \u001b[36mAsyncAPIClient.__init__\u001b[39m\u001b[34m(self, version, base_url, _strict_response_validation, max_retries, timeout, http_client, custom_headers, custom_query)\u001b[39m\n\u001b[32m   1385\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1386\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid `http_client` argument; Expected an instance of `httpx.AsyncClient` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1389\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m   1390\u001b[39m     version=version,\n\u001b[32m   1391\u001b[39m     base_url=base_url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1397\u001b[39m     _strict_response_validation=_strict_response_validation,\n\u001b[32m   1398\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1399\u001b[39m \u001b[38;5;28mself\u001b[39m._client = http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mAsyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1400\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1401\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[32m   1402\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\openai\\_base_client.py:1306\u001b[39m, in \u001b[36m_DefaultAsyncHttpxClient.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1304\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mlimits\u001b[39m\u001b[33m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[32m   1305\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mfollow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1306\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\httpx\\_client.py:1402\u001b[39m, in \u001b[36mAsyncClient.__init__\u001b[39m\u001b[34m(self, auth, params, headers, cookies, verify, cert, http1, http2, proxy, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, trust_env, default_encoding)\u001b[39m\n\u001b[32m   1399\u001b[39m allow_env_proxies = trust_env \u001b[38;5;129;01mand\u001b[39;00m transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1400\u001b[39m proxy_map = \u001b[38;5;28mself\u001b[39m._get_proxy_map(proxy, allow_env_proxies)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28mself\u001b[39m._transport = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_transport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[38;5;28mself\u001b[39m._mounts: \u001b[38;5;28mdict\u001b[39m[URLPattern, AsyncBaseTransport | \u001b[38;5;28;01mNone\u001b[39;00m] = {\n\u001b[32m   1413\u001b[39m     URLPattern(key): \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, proxy \u001b[38;5;129;01min\u001b[39;00m proxy_map.items()\n\u001b[32m   1425\u001b[39m }\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mounts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\httpx\\_client.py:1445\u001b[39m, in \u001b[36mAsyncClient._init_transport\u001b[39m\u001b[34m(self, verify, cert, trust_env, http1, http2, limits, transport)\u001b[39m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transport\n\u001b[32m-> \u001b[39m\u001b[32m1445\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAsyncHTTPTransport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\httpx\\_transports\\default.py:297\u001b[39m, in \u001b[36mAsyncHTTPTransport.__init__\u001b[39m\u001b[34m(self, verify, cert, trust_env, http1, http2, limits, proxy, uds, local_address, retries, socket_options)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    296\u001b[39m proxy = Proxy(url=proxy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxy, (\u001b[38;5;28mstr\u001b[39m, URL)) \u001b[38;5;28;01melse\u001b[39;00m proxy\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m ssl_context = \u001b[43mcreate_ssl_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m._pool = httpcore.AsyncConnectionPool(\n\u001b[32m    301\u001b[39m         ssl_context=ssl_context,\n\u001b[32m    302\u001b[39m         max_connections=limits.max_connections,\n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m         socket_options=socket_options,\n\u001b[32m    311\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\httpx\\_config.py:40\u001b[39m, in \u001b[36mcreate_ssl_context\u001b[39m\u001b[34m(verify, cert, trust_env)\u001b[39m\n\u001b[32m     37\u001b[39m         ctx = ssl.create_default_context(capath=os.environ[\u001b[33m\"\u001b[39m\u001b[33mSSL_CERT_DIR\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;66;03m# Default case...\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         ctx = \u001b[43mssl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_default_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcertifi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m verify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m     42\u001b[39m     ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:717\u001b[39m, in \u001b[36mcreate_default_context\u001b[39m\u001b[34m(purpose, cafile, capath, cadata)\u001b[39m\n\u001b[32m    713\u001b[39m context.verify_flags |= (_ssl.VERIFY_X509_PARTIAL_CHAIN |\n\u001b[32m    714\u001b[39m                          _ssl.VERIFY_X509_STRICT)\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cafile \u001b[38;5;129;01mor\u001b[39;00m capath \u001b[38;5;129;01mor\u001b[39;00m cadata:\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m context.verify_mode != CERT_NONE:\n\u001b[32m    719\u001b[39m     \u001b[38;5;66;03m# no explicit cafile, capath or cadata but the verify mode is\u001b[39;00m\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# CERT_OPTIONAL or CERT_REQUIRED. Let's try to load default system\u001b[39;00m\n\u001b[32m    721\u001b[39m     \u001b[38;5;66;03m# root CA certificates for the given purpose. This may fail silently.\u001b[39;00m\n\u001b[32m    722\u001b[39m     context.load_default_certs(purpose)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from git import Repo\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# --- Utility: Determine low-value files ---\n",
    "def is_low_value_file(filepath):\n",
    "    low_value_exts = ['.css', '.min.js', '.json', '.svg']\n",
    "    filename = os.path.basename(filepath).lower()\n",
    "    return any(filename.endswith(ext) for ext in low_value_exts) or 'mock' in filename\n",
    "\n",
    "# --- Step 1: Clone the repo ---\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if os.path.exists(clone_path):\n",
    "        print(\"üßπ Removing existing repo...\")\n",
    "        shutil.rmtree(clone_path, ignore_errors=True)\n",
    "    print(f\"üîÑ Cloning {repo_url} ...\")\n",
    "    Repo.clone_from(repo_url, clone_path)\n",
    "    print(\"‚úÖ Repo cloned.\")\n",
    "\n",
    "# --- Step 2: Extract chunks using AST or direct code ---\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    chunks = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.*\"):\n",
    "        if is_low_value_file(filepath):\n",
    "            continue\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                code = f.read()\n",
    "            if filepath.suffix == \".py\":\n",
    "                tree = ast.parse(code)\n",
    "                for node in tree.body:\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        content = ast.get_source_segment(code, node)\n",
    "                        if content and 50 < len(content) < 5000:\n",
    "                            meta = {\n",
    "                                \"source\": str(filepath),\n",
    "                                \"type\": type(node).__name__,\n",
    "                                \"name\": node.name\n",
    "                            }\n",
    "                            chunks.append(Document(page_content=content, metadata=meta))\n",
    "            else:\n",
    "                if 50 < len(code) < 5000:\n",
    "                    meta = {\n",
    "                        \"source\": str(filepath),\n",
    "                        \"type\": filepath.suffix,\n",
    "                        \"name\": os.path.basename(filepath)\n",
    "                    }\n",
    "                    chunks.append(Document(page_content=code, metadata=meta))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping {filepath}: {e}\")\n",
    "    return chunks\n",
    "\n",
    "# --- Step 3: Create FAISS vector DB ---\n",
    "def build_faiss_from_ast_chunks(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    print(f\"‚úÖ FAISS DB saved to `{db_path}`\")\n",
    "    return vectordb\n",
    "\n",
    "# --- Step 4: Generate documentation using LLM ---\n",
    "def generate_doc_for_code_chunk(chunk: Document, model_name=\"gpt-3.5-turbo\") -> str:\n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "    prompt = f\"\"\"\n",
    "You are a senior software engineer.\n",
    "\n",
    "Generate a clean, concise, and technical docstring or explanation for the following {chunk.metadata.get('type')} named `{chunk.metadata.get('name')}`.\n",
    "\n",
    "### Code:\n",
    "{chunk.page_content}\n",
    "\n",
    "### Documentation:\n",
    "\"\"\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "# --- Step 5: Generate DOCX with summaries ---\n",
    "def generate_technical_doc(chunks: List[Document], output_path=\"technical_documentation.docx\"):\n",
    "    doc = DocxDocument()\n",
    "    doc.add_heading(\"üìò Technical Documentation Summary\", 0)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if chunk.metadata.get(\"type\") in ['FunctionDef', 'ClassDef', '.py']:\n",
    "            explanation = generate_doc_for_code_chunk(chunk)\n",
    "            doc.add_heading(chunk.metadata.get(\"name\", \"Unnamed\"), level=2)\n",
    "            doc.add_paragraph(f\"üìÑ Source: {chunk.metadata.get('source')}\")\n",
    "            doc.add_paragraph(explanation)\n",
    "            doc.add_paragraph(\"‚Äî\" * 30)\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"‚úÖ Saved technical documentation to `{output_path}`\")\n",
    "\n",
    "# --- Step 6: Main runner ---\n",
    "def main(repo_url: str):\n",
    "    repo_path = \"tmp_repo\"\n",
    "    clone_repo(repo_url, repo_path)\n",
    "    ast_chunks = extract_ast_chunks(repo_path)\n",
    "    print(f\"üß© Extracted {len(ast_chunks)} chunks\")\n",
    "\n",
    "    df_summary = pd.DataFrame([{\n",
    "        \"name\": d.metadata.get(\"name\"),\n",
    "        \"type\": d.metadata.get(\"type\"),\n",
    "        \"source\": d.metadata.get(\"source\"),\n",
    "        \"length\": len(d.page_content)\n",
    "    } for d in ast_chunks])\n",
    "    df_summary.to_csv(\"ast_chunk_summary.csv\", index=False)\n",
    "    print(\"üìÑ Saved summary to `ast_chunk_summary.csv`\")\n",
    "\n",
    "    build_faiss_from_ast_chunks(ast_chunks, db_path=\"faiss_ast_index\")\n",
    "    generate_technical_doc(ast_chunks, output_path=\"technical_documentation.docx\")\n",
    "\n",
    "# üîÅ Replace URL as needed\n",
    "if __name__ == \"__main__\":\n",
    "    repo_url = \"https://github.com/VESIT-CMPN-Projects/2023-24-BE26\"\n",
    "    main(repo_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d360ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cloning https://github.com/adarshlearnngrow/StepUpYourCareer.AI ...\n",
      "‚úÖ Repo cloned.\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.devcontainer: [Errno 13] Permission denied: 'tmp_repo\\\\.devcontainer'\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git: [Errno 13] Permission denied: 'tmp_repo\\\\.git'\n",
      "‚ö†Ô∏è Skipping tmp_repo\\Presentation - StepUpYourCareer.ai Elevate Your Future.pdf: 'utf-8' codec can't decode byte 0xf6 in position 10: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\StepUpAI\\models\\fitted_vectorizer.pkl: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\StepUpAI\\models\\mentor_clustering_model.pkl: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git\\objects\\pack\\pack-a5b318ad826ebceb70d739baaee5679d9a54860a.idx: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git\\objects\\pack\\pack-a5b318ad826ebceb70d739baaee5679d9a54860a.pack: 'utf-8' codec can't decode byte 0x86 in position 11: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git\\objects\\pack\\pack-a5b318ad826ebceb70d739baaee5679d9a54860a.rev: 'utf-8' codec can't decode byte 0x84 in position 27: invalid start byte\n",
      "üß© Extracted 30 chunks\n",
      "üìÑ Saved summary to `ast_chunk_summary.csv`\n",
      "‚úÖ FAISS DB saved to `faiss_ast_index`\n",
      "‚úÖ Saved technical documentation to `technical_documentation_2.docx`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from git import Repo\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# --- Utility: Determine low-value files ---\n",
    "def is_low_value_file(filepath):\n",
    "    low_value_exts = ['.css', '.min.js', '.json', '.svg']\n",
    "    filename = os.path.basename(filepath).lower()\n",
    "    return any(filename.endswith(ext) for ext in low_value_exts) or 'mock' in filename\n",
    "\n",
    "# --- Step 1: Clone the repo ---\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if os.path.exists(clone_path):\n",
    "        print(\"üßπ Removing existing repo...\")\n",
    "        shutil.rmtree(clone_path, ignore_errors=True)\n",
    "    print(f\"üîÑ Cloning {repo_url} ...\")\n",
    "    Repo.clone_from(repo_url, clone_path)\n",
    "    print(\"‚úÖ Repo cloned.\")\n",
    "\n",
    "# --- Step 2: Extract chunks using AST or direct code ---\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    chunks = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.*\"):\n",
    "        if is_low_value_file(filepath):\n",
    "            continue\n",
    "        try:\n",
    "            with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                code = f.read()\n",
    "            if filepath.suffix == \".py\":\n",
    "                tree = ast.parse(code)\n",
    "                for node in tree.body:\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        content = ast.get_source_segment(code, node)\n",
    "                        if content and 50 < len(content) < 5000:\n",
    "                            meta = {\n",
    "                                \"source\": str(filepath),\n",
    "                                \"type\": type(node).__name__,\n",
    "                                \"name\": node.name\n",
    "                            }\n",
    "                            chunks.append(Document(page_content=content, metadata=meta))\n",
    "            else:\n",
    "                if 50 < len(code) < 5000:\n",
    "                    meta = {\n",
    "                        \"source\": str(filepath),\n",
    "                        \"type\": filepath.suffix,\n",
    "                        \"name\": os.path.basename(filepath)\n",
    "                    }\n",
    "                    chunks.append(Document(page_content=code, metadata=meta))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping {filepath}: {e}\")\n",
    "    return chunks\n",
    "\n",
    "# --- Step 3: Create FAISS vector DB ---\n",
    "def build_faiss_from_ast_chunks(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    print(f\"‚úÖ FAISS DB saved to `{db_path}`\")\n",
    "    return vectordb\n",
    "\n",
    "# --- Step 4: Generate documentation using LLM ---\n",
    "def generate_doc_for_code_chunk(chunk: Document, model_name=\"gpt-3.5-turbo\") -> str:\n",
    "    llm = ChatOpenAI(model=model_name)\n",
    "    prompt = f\"\"\"\n",
    "You are a senior software engineer.\n",
    "\n",
    "Generate a clean, concise, and business overview for the following {chunk.metadata.get('type')} named `{chunk.metadata.get('name')}` on what it is doing.\n",
    "\n",
    "### Code:\n",
    "{chunk.page_content}\n",
    "\n",
    "### Documentation:\n",
    "\"\"\"\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "# --- Step 5: Generate DOCX with summaries ---\n",
    "def generate_technical_doc(chunks: List[Document], output_path=\"technical_documentation.docx\"):\n",
    "    doc = DocxDocument()\n",
    "    doc.add_heading(\"üìò Technical Documentation Summary\", 0)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if chunk.metadata.get(\"type\") in ['FunctionDef', 'ClassDef', '.py']:\n",
    "            explanation = generate_doc_for_code_chunk(chunk)\n",
    "            doc.add_heading(chunk.metadata.get(\"name\", \"Unnamed\"), level=2)\n",
    "            doc.add_paragraph(f\"üìÑ Source: {chunk.metadata.get('source')}\")\n",
    "            doc.add_paragraph(explanation)\n",
    "            doc.add_paragraph(\"‚Äî\" * 30)\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"‚úÖ Saved technical documentation to `{output_path}`\")\n",
    "    \n",
    "\n",
    "# --- Step 6: Main runner ---\n",
    "def main(repo_url: str):\n",
    "    repo_path = \"tmp_repo\"\n",
    "    clone_repo(repo_url, repo_path)\n",
    "    ast_chunks = extract_ast_chunks(repo_path)\n",
    "    print(f\"üß© Extracted {len(ast_chunks)} chunks\")\n",
    "\n",
    "    df_summary = pd.DataFrame([{\n",
    "        \"name\": d.metadata.get(\"name\"),\n",
    "        \"type\": d.metadata.get(\"type\"),\n",
    "        \"source\": d.metadata.get(\"source\"),\n",
    "        \"length\": len(d.page_content)\n",
    "    } for d in ast_chunks])\n",
    "    df_summary.to_csv(\"ast_chunk_summary.csv\", index=False)\n",
    "    print(\"üìÑ Saved summary to `ast_chunk_summary.csv`\")\n",
    "\n",
    "    build_faiss_from_ast_chunks(ast_chunks, db_path=\"faiss_ast_index\")\n",
    "    generate_technical_doc(ast_chunks, output_path=\"technical_documentation_2.docx\")\n",
    "\n",
    "# üîÅ Replace URL as needed\n",
    "if __name__ == \"__main__\":\n",
    "    repo_url = \"https://github.com/adarshlearnngrow/StepUpYourCareer.AI\"\n",
    "    main(repo_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f40bea10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Cloning https://github.com/adarshlearnngrow/StepUpYourCareer.AI ...\n",
      "‚úÖ Repo cloned.\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.devcontainer: [Errno 13] Permission denied: 'tmp_repo\\\\.devcontainer'\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git: [Errno 13] Permission denied: 'tmp_repo\\\\.git'\n",
      "‚ö†Ô∏è Skipping tmp_repo\\Presentation - StepUpYourCareer.ai Elevate Your Future.pdf: 'utf-8' codec can't decode byte 0xf6 in position 10: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\StepUpAI\\models\\fitted_vectorizer.pkl: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\StepUpAI\\models\\mentor_clustering_model.pkl: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git\\objects\\pack\\pack-a5b318ad826ebceb70d739baaee5679d9a54860a.idx: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git\\objects\\pack\\pack-a5b318ad826ebceb70d739baaee5679d9a54860a.pack: 'utf-8' codec can't decode byte 0x86 in position 11: invalid start byte\n",
      "‚ö†Ô∏è Skipping tmp_repo\\.git\\objects\\pack\\pack-a5b318ad826ebceb70d739baaee5679d9a54860a.rev: 'utf-8' codec can't decode byte 0x84 in position 27: invalid start byte\n",
      "üß© Extracted 29 chunks\n",
      "üìÑ Saved summary to `ast_chunk_summary.csv`\n",
      "‚úÖ FAISS DB saved to `faiss_ast_index`\n",
      "‚úÖ Saved technical documentation to `technical_documentation_2.docx`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from git import Repo, GitCommandError\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import Tool, initialize_agent, AgentType\n",
    "\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# --- Utility: Determine low-value files ---\n",
    "def is_low_value_file(filepath):\n",
    "    low_value_exts = ['.css', '.min.js', '.json', '.svg', '.csv', '.xlsx', '.xls']\n",
    "    filename = os.path.basename(filepath).lower()\n",
    "    return any(filename.endswith(ext) for ext in low_value_exts) or 'mock' in filename\n",
    "\n",
    "# --- Step 1: Clone the repo ---\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if os.path.exists(clone_path):\n",
    "        print(\"üßπ Removing existing repo...\")\n",
    "        shutil.rmtree(clone_path, ignore_errors=True)\n",
    "    print(f\"üîÑ Cloning {repo_url} ...\")\n",
    "    Repo.clone_from(repo_url, clone_path)\n",
    "    print(\"‚úÖ Repo cloned.\")\n",
    "\n",
    "# --- Step 2: Extract chunks using AST or direct code ---\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    chunks = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.*\"):\n",
    "        if is_low_value_file(filepath):\n",
    "            continue\n",
    "        try:\n",
    "            code = filepath.read_text(encoding=\"utf-8\")\n",
    "            if filepath.suffix == \".py\":\n",
    "                tree = ast.parse(code)\n",
    "                for node in tree.body:\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        content = ast.get_source_segment(code, node)\n",
    "                        if content and 50 < len(content) < 5000:\n",
    "                            meta = {\"source\": str(filepath), \"type\": type(node).__name__, \"name\": node.name}\n",
    "                            chunks.append(Document(page_content=content, metadata=meta))\n",
    "            else:\n",
    "                if 50 < len(code) < 5000:\n",
    "                    meta = {\"source\": str(filepath), \"type\": filepath.suffix, \"name\": os.path.basename(filepath)}\n",
    "                    chunks.append(Document(page_content=code, metadata=meta))\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skipping {filepath}: {e}\")\n",
    "    return chunks\n",
    "\n",
    "# --- Step 3: Create FAISS vector DB ---\n",
    "def build_faiss_from_ast_chunks(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    print(f\"‚úÖ FAISS DB saved to `{db_path}`\")\n",
    "    return vectordb\n",
    "\n",
    "# --- Setup LLM and Tool for LangChain Agent ---\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "def _summarize_code(code: str) -> str:\n",
    "    prompt = (\n",
    "        \"You are a senior software engineer. Generate concise, business-focused documentation for the following code snippet:\\n\\n\"\n",
    "        f\"{code}\\n---\\nDocumentation:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "summarization_tool = Tool(\n",
    "    name=\"summarize_code\",\n",
    "    func=_summarize_code,\n",
    "    description=\"Generates documentation for a code snippet\"\n",
    ")\n",
    "\n",
    "agent = initialize_agent(\n",
    "    tools=[summarization_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# --- Step 4: Generate documentation using LangChain Agent ---\n",
    "def generate_doc_for_code_chunk(chunk: Document) -> str:\n",
    "    # Directly call the summarization tool function to avoid parsing errors\n",
    "    return summarization_tool.func(chunk.page_content)\n",
    "\n",
    "# --- Step 5: Generate DOCX with summaries ---\n",
    "def generate_technical_doc(chunks: List[Document], output_path=\"technical_documentation.docx\"):\n",
    "    doc = DocxDocument()\n",
    "    doc.add_heading(\"üìò Technical Documentation Summary\", 0)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        if chunk.metadata.get(\"type\") in ['FunctionDef', 'ClassDef']:\n",
    "            summary = generate_doc_for_code_chunk(chunk)\n",
    "            doc.add_heading(chunk.metadata.get(\"name\", \"Unnamed\"), level=2)\n",
    "            doc.add_paragraph(f\"üìÑ Source: {chunk.metadata.get('source')}\")\n",
    "            doc.add_paragraph(summary)\n",
    "            doc.add_paragraph(\"‚Äî\" * 30)\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"‚úÖ Saved technical documentation to `{output_path}`\")\n",
    "\n",
    "# --- Step 6: Main runner ---\n",
    "def main(repo_url: str):\n",
    "    repo_path = \"tmp_repo\"\n",
    "    clone_repo(repo_url, repo_path)\n",
    "    chunks = extract_ast_chunks(repo_path)\n",
    "    print(f\"üß© Extracted {len(chunks)} chunks\")\n",
    "\n",
    "    import pandas as pd\n",
    "    df_summary = pd.DataFrame([{\n",
    "        \"name\": d.metadata.get(\"name\"),\n",
    "        \"type\": d.metadata.get(\"type\"),\n",
    "        \"source\": d.metadata.get(\"source\"),\n",
    "        \"length\": len(d.page_content)\n",
    "    } for d in chunks])\n",
    "    df_summary.to_csv(\"ast_chunk_summary.csv\", index=False)\n",
    "    print(\"üìÑ Saved summary to `ast_chunk_summary.csv`\")\n",
    "\n",
    "    build_faiss_from_ast_chunks(chunks, db_path=\"faiss_ast_index\")\n",
    "    generate_technical_doc(chunks, output_path=\"technical_documentation_2.docx\")\n",
    "\n",
    "# üîÅ Replace URL as needed\n",
    "if __name__ == \"__main__\":\n",
    "    repo_url = \"https://github.com/adarshlearnngrow/StepUpYourCareer.AI\"\n",
    "    main(repo_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9088cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved technical documentation to `technical_documentation.docx`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from git import Repo, GitCommandError\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# --- Utility: Determine low-value files ---\n",
    "def is_low_value_file(filepath):\n",
    "    low_value_exts = ['.css', '.min.js', '.json', '.svg', '.csv', '.xlsx', '.xls']\n",
    "    filename = os.path.basename(filepath).lower()\n",
    "    return any(filename.endswith(ext) for ext in low_value_exts) or 'mock' in filename\n",
    "\n",
    "# --- Step 1: Clone the repo (latest commit only) ---\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if os.path.exists(clone_path):\n",
    "        shutil.rmtree(clone_path, ignore_errors=True)\n",
    "    Repo.clone_from(repo_url, clone_path, depth=1)\n",
    "\n",
    "# --- Step 2: Extract code chunks via AST ---\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    chunks = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.*\"):\n",
    "        if is_low_value_file(filepath):\n",
    "            continue\n",
    "        try:\n",
    "            code = filepath.read_text(encoding=\"utf-8\")\n",
    "            if filepath.suffix == \".py\":\n",
    "                tree = ast.parse(code)\n",
    "                for node in tree.body:\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        content = ast.get_source_segment(code, node)\n",
    "                        if content and 50 < len(content) < 5000:\n",
    "                            meta = {\"source\": str(filepath), \"type\": type(node).__name__, \"name\": node.name}\n",
    "                            chunks.append(Document(page_content=content, metadata=meta))\n",
    "            else:\n",
    "                if 50 < len(code) < 5000:\n",
    "                    meta = {\"source\": str(filepath), \"type\": filepath.suffix, \"name\": os.path.basename(filepath)}\n",
    "                    chunks.append(Document(page_content=code, metadata=meta))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return chunks\n",
    "\n",
    "# --- Step 3: Build FAISS vector store ---\n",
    "def build_faiss_from_ast_chunks(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    return vectordb\n",
    "\n",
    "# --- LLM client ---\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# --- Section functions ---\n",
    "def generate_business_overview(repo_path: str) -> str:\n",
    "    files = [p.relative_to(repo_path).as_posix() for p in Path(repo_path).rglob(\"*\") if p.is_file()]\n",
    "    context = \"\\n\".join(files)\n",
    "    prompt = (\n",
    "        \"You are a technical writer for enterprise software. Provide a high-level business overview of this codebase, \"\n",
    "        \"including purpose, scope, and value delivered.\\n\"\n",
    "        f\"Files in project:\\n{context}\\n---\\nOverview:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def generate_technical_specifications(repo_path: str) -> str:\n",
    "    ext_map = {'.py':'Python','.js':'JavaScript','.ts':'TypeScript','.java':'Java', '.html':'HTML','.css':'CSS'}\n",
    "    techs = set()\n",
    "    for p in Path(repo_path).rglob('*'):\n",
    "        if p.suffix in ext_map:\n",
    "            techs.add(ext_map[p.suffix])\n",
    "        if p.name.lower() == 'dockerfile':\n",
    "            techs.add('Docker')\n",
    "    tech_list = ', '.join(sorted(techs))\n",
    "    prompt = (\n",
    "        \"You are a senior software engineer. List the technologies and frameworks used in this project, \"\n",
    "        \"with a brief rationale for each.\\n\"\n",
    "        f\"Technologies: {tech_list}\\n---\\nSpecifications:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def generate_folder_structure(repo_path: str) -> str:\n",
    "    lines = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        level = root.replace(repo_path, '').count(os.sep)\n",
    "        indent = '  ' * level\n",
    "        lines.append(f\"{indent}{os.path.basename(root)}/\")\n",
    "        for f in files:\n",
    "            lines.append(f\"{indent}  {f}\")\n",
    "    tree = \"\\n\".join(lines)\n",
    "    prompt = (\n",
    "        \"You are a software architect. Describe the folder structure and modular organization of this project.\\n\"\n",
    "        f\"Directory tree:\\n{tree}\\n---\\nDescription:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def generate_code_flow(chunks: List[Document]) -> str:\n",
    "    entries = [f\"{d.metadata['type']} {d.metadata['name']}\" for d in chunks]\n",
    "    context = '\\n'.join(entries)\n",
    "    prompt = (\n",
    "        \"You are a senior software engineer. Provide a step-by-step execution flow of this codebase, \"\n",
    "        \"referencing functions and classes.\\n\"\n",
    "        f\"Components:\\n{context}\\n---\\nFlow:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def summarize_chunk(chunk: Document) -> str:\n",
    "    prompt = (\n",
    "        f\"You are a senior software engineer. Generate concise documentation for the {chunk.metadata['type']} '{chunk.metadata['name']}'.\\n\"\n",
    "        f\"Code:\\n{chunk.page_content}\\n---\\nDocumentation:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "# --- Step 4: Generate multi-section DOCX ---\n",
    "def generate_technical_doc(chunks: List[Document], repo_path: str, output_path=\"technical_documentation.docx\"):\n",
    "    doc = DocxDocument()\n",
    "\n",
    "    # Business Overview\n",
    "    doc.add_heading(\"Business Overview\", level=1)\n",
    "    doc.add_paragraph(generate_business_overview(repo_path))\n",
    "\n",
    "    # Technical Specifications\n",
    "    doc.add_heading(\"Technical Specifications\", level=1)\n",
    "    doc.add_paragraph(generate_technical_specifications(repo_path))\n",
    "\n",
    "    # Folder Structure\n",
    "    doc.add_heading(\"Folder Structure\", level=1)\n",
    "    doc.add_paragraph(generate_folder_structure(repo_path))\n",
    "\n",
    "    # Code Flow\n",
    "    doc.add_heading(\"Code Flow\", level=1)\n",
    "    doc.add_paragraph(generate_code_flow(chunks))\n",
    "\n",
    "    # Detailed Function/Class Documentation\n",
    "    doc.add_heading(\"Detailed Documentation\", level=1)\n",
    "    for chunk in chunks:\n",
    "        if chunk.metadata.get(\"type\") in ['FunctionDef', 'ClassDef']:\n",
    "            doc.add_heading(chunk.metadata.get(\"name\", \"Unnamed\"), level=2)\n",
    "            doc.add_paragraph(f\"Source: {chunk.metadata.get('source')}\")\n",
    "            doc.add_paragraph(summarize_chunk(chunk))\n",
    "            doc.add_paragraph(\"‚Äî\" * 30)\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"‚úÖ Saved technical documentation to `{output_path}`\")\n",
    "\n",
    "# --- Step 5: Main runner ---\n",
    "def main(repo_url: str):\n",
    "    repo_path = \"tmp_repo\"\n",
    "    clone_repo(repo_url, repo_path)\n",
    "    chunks = extract_ast_chunks(repo_path)\n",
    "    pd.DataFrame([{\n",
    "        \"name\": d.metadata.get(\"name\"),\n",
    "        \"type\": d.metadata.get(\"type\"),\n",
    "        \"source\": d.metadata.get(\"source\"),\n",
    "        \"length\": len(d.page_content)\n",
    "    } for d in chunks]).to_csv(\"ast_chunk_summary.csv\", index=False)\n",
    "    build_faiss_from_ast_chunks(chunks)\n",
    "    generate_technical_doc(chunks, repo_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"https://github.com/adarshlearnngrow/StepUpYourCareer.AI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ac7ac1",
   "metadata": {},
   "source": [
    "# Working code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e6276",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624c2bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a306660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusteringMentorModelTraining.ipynb\n",
      "Job_Description_JD_Manupulation.ipynb\n",
      "Presentation - StepUpYourCareer.ai Elevate Your Future.pdf\n",
      "README.md\n",
      "Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb\n",
      ".devcontainer/devcontainer.json\n",
      ".git/config\n",
      ".git/description\n",
      ".git/HEAD\n",
      ".git/index\n",
      ".git/packed-refs\n",
      ".git/shallow\n",
      "data/all_roles_student_resumes.json\n",
      "data/req_job_desc.csv\n",
      "data/role_skills.csv\n",
      "data/role_skills.json\n",
      "data/sim_resume.json\n",
      "data/skill_gap_analysis.csv\n",
      "data/skill_gap_analysis.json\n",
      "StepUpAI/app.py\n",
      "StepUpAI/generated_mentors.json\n",
      "StepUpAI/mentors_final_data.json\n",
      "StepUpAI/requirements.txt\n",
      "StepUpAI/role_skills.json\n",
      "StepUpAI/skill_gap_analysis.json\n",
      "StepUpAI/skill_resource_mapping.json\n",
      "StepUpAI/models/fitted_vectorizer.pkl\n",
      "StepUpAI/models/mentor_clustering_model.pkl\n",
      ".git/hooks/applypatch-msg.sample\n",
      ".git/hooks/commit-msg.sample\n",
      ".git/hooks/fsmonitor-watchman.sample\n",
      ".git/hooks/post-update.sample\n",
      ".git/hooks/pre-applypatch.sample\n",
      ".git/hooks/pre-commit.sample\n",
      ".git/hooks/pre-merge-commit.sample\n",
      ".git/hooks/pre-push.sample\n",
      ".git/hooks/pre-rebase.sample\n",
      ".git/hooks/pre-receive.sample\n",
      ".git/hooks/prepare-commit-msg.sample\n",
      ".git/hooks/push-to-checkout.sample\n",
      ".git/hooks/sendemail-validate.sample\n",
      ".git/hooks/update.sample\n",
      ".git/info/exclude\n",
      ".git/logs/HEAD\n",
      ".git/refs/heads/main\n",
      ".git/refs/remotes/origin/HEAD\n",
      ".git/objects/pack/pack-77834c024e709f430795a541c7d59da466020bfc.idx\n",
      ".git/objects/pack/pack-77834c024e709f430795a541c7d59da466020bfc.pack\n",
      ".git/objects/pack/pack-77834c024e709f430795a541c7d59da466020bfc.rev\n",
      ".git/logs/refs/heads/main\n",
      ".git/logs/refs/remotes/origin/HEAD\n",
      "‚úÖ Saved technical documentation to `technical_documentation.docx`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from git import Repo, GitCommandError\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# --- Utility: Determine low-value files ---\n",
    "def is_low_value_file(filepath):\n",
    "    low_value_exts = ['.css', '.min.js', '.json', '.svg', '.csv', '.xlsx', '.xls']\n",
    "    filename = os.path.basename(filepath).lower()\n",
    "    return any(filename.endswith(ext) for ext in low_value_exts) or 'mock' in filename\n",
    "\n",
    "# --- Step 1: Clone the repo (latest commit only) ---\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if os.path.exists(clone_path):\n",
    "        shutil.rmtree(clone_path, ignore_errors=True)\n",
    "    Repo.clone_from(repo_url, clone_path, depth=1)\n",
    "\n",
    "# --- Step 2: Extract code chunks via AST ---\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    chunks = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.*\"):\n",
    "        if is_low_value_file(filepath):\n",
    "            continue\n",
    "        try:\n",
    "            code = filepath.read_text(encoding=\"utf-8\")\n",
    "            if filepath.suffix == \".py\":\n",
    "                tree = ast.parse(code)\n",
    "                for node in tree.body:\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        content = ast.get_source_segment(code, node)\n",
    "                        if content and 50 < len(content) < 5000:\n",
    "                            meta = {\"source\": str(filepath), \"type\": type(node).__name__, \"name\": node.name}\n",
    "                            chunks.append(Document(page_content=content, metadata=meta))\n",
    "            else:\n",
    "                if 50 < len(code) < 5000:\n",
    "                    meta = {\"source\": str(filepath), \"type\": filepath.suffix, \"name\": os.path.basename(filepath)}\n",
    "                    chunks.append(Document(page_content=code, metadata=meta))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return chunks\n",
    "\n",
    "# --- Step 3: Build FAISS vector store ---\n",
    "def build_faiss_from_ast_chunks(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    return vectordb\n",
    "\n",
    "# --- LLM client ---\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# --- Section functions ---\n",
    "def generate_business_overview(repo_path: str) -> str:\n",
    "    files = [p.relative_to(repo_path).as_posix() for p in Path(repo_path).rglob(\"*\") if p.is_file()]\n",
    "    context = \"\\n\".join(files)\n",
    "    print(context)\n",
    "    prompt = (\n",
    "        \"You are a technical writer for enterprise software. Provide a high-level business overview of this solution, \"\n",
    "        \"including purpose, scope, and value delivered.\\n\"\n",
    "        f\"Files in project:\\n{context}\\n---\\nOverview:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def generate_technical_specifications(repo_path: str) -> str:\n",
    "    # Deterministically list technologies used without LLM to avoid hallucination\n",
    "    ext_map = {'.py':'Python','.js':'JavaScript','.ts':'TypeScript','.java':'Java', '.html':'HTML','.css':'CSS', '.go':'Go', '.rs':'Rust'}\n",
    "    techs = set()\n",
    "    for p in Path(repo_path).rglob('*'):\n",
    "        if p.suffix in ext_map:\n",
    "            techs.add(ext_map[p.suffix])\n",
    "        if p.name.lower() == 'dockerfile':\n",
    "            techs.add('Docker')\n",
    "        if p.name.lower() in ('requirements.txt', 'pyproject.toml'):\n",
    "            techs.add('Python (dependencies)')\n",
    "        if p.name.lower() == 'package.json':\n",
    "            techs.add('Node.js (npm)')\n",
    "    # Format as bullet list\n",
    "    lines = [f\"- {tech}\" for tech in sorted(techs)]\n",
    "    return \"\".join(lines)\n",
    "\n",
    "\n",
    "def generate_folder_structure(repo_path: str) -> str:\n",
    "    lines = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        level = root.replace(repo_path, '').count(os.sep)\n",
    "        indent = '  ' * level\n",
    "        lines.append(f\"{indent}{os.path.basename(root)}/\")\n",
    "        for f in files:\n",
    "            lines.append(f\"{indent}  {f}\")\n",
    "    tree = \"\\n\".join(lines)\n",
    "    prompt = (\n",
    "        \"You are a software architect. Describe the folder structure and modular organization of this project.\\n\"\n",
    "        f\"Directory tree:\\n{tree}\\n---\\nDescription:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def generate_code_flow(chunks: List[Document]) -> str:\n",
    "    entries = [f\"{d.metadata['type']} {d.metadata['name']}\" for d in chunks]\n",
    "    context = '\\n'.join(entries)\n",
    "    prompt = (\n",
    "        \"You are a senior software engineer. Provide a step-by-step execution flow of this codebase, \"\n",
    "        \"referencing functions and classes.\\n\"\n",
    "        f\"Components:\\n{context}\\n---\\nFlow:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def summarize_chunk(chunk: Document) -> str:\n",
    "    prompt = (\n",
    "        f\"You are a senior software engineer. Generate concise documentation for the {chunk.metadata['type']} '{chunk.metadata['name']}'.\\n\"\n",
    "        f\"Code:\\n{chunk.page_content}\\n---\\nDocumentation:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "# --- Step 4: Generate multi-section DOCX ---\n",
    "def generate_technical_doc(chunks: List[Document], repo_path: str, output_path=\"technical_documentation.docx\"):\n",
    "    doc = DocxDocument()\n",
    "\n",
    "    # Business Overview\n",
    "    doc.add_heading(\"Business Overview\", level=1)\n",
    "    doc.add_paragraph(generate_business_overview(repo_path))\n",
    "    '''\n",
    "    # Technical Specifications\n",
    "    doc.add_heading(\"Technical Specifications\", level=1)\n",
    "    doc.add_paragraph(generate_technical_specifications(repo_path))\n",
    "\n",
    "    # Folder Structure\n",
    "    doc.add_heading(\"Folder Structure\", level=1)\n",
    "    doc.add_paragraph(generate_folder_structure(repo_path))\n",
    "\n",
    "    # Code Flow\n",
    "    doc.add_heading(\"Code Flow\", level=1)\n",
    "    doc.add_paragraph(generate_code_flow(chunks))\n",
    "\n",
    "    # Detailed Function/Class Documentation\n",
    "    doc.add_heading(\"Detailed Documentation\", level=1)\n",
    "    for chunk in chunks:\n",
    "        if chunk.metadata.get(\"type\") in ['FunctionDef', 'ClassDef']:\n",
    "            doc.add_heading(chunk.metadata.get(\"name\", \"Unnamed\"), level=2)\n",
    "            doc.add_paragraph(f\"Source: {chunk.metadata.get('source')}\")\n",
    "            doc.add_paragraph(summarize_chunk(chunk))\n",
    "            doc.add_paragraph(\"‚Äî\" * 30)\n",
    "    '''\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"‚úÖ Saved technical documentation to `{output_path}`\")\n",
    "\n",
    "# --- Step 5: Main runner ---\n",
    "def main(repo_url: str):\n",
    "    repo_path = \"tmp_repo\"\n",
    "    #clone_repo(repo_url, repo_path)\n",
    "    chunks = extract_ast_chunks(repo_path)\n",
    "    pd.DataFrame([{\n",
    "        \"name\": d.metadata.get(\"name\"),\n",
    "        \"type\": d.metadata.get(\"type\"),\n",
    "        \"source\": d.metadata.get(\"source\"),\n",
    "        \"length\": len(d.page_content)\n",
    "    } for d in chunks]).to_csv(\"ast_chunk_summary.csv\", index=False)\n",
    "    build_faiss_from_ast_chunks(chunks)\n",
    "    generate_technical_doc(chunks, repo_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"https://github.com/adarshlearnngrow/StepUpYourCareer.AI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281b164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_doc_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
