{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ac7ac1",
   "metadata": {},
   "source": [
    "# Working code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e6276",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a306660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved technical documentation to `Documents\\technical_documentation.docx`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from git import Repo, GitCommandError\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# --- Utility: Determine low-value files ---\n",
    "def is_low_value_file(filepath):\n",
    "    low_value_exts = ['.css', '.min.js', '.json', '.svg', '.csv', '.xlsx', '.xls']\n",
    "    filename = os.path.basename(filepath).lower()\n",
    "    return any(filename.endswith(ext) for ext in low_value_exts) or 'mock' in filename\n",
    "\n",
    "# --- Step 1: Clone the repo (latest commit only) ---\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if os.path.exists(clone_path):\n",
    "        shutil.rmtree(clone_path, ignore_errors=True)\n",
    "    Repo.clone_from(repo_url, clone_path, depth=1)\n",
    "\n",
    "# --- Step 2: Extract code chunks via AST ---\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    chunks = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.*\"):\n",
    "        if is_low_value_file(filepath):\n",
    "            continue\n",
    "        try:\n",
    "            code = filepath.read_text(encoding=\"utf-8\")\n",
    "            if filepath.suffix == \".py\":\n",
    "                tree = ast.parse(code)\n",
    "                for node in tree.body:\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        content = ast.get_source_segment(code, node)\n",
    "                        if content and 50 < len(content) < 5000:\n",
    "                            meta = {\"source\": str(filepath), \"type\": type(node).__name__, \"name\": node.name}\n",
    "                            chunks.append(Document(page_content=content, metadata=meta))\n",
    "            else:\n",
    "                if 50 < len(code) < 5000:\n",
    "                    meta = {\"source\": str(filepath), \"type\": filepath.suffix, \"name\": os.path.basename(filepath)}\n",
    "                    chunks.append(Document(page_content=code, metadata=meta))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return chunks\n",
    "\n",
    "# --- Step 3: Build FAISS vector store ---\n",
    "def build_faiss_from_ast_chunks(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    return vectordb\n",
    "\n",
    "# --- LLM client ---\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=1.5)\n",
    "\n",
    "# --- Section functions ---\n",
    "def generate_business_overview(repo_path: str) -> str:\n",
    "    files = [p.relative_to(repo_path).as_posix() for p in Path(repo_path).rglob(\"*\") if p.is_file()]\n",
    "    context = \"\\n\".join(files)\n",
    "    #print(context)\n",
    "    prompt = (\n",
    "        \"You are a technical writer for enterprise software. Provide a high-level business overview of this solution, \"\n",
    "        \"including purpose, scope, and value delivered.\\n\"\n",
    "        f\"Files in project:\\n{context}\\n---\\nOverview:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def generate_technical_specifications(repo_path: str) -> str:\n",
    "    # Deterministically list technologies used without LLM to avoid hallucination\n",
    "    ext_map = {'.py':'Python','.js':'JavaScript','.ts':'TypeScript','.java':'Java', '.html':'HTML','.css':'CSS', '.go':'Go', '.rs':'Rust'}\n",
    "    techs = set()\n",
    "    for p in Path(repo_path).rglob('*'):\n",
    "        if p.suffix in ext_map:\n",
    "            techs.add(ext_map[p.suffix])\n",
    "        if p.name.lower() == 'dockerfile':\n",
    "            techs.add('Docker')\n",
    "        if p.name.lower() in ('requirements.txt', 'pyproject.toml'):\n",
    "            techs.add('Python (dependencies)')\n",
    "        if p.name.lower() == 'package.json':\n",
    "            techs.add('Node.js (npm)')\n",
    "    # Format as bullet list\n",
    "    lines = [f\"- {tech}\" for tech in sorted(techs)]\n",
    "    return \"\".join(lines)\n",
    "\n",
    "\n",
    "def generate_folder_structure(repo_path: str) -> str:\n",
    "    lines = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        level = root.replace(repo_path, '').count(os.sep)\n",
    "        indent = '  ' * level\n",
    "        lines.append(f\"{indent}{os.path.basename(root)}/\")\n",
    "        for f in files:\n",
    "            lines.append(f\"{indent}  {f}\")\n",
    "    tree = \"\\n\".join(lines)\n",
    "    prompt = (\n",
    "        \"You are a software architect. Describe the folder structure and modular organization of this project.\\n\"\n",
    "        f\"Directory tree:\\n{tree}\\n---\\nDescription:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def generate_code_flow(chunks: List[Document]) -> str:\n",
    "    entries = [f\"{d.metadata['type']} {d.metadata['name']}\" for d in chunks]\n",
    "    context = '\\n'.join(entries)\n",
    "    prompt = (\n",
    "\n",
    "        \"You are a senior software engineer. Provide a step-by-step execution flow of this codebase, \"\n",
    "        \"referencing functions and classes.\\n\"\n",
    "        f\"Components:\\n{context}\\n---\\nFlow:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def summarize_chunk(chunk: Document) -> str:\n",
    "    prompt = (\n",
    "        f\"You are a senior software engineer. Generate concise documentation for the {chunk.metadata['type']} '{chunk.metadata['name']}'.\\n\"\n",
    "        f\"Code:\\n{chunk.page_content}\\n---\\nDocumentation:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "# --- Step 4: Generate multi-section DOCX ---\n",
    "def generate_technical_doc(chunks: List[Document], repo_path: str, output_path=\"Documents\\\\technical_documentation.docx\"):\n",
    "    doc = DocxDocument()\n",
    "\n",
    "    # Business Overview\n",
    "    doc.add_heading(\"Business Overview\", level=1)\n",
    "    doc.add_paragraph(generate_business_overview(repo_path))\n",
    "    '''\n",
    "    # Technical Specifications\n",
    "    doc.add_heading(\"Technical Specifications\", level=1)\n",
    "    doc.add_paragraph(generate_technical_specifications(repo_path))\n",
    "\n",
    "    # Folder Structure\n",
    "    doc.add_heading(\"Folder Structure\", level=1)\n",
    "    doc.add_paragraph(generate_folder_structure(repo_path))\n",
    "\n",
    "    # Code Flow\n",
    "    doc.add_heading(\"Code Flow\", level=1)\n",
    "    doc.add_paragraph(generate_code_flow(chunks))\n",
    "\n",
    "    # Detailed Function/Class Documentation\n",
    "    doc.add_heading(\"Detailed Documentation\", level=1)\n",
    "    for chunk in chunks:\n",
    "        if chunk.metadata.get(\"type\") in ['FunctionDef', 'ClassDef']:\n",
    "            doc.add_heading(chunk.metadata.get(\"name\", \"Unnamed\"), level=2)\n",
    "            doc.add_paragraph(f\"Source: {chunk.metadata.get('source')}\")\n",
    "            doc.add_paragraph(summarize_chunk(chunk))\n",
    "            doc.add_paragraph(\"â€”\" * 30)\n",
    "    '''\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"Saved technical documentation to `{output_path}`\")\n",
    "\n",
    "# --- Step 5: Main runner ---\n",
    "def main(repo_url: str):\n",
    "    repo_path = \"tmp_repo\"\n",
    "    #clone_repo(repo_url, repo_path)\n",
    "    chunks = extract_ast_chunks(repo_path)\n",
    "    pd.DataFrame([{\n",
    "        \"name\": d.metadata.get(\"name\"),\n",
    "        \"type\": d.metadata.get(\"type\"),\n",
    "        \"source\": d.metadata.get(\"source\"),\n",
    "        \"length\": len(d.page_content)\n",
    "    } for d in chunks]).to_csv(\"ast_chunk_summary.csv\", index=False)\n",
    "    build_faiss_from_ast_chunks(chunks)\n",
    "    generate_technical_doc(chunks, repo_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"https://github.com/adarshlearnngrow/StepUpYourCareer.AI\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e5622",
   "metadata": {},
   "source": [
    "## Overview\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281b164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Overview: The StepUpYourCareer.ai codebase is a comprehensive AI-powered career assistant designed to address the common problem faced by graduates and job seekers of not knowing what skills are required by employers and how to efficiently upskill. The purpose of this codebase is to transform resumes into personalized upskilling journeys by analyzing skill gaps, generating action plans for upskilling, and matching users with industry mentors.\n",
      "\n",
      "The scope of this codebase includes a Skill Gap Analyzer that extracts skills from resumes and compares them to target roles, an Action Plan Generator that recommends online courses and resources for missing skills, and a Mentor Matching feature that connects users with industry experts through clustering algorithms.\n",
      "\n",
      "The value delivered by this codebase is the ability to provide personalized learning roadmaps, identify skill gaps, and facilitate connections with industry mentors, all from a single resume upload. This not only helps users understand what skills they are lacking but also provides them with a clear path to upskilling and connecting with professionals in their field. Ultimately, this codebase aims to help users elevate their future career prospects and increase their chances of success in the job market.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import sys\n",
    "\n",
    "from git import Repo, GitCommandError\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# --- Utility: Determine low-value files ---\n",
    "def is_low_value_file(filepath):\n",
    "    low_value_exts = ['.css', '.min.js', '.json', '.svg', '.csv', '.xlsx', '.xls']\n",
    "    filename = os.path.basename(filepath).lower()\n",
    "    return any(filename.endswith(ext) for ext in low_value_exts) or 'mock' in filename\n",
    "\n",
    "# --- Step 1: Clone the repo (latest commit only) ---\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\") -> str:\n",
    "    \"\"\"\n",
    "    Shallow-clone the repo at the latest commit only and return the clone path.\n",
    "    \"\"\"\n",
    "    if os.path.exists(clone_path):\n",
    "        shutil.rmtree(clone_path, ignore_errors=True)\n",
    "    Repo.clone_from(repo_url, clone_path, depth=1)\n",
    "    return clone_path\n",
    "\n",
    "# --- Step 2: Extract code chunks via AST ---\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    chunks = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.*\"):\n",
    "        if is_low_value_file(filepath):\n",
    "            continue\n",
    "        try:\n",
    "            code = filepath.read_text(encoding=\"utf-8\")\n",
    "            if filepath.suffix == \".py\":\n",
    "                tree = ast.parse(code)\n",
    "                for node in tree.body:\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        content = ast.get_source_segment(code, node)\n",
    "                        if content and 50 < len(content) < 5000:\n",
    "                            meta = {\"source\": str(filepath), \"type\": type(node).__name__, \"name\": node.name}\n",
    "                            chunks.append(Document(page_content=content, metadata=meta))\n",
    "            else:\n",
    "                if 50 < len(code) < 5000:\n",
    "                    meta = {\"source\": str(filepath), \"type\": filepath.suffix, \"name\": os.path.basename(filepath)}\n",
    "                    chunks.append(Document(page_content=code, metadata=meta))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return chunks\n",
    "\n",
    "# --- Step 3: Build FAISS vector store ---\n",
    "def build_faiss_from_ast_chunks(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    return vectordb\n",
    "\n",
    "# --- Step 4: Load README and docstrings into vectorstore ---\n",
    "def load_readme(repo_path: str) -> str:\n",
    "    for pattern in ('README.md', 'README.rst', 'README.txt'):\n",
    "        readme = Path(repo_path) / pattern\n",
    "        if readme.exists():\n",
    "            return readme.read_text(encoding='utf-8')\n",
    "    return \"\"\n",
    "\n",
    "def extract_python_docstrings(repo_path: str) -> List[str]:\n",
    "    docs: List[str] = []\n",
    "    for py in Path(repo_path).rglob('*.py'):\n",
    "        try:\n",
    "            tree = ast.parse(py.read_text(encoding='utf-8'))\n",
    "            for node in ast.walk(tree):\n",
    "                if isinstance(node, (ast.Module, ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "                    doc = ast.get_docstring(node)\n",
    "                    if doc:\n",
    "                        docs.append(doc)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return docs\n",
    "\n",
    "def load_all_markdown(repo_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Concatenate every .md file under the repo (including subdirectories).\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for md in Path(repo_path).rglob(\"*.md\"):\n",
    "        texts.append(md.read_text(encoding=\"utf-8\"))\n",
    "    return \"\\n\\n\".join(texts)\n",
    "\n",
    "def build_doc_vectorstore(readme: str, docstrings: List[str], index_dir: str = 'docs_index') -> FAISS:\n",
    "    texts = []\n",
    "    if readme:\n",
    "        texts.append(readme)\n",
    "    texts.extend(docstrings)\n",
    "    docs = [Document(page_content=text, metadata={'source': 'readme' if i==0 else 'docstring'})\n",
    "            for i, text in enumerate(texts)]\n",
    "    embedder = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    vectordb = FAISS.from_documents(docs, embedder)\n",
    "    vectordb.save_local(index_dir)\n",
    "    return vectordb\n",
    "\n",
    "# --- Step 5: Generate Business Overview ---\n",
    "def generate_business_overview(readme_text: str, repo_path: str, vectordb: FAISS = None, k: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Use README content if available, otherwise fall back to vectordb or prompt user for input.\n",
    "    \"\"\"\n",
    "    # Prefer README content\n",
    "    if readme_text and readme_text.strip():\n",
    "        context = readme_text\n",
    "    else:\n",
    "        context = \"\"\n",
    "        if vectordb:\n",
    "            snippets = vectordb.similarity_search(\"business overview\", k=k)\n",
    "            if snippets:\n",
    "                context = \"\\n\\n\".join([doc.page_content for doc in snippets])\n",
    "        if not context.strip():\n",
    "            context = input(\n",
    "                \"No README or docstrings found. Please provide high-level context or project description: \"\n",
    "            )\n",
    "            sys.exit(1)\n",
    "    \n",
    "        \n",
    "    # Construct prompt with proper quoting\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    prompt = f\"\"\"\n",
    "You are a technical writer for enterprise software. Given the following context, provide a high-level business overview of this codebase, including purpose, scope, and value delivered.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "---\n",
    "Overview:\n",
    "\"\"\"\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "# --- Entry Point Example ---\n",
    "if __name__ == \"__main__\":\n",
    "    repo_url = \"https://github.com/adarshlearnngrow/StepUpYourCareer.AI\"\n",
    "    repo_path = clone_repo(repo_url)\n",
    "\n",
    "    readme_text = load_readme(repo_path)\n",
    "    docstrings = extract_python_docstrings(repo_path)\n",
    "    docs_vectordb = build_doc_vectorstore(readme_text, docstrings)\n",
    "    overview = generate_business_overview(readme_text, repo_path, docs_vectordb)\n",
    "    print(\"Business Overview:\", overview)\n",
    "\n",
    "    chunks = extract_ast_chunks(repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e366b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m readme_text = load_readme(repo_path)\n\u001b[32m      6\u001b[39m docstrings = extract_python_docstrings(repo_path)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m docs_vectordb = \u001b[43mbuild_doc_vectorstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreadme_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocstrings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m overview = generate_business_overview(readme_text, repo_path, docs_vectordb)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mBusiness Overview:\u001b[39m\u001b[33m\"\u001b[39m, overview)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mbuild_doc_vectorstore\u001b[39m\u001b[34m(readme, docstrings, index_dir)\u001b[39m\n\u001b[32m    100\u001b[39m docs = [Document(page_content=text, metadata={\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mreadme\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m i==\u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdocstring\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m    101\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts)]\n\u001b[32m    102\u001b[39m embedder = HuggingFaceEmbeddings(model_name=\u001b[33m'\u001b[39m\u001b[33msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m vectordb = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    104\u001b[39m vectordb.save_local(index_dir)\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m vectordb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:848\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    846\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m848\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1044\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03mThis is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1043\u001b[39m embeddings = embedding.embed_documents(texts)\n\u001b[32m-> \u001b[39m\u001b[32m1044\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__from\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1001\u001b[39m, in \u001b[36mFAISS.__from\u001b[39m\u001b[34m(cls, texts, embeddings, embedding, metadatas, ids, normalize_L2, distance_strategy, **kwargs)\u001b[39m\n\u001b[32m    998\u001b[39m     index = faiss.IndexFlatIP(\u001b[38;5;28mlen\u001b[39m(embeddings[\u001b[32m0\u001b[39m]))\n\u001b[32m    999\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1000\u001b[39m     \u001b[38;5;66;03m# Default to L2, currently other metric types not initialized.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1001\u001b[39m     index = faiss.IndexFlatL2(\u001b[38;5;28mlen\u001b[39m(\u001b[43membeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[32m   1002\u001b[39m docstore = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mdocstore\u001b[39m\u001b[33m\"\u001b[39m, InMemoryDocstore())\n\u001b[32m   1003\u001b[39m index_to_docstore_id = kwargs.pop(\u001b[33m\"\u001b[39m\u001b[33mindex_to_docstore_id\u001b[39m\u001b[33m\"\u001b[39m, {})\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ce0de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_doc_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
