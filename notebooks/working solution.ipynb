{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ac7ac1",
   "metadata": {},
   "source": [
    "# Working code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e6276",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a306660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClusteringMentorModelTraining.ipynb\n",
      "Job_Description_JD_Manupulation.ipynb\n",
      "Presentation - StepUpYourCareer.ai Elevate Your Future.pdf\n",
      "README.md\n",
      "Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb\n",
      ".devcontainer/devcontainer.json\n",
      ".git/config\n",
      ".git/description\n",
      ".git/HEAD\n",
      ".git/index\n",
      ".git/packed-refs\n",
      ".git/shallow\n",
      "data/all_roles_student_resumes.json\n",
      "data/req_job_desc.csv\n",
      "data/role_skills.csv\n",
      "data/role_skills.json\n",
      "data/sim_resume.json\n",
      "data/skill_gap_analysis.csv\n",
      "data/skill_gap_analysis.json\n",
      "StepUpAI/app.py\n",
      "StepUpAI/generated_mentors.json\n",
      "StepUpAI/mentors_final_data.json\n",
      "StepUpAI/requirements.txt\n",
      "StepUpAI/role_skills.json\n",
      "StepUpAI/skill_gap_analysis.json\n",
      "StepUpAI/skill_resource_mapping.json\n",
      "StepUpAI/models/fitted_vectorizer.pkl\n",
      "StepUpAI/models/mentor_clustering_model.pkl\n",
      ".git/hooks/applypatch-msg.sample\n",
      ".git/hooks/commit-msg.sample\n",
      ".git/hooks/fsmonitor-watchman.sample\n",
      ".git/hooks/post-update.sample\n",
      ".git/hooks/pre-applypatch.sample\n",
      ".git/hooks/pre-commit.sample\n",
      ".git/hooks/pre-merge-commit.sample\n",
      ".git/hooks/pre-push.sample\n",
      ".git/hooks/pre-rebase.sample\n",
      ".git/hooks/pre-receive.sample\n",
      ".git/hooks/prepare-commit-msg.sample\n",
      ".git/hooks/push-to-checkout.sample\n",
      ".git/hooks/sendemail-validate.sample\n",
      ".git/hooks/update.sample\n",
      ".git/info/exclude\n",
      ".git/logs/HEAD\n",
      ".git/refs/heads/main\n",
      ".git/refs/remotes/origin/HEAD\n",
      ".git/objects/pack/pack-77834c024e709f430795a541c7d59da466020bfc.idx\n",
      ".git/objects/pack/pack-77834c024e709f430795a541c7d59da466020bfc.pack\n",
      ".git/objects/pack/pack-77834c024e709f430795a541c7d59da466020bfc.rev\n",
      ".git/logs/refs/heads/main\n",
      ".git/logs/refs/remotes/origin/HEAD\n",
      "âœ… Saved technical documentation to `technical_documentation.docx`\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from git import Repo, GitCommandError\n",
    "\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from docx import Document as DocxDocument\n",
    "\n",
    "# --- Utility: Determine low-value files ---\n",
    "def is_low_value_file(filepath):\n",
    "    low_value_exts = ['.css', '.min.js', '.json', '.svg', '.csv', '.xlsx', '.xls']\n",
    "    filename = os.path.basename(filepath).lower()\n",
    "    return any(filename.endswith(ext) for ext in low_value_exts) or 'mock' in filename\n",
    "\n",
    "# --- Step 1: Clone the repo (latest commit only) ---\n",
    "def clone_repo(repo_url, clone_path=\"tmp_repo\"):\n",
    "    if os.path.exists(clone_path):\n",
    "        shutil.rmtree(clone_path, ignore_errors=True)\n",
    "    Repo.clone_from(repo_url, clone_path, depth=1)\n",
    "\n",
    "# --- Step 2: Extract code chunks via AST ---\n",
    "def extract_ast_chunks(repo_path: str) -> List[Document]:\n",
    "    chunks = []\n",
    "    for filepath in Path(repo_path).rglob(\"*.*\"):\n",
    "        if is_low_value_file(filepath):\n",
    "            continue\n",
    "        try:\n",
    "            code = filepath.read_text(encoding=\"utf-8\")\n",
    "            if filepath.suffix == \".py\":\n",
    "                tree = ast.parse(code)\n",
    "                for node in tree.body:\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        content = ast.get_source_segment(code, node)\n",
    "                        if content and 50 < len(content) < 5000:\n",
    "                            meta = {\"source\": str(filepath), \"type\": type(node).__name__, \"name\": node.name}\n",
    "                            chunks.append(Document(page_content=content, metadata=meta))\n",
    "            else:\n",
    "                if 50 < len(code) < 5000:\n",
    "                    meta = {\"source\": str(filepath), \"type\": filepath.suffix, \"name\": os.path.basename(filepath)}\n",
    "                    chunks.append(Document(page_content=code, metadata=meta))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return chunks\n",
    "\n",
    "# --- Step 3: Build FAISS vector store ---\n",
    "def build_faiss_from_ast_chunks(chunks: List[Document], db_path=\"faiss_ast_index\"):\n",
    "    embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vectordb = FAISS.from_documents(chunks, embedder)\n",
    "    vectordb.save_local(db_path)\n",
    "    return vectordb\n",
    "\n",
    "# --- LLM client ---\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "# --- Section functions ---\n",
    "def generate_business_overview(repo_path: str) -> str:\n",
    "    files = [p.relative_to(repo_path).as_posix() for p in Path(repo_path).rglob(\"*\") if p.is_file()]\n",
    "    context = \"\\n\".join(files)\n",
    "    print(context)\n",
    "    prompt = (\n",
    "        \"You are a technical writer for enterprise software. Provide a high-level business overview of this solution, \"\n",
    "        \"including purpose, scope, and value delivered.\\n\"\n",
    "        f\"Files in project:\\n{context}\\n---\\nOverview:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def generate_technical_specifications(repo_path: str) -> str:\n",
    "    # Deterministically list technologies used without LLM to avoid hallucination\n",
    "    ext_map = {'.py':'Python','.js':'JavaScript','.ts':'TypeScript','.java':'Java', '.html':'HTML','.css':'CSS', '.go':'Go', '.rs':'Rust'}\n",
    "    techs = set()\n",
    "    for p in Path(repo_path).rglob('*'):\n",
    "        if p.suffix in ext_map:\n",
    "            techs.add(ext_map[p.suffix])\n",
    "        if p.name.lower() == 'dockerfile':\n",
    "            techs.add('Docker')\n",
    "        if p.name.lower() in ('requirements.txt', 'pyproject.toml'):\n",
    "            techs.add('Python (dependencies)')\n",
    "        if p.name.lower() == 'package.json':\n",
    "            techs.add('Node.js (npm)')\n",
    "    # Format as bullet list\n",
    "    lines = [f\"- {tech}\" for tech in sorted(techs)]\n",
    "    return \"\".join(lines)\n",
    "\n",
    "\n",
    "def generate_folder_structure(repo_path: str) -> str:\n",
    "    lines = []\n",
    "    for root, dirs, files in os.walk(repo_path):\n",
    "        level = root.replace(repo_path, '').count(os.sep)\n",
    "        indent = '  ' * level\n",
    "        lines.append(f\"{indent}{os.path.basename(root)}/\")\n",
    "        for f in files:\n",
    "            lines.append(f\"{indent}  {f}\")\n",
    "    tree = \"\\n\".join(lines)\n",
    "    prompt = (\n",
    "        \"You are a software architect. Describe the folder structure and modular organization of this project.\\n\"\n",
    "        f\"Directory tree:\\n{tree}\\n---\\nDescription:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def generate_code_flow(chunks: List[Document]) -> str:\n",
    "    entries = [f\"{d.metadata['type']} {d.metadata['name']}\" for d in chunks]\n",
    "    context = '\\n'.join(entries)\n",
    "    prompt = (\n",
    "        \"You are a senior software engineer. Provide a step-by-step execution flow of this codebase, \"\n",
    "        \"referencing functions and classes.\\n\"\n",
    "        f\"Components:\\n{context}\\n---\\nFlow:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "\n",
    "def summarize_chunk(chunk: Document) -> str:\n",
    "    prompt = (\n",
    "        f\"You are a senior software engineer. Generate concise documentation for the {chunk.metadata['type']} '{chunk.metadata['name']}'.\\n\"\n",
    "        f\"Code:\\n{chunk.page_content}\\n---\\nDocumentation:\\n\"\n",
    "    )\n",
    "    return llm.invoke(prompt).content.strip()\n",
    "\n",
    "# --- Step 4: Generate multi-section DOCX ---\n",
    "def generate_technical_doc(chunks: List[Document], repo_path: str, output_path=\"technical_documentation.docx\"):\n",
    "    doc = DocxDocument()\n",
    "\n",
    "    # Business Overview\n",
    "    doc.add_heading(\"Business Overview\", level=1)\n",
    "    doc.add_paragraph(generate_business_overview(repo_path))\n",
    "    '''\n",
    "    # Technical Specifications\n",
    "    doc.add_heading(\"Technical Specifications\", level=1)\n",
    "    doc.add_paragraph(generate_technical_specifications(repo_path))\n",
    "\n",
    "    # Folder Structure\n",
    "    doc.add_heading(\"Folder Structure\", level=1)\n",
    "    doc.add_paragraph(generate_folder_structure(repo_path))\n",
    "\n",
    "    # Code Flow\n",
    "    doc.add_heading(\"Code Flow\", level=1)\n",
    "    doc.add_paragraph(generate_code_flow(chunks))\n",
    "\n",
    "    # Detailed Function/Class Documentation\n",
    "    doc.add_heading(\"Detailed Documentation\", level=1)\n",
    "    for chunk in chunks:\n",
    "        if chunk.metadata.get(\"type\") in ['FunctionDef', 'ClassDef']:\n",
    "            doc.add_heading(chunk.metadata.get(\"name\", \"Unnamed\"), level=2)\n",
    "            doc.add_paragraph(f\"Source: {chunk.metadata.get('source')}\")\n",
    "            doc.add_paragraph(summarize_chunk(chunk))\n",
    "            doc.add_paragraph(\"â€”\" * 30)\n",
    "    '''\n",
    "\n",
    "    doc.save(output_path)\n",
    "    print(f\"âœ… Saved technical documentation to `{output_path}`\")\n",
    "\n",
    "# --- Step 5: Main runner ---\n",
    "def main(repo_url: str):\n",
    "    repo_path = \"tmp_repo\"\n",
    "    #clone_repo(repo_url, repo_path)\n",
    "    chunks = extract_ast_chunks(repo_path)\n",
    "    pd.DataFrame([{\n",
    "        \"name\": d.metadata.get(\"name\"),\n",
    "        \"type\": d.metadata.get(\"type\"),\n",
    "        \"source\": d.metadata.get(\"source\"),\n",
    "        \"length\": len(d.page_content)\n",
    "    } for d in chunks]).to_csv(\"ast_chunk_summary.csv\", index=False)\n",
    "    build_faiss_from_ast_chunks(chunks)\n",
    "    generate_technical_doc(chunks, repo_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(\"https://github.com/adarshlearnngrow/StepUpYourCareer.AI\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a281b164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_doc_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
