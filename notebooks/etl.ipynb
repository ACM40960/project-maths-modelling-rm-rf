{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15060132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import sys\n",
    "import nbformat\n",
    "from git import Repo, GitCommandError\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from docx import Document as DocxDocument\n",
    "from itertools import chain\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70891df4",
   "metadata": {},
   "source": [
    "# Reading the repo and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eee836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_low_value_file(filepath):\n",
    "    low_value_exts = [\n",
    "        '.css', '.min.js', '.json', '.svg', '.csv', '.xlsx', '.xls',\n",
    "        '.log', '.lock', '.pyc', '.pyo', '.pyd', '.class', '.jar', '.war',\n",
    "        '.o', '.obj', '.dll', '.exe', '.so', '.a', '.db', '.sqlite', '.sqlite3',\n",
    "        '.bak', '.tmp', '.ico', '.icns', '.pdf', '.docx', '.pptx',\n",
    "        '.7z', '.zip', '.tar', '.gz', '.rar', '.iml'\n",
    "    ]\n",
    "\n",
    "    low_value_files = [\n",
    "        'readme.md', 'license', '.gitignore', '.gitattributes', 'post-update.sample',\n",
    "        'fsmonitor-watchman.sample', 'pre-commit', 'pre-push', 'commit-msg',\n",
    "        'tags', 'head', 'config', 'description', 'index', '.editorconfig',\n",
    "        '.prettierrc', '.eslintrc', '.gitmodules', '.mailmap', '.clang-format',\n",
    "        'pipfile.lock', 'yarn.lock', 'package-lock.json', '.env', '.env.example', '.npmrc',\n",
    "        'update.sample'\n",
    "    ]\n",
    "\n",
    "    low_value_dirs = {\n",
    "        '.git', '.vscode', '.idea', '__pycache__',\n",
    "        'node_modules', 'dist', 'build', '.pytest_cache'\n",
    "    }\n",
    "\n",
    "    filepath_str = str(filepath).lower()\n",
    "    parts = set(Path(filepath).parts)\n",
    "\n",
    "    return (\n",
    "        Path(filepath).suffix.lower() in low_value_exts or\n",
    "        os.path.basename(filepath).lower() in low_value_files or\n",
    "        any(d in parts for d in low_value_dirs) or\n",
    "        'mock' in filepath_str\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1a2ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, shutil, os, stat\n",
    "from pathlib import Path\n",
    "from git import Repo\n",
    "\n",
    "def _on_rm_error(func, path, exc_info):\n",
    "    try:\n",
    "        os.chmod(path, stat.S_IWRITE)\n",
    "    except Exception:\n",
    "        pass\n",
    "    func(path)\n",
    "\n",
    "def clone_repo(repo_url, clone_path=None) -> str:\n",
    "    # Use a fresh temp dir by default to avoid collisions\n",
    "    if clone_path is None:\n",
    "        clone_path = tempfile.mkdtemp(prefix=\"repo_\")\n",
    "    cp = Path(clone_path)\n",
    "\n",
    "    if cp.exists():\n",
    "        shutil.rmtree(cp, onerror=_on_rm_error)\n",
    "\n",
    "    Repo.clone_from(repo_url, str(cp), depth=1)\n",
    "    return str(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chunks(repo_path: str, index_dir: str = \"docs_index\") -> list[Document]:\n",
    "    chunks = []\n",
    "    repo_path = Path(repo_path)\n",
    "\n",
    "    # 1. README files\n",
    "    for readme_name in (\"README.md\", \"README.rst\", \"README.txt\"):\n",
    "        readme_path = repo_path / readme_name\n",
    "        if readme_path.exists():\n",
    "            content = readme_path.read_text(encoding=\"utf-8\").strip()\n",
    "            if 50 < len(content) < 5000:\n",
    "                chunks.append(Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": str(readme_path),\n",
    "                        \"file_ext\": \"text\",\n",
    "                        \"type\": \"readme\",\n",
    "                        \"name\": readme_name,\n",
    "                        \"lines\": f\"1-{content.count(chr(10)) + 1}\"\n",
    "                    }\n",
    "                ))\n",
    "            break\n",
    "\n",
    "    # 2. Other Markdown files\n",
    "    # 2. Other text files (Markdown/RST/TXT)\n",
    "    for text_path in chain(\n",
    "        repo_path.rglob(\"*.md\"),\n",
    "        repo_path.rglob(\"*.rst\"),\n",
    "        repo_path.rglob(\"*.txt\"),\n",
    "    ):\n",
    "        if text_path.name.lower() == \"readme.md\":\n",
    "            continue\n",
    "        try:\n",
    "            content = text_path.read_text(encoding=\"utf-8\").strip()\n",
    "            if 50 < len(content) < 5000:\n",
    "                ext = text_path.suffix.lower().lstrip(\".\") or \"txt\"  # <- suffix as type/ext\n",
    "                chunks.append(Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": str(text_path),\n",
    "                        \"file_ext\": ext,     # was \"markdown\"\n",
    "                        \"type\": ext,         # was \"markdown\" — now suffix-based\n",
    "                        \"name\": text_path.name,\n",
    "                        \"lines\": f\"1-{content.count(chr(10)) + 1}\",\n",
    "                    }\n",
    "                ))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 3. Code and misc files\n",
    "    for filepath in repo_path.rglob(\"*.*\"):\n",
    "        if is_low_value_file(filepath):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            suffix = filepath.suffix.lower()\n",
    "\n",
    "            # 3a. Python files\n",
    "            if suffix == \".py\":\n",
    "                code = filepath.read_text(encoding=\"utf-8\")\n",
    "                tree = ast.parse(code)\n",
    "\n",
    "                mod_doc = ast.get_docstring(tree)\n",
    "                if mod_doc and 50 < len(mod_doc) < 5000:\n",
    "                    chunks.append(Document(\n",
    "                        page_content=mod_doc,\n",
    "                        metadata={\n",
    "                            \"source\": str(filepath),\n",
    "                            \"file_ext\": \"code\",\n",
    "                            \"type\": \"module_docstring\",\n",
    "                            \"name\": filepath.name,\n",
    "                            \"lines\": f\"1-{code.count(chr(10)) + 1}\"\n",
    "                        }\n",
    "                    ))\n",
    "\n",
    "                for node in tree.body:\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        content = ast.get_source_segment(code, node)\n",
    "                        if content and 50 < len(content) < 5000:\n",
    "                            chunks.append(Document(\n",
    "                                page_content=content,\n",
    "                                metadata={\n",
    "                                    \"source\": str(filepath),\n",
    "                                    \"file_ext\": \"code\",\n",
    "                                    \"type\": type(node).__name__.lower(),\n",
    "                                    \"name\": node.name,\n",
    "                                    \"lines\": f\"{node.lineno}-{getattr(node, 'end_lineno', node.lineno)}\"\n",
    "                                }\n",
    "                            ))\n",
    "\n",
    "                        doc = ast.get_docstring(node)\n",
    "                        if doc and 50 < len(doc) < 5000:\n",
    "                            chunks.append(Document(\n",
    "                                page_content=doc,\n",
    "                                metadata={\n",
    "                                    \"source\": str(filepath),\n",
    "                                    \"file_ext\": \"code\",\n",
    "                                    \"type\": f\"{type(node).__name__.lower()}_docstring\",\n",
    "                                    \"name\": node.name,\n",
    "                                    \"lines\": f\"{node.lineno}-{getattr(node, 'end_lineno', node.lineno)}\"\n",
    "                                }\n",
    "                            ))\n",
    "\n",
    "            # 3b. Notebooks\n",
    "            elif suffix == \".ipynb\":\n",
    "                nb = nbformat.read(filepath, as_version=4)\n",
    "                for i, cell in enumerate(nb.cells):\n",
    "                    if cell.cell_type in (\"markdown\", \"code\"):\n",
    "                        content = cell.source.strip()\n",
    "                        if 50 < len(content) < 5000:\n",
    "                            chunks.append(Document(\n",
    "                                page_content=content,\n",
    "                                metadata={\n",
    "                                    \"source\": str(filepath),\n",
    "                                    \"file_ext\": \"code\",\n",
    "                                    \"type\": f\"{cell.cell_type}_cell\",\n",
    "                                    \"name\": f\"{filepath.name} - cell {i}\",\n",
    "                                    \"lines\": f\"cell_{i}\"\n",
    "                                }\n",
    "                            ))\n",
    "\n",
    "            # 3c. Other code/text files\n",
    "            else:\n",
    "                code = filepath.read_text(encoding=\"utf-8\")\n",
    "                if 50 < len(code) < 5000:\n",
    "                    chunks.append(Document(\n",
    "                        page_content=code,\n",
    "                        metadata={\n",
    "                            \"source\": str(filepath),\n",
    "                            \"file_ext\": \"code\",\n",
    "                            \"type\": suffix,\n",
    "                            \"name\": filepath.name,\n",
    "                            \"lines\": f\"1-{code.count(chr(10)) + 1}\"\n",
    "                        }\n",
    "                    ))\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Optional FAISS index creation\n",
    "    # embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    # vectordb = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    # vectordb.save_local(index_dir)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "7682d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/adarshlearnngrow/StepUpYourCareer.AI\"\n",
    "repo_path = clone_repo(repo_url)\n",
    "\n",
    "# 1️⃣ Build the FAISS DB from your repo\n",
    "chunks = extract_all_chunks(repo_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "f4b4013d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cef68e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-openai langchain-community faiss-cpu\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import hashlib, json, os\n",
    "\n",
    "from langchain_core.documents import Document  # or: from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# --- helpers ---------------------------------------------------------------\n",
    "\n",
    "def _stable_id(doc: Document) -> str:\n",
    "    meta_json = json.dumps(doc.metadata or {}, sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.md5((doc.page_content + \"||\" + meta_json).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _norm_ext(ext: str) -> str:\n",
    "    ext = (ext or \"\").lower().lstrip(\".\")\n",
    "    return ext\n",
    "\n",
    "def _guess_ext(doc: Document) -> str:\n",
    "    # 1) try metadata.file_ext\n",
    "    ext = _norm_ext(str(doc.metadata.get(\"file_ext\", \"\")))\n",
    "    # some pipelines put \"code\" there; treat as unknown\n",
    "    if ext in {\"\", \"code\"}:\n",
    "        # 2) try source path\n",
    "        src = str(doc.metadata.get(\"source\", \"\"))\n",
    "        if src:\n",
    "            ext = _norm_ext(Path(src).suffix)\n",
    "    # 3) fall back to name\n",
    "    if not ext and doc.metadata.get(\"name\"):\n",
    "        ext = _norm_ext(Path(str(doc.metadata[\"name\"])).suffix)\n",
    "    return ext\n",
    "\n",
    "TEXT_EXTS = {\"md\", \"rst\", \"txt\"}                       # pure prose\n",
    "CONFIG_EXTS = {\"yml\", \"yaml\", \"toml\", \"ini\", \"cfg\"}    # config (we'll treat as CODE for retrieval)\n",
    "NB_EXTS = {\"ipynb\"}                                    # notebooks\n",
    "CODE_EXTS = {\"py\", \"js\", \"ts\", \"java\", \"go\", \"cpp\", \"c\", \"cs\", \"rb\", \"php\"}\n",
    "\n",
    "def _is_text(doc: Document) -> bool:\n",
    "    \"\"\"Decide TEXT vs CODE using both 'type' and real extension.\"\"\"\n",
    "    t = str(doc.metadata.get(\"type\", \"\")).lower()\n",
    "    ext = _guess_ext(doc)\n",
    "\n",
    "    # type-first rules\n",
    "    if \"docstring\" in t or t in {\"readme\", \"markdown\", \"inline_comment\", \"markdown_cell\", \"module_docstring\"}:\n",
    "        return True\n",
    "    if t in {\"functiondef\", \"asyncfunctiondef\", \"classdef\", \"code\", \"ipynb_cell\"}:\n",
    "        return False\n",
    "\n",
    "    # extension fallbacks\n",
    "    if ext in TEXT_EXTS:\n",
    "        return True\n",
    "    if ext in CODE_EXTS | CONFIG_EXTS | NB_EXTS:\n",
    "        return False\n",
    "\n",
    "    # special names\n",
    "    name = str(doc.metadata.get(\"name\", \"\")).lower()\n",
    "    if name in {\"readme.md\", \"readme.rst\", \"readme.txt\", \"license\", \"license.txt\"}:\n",
    "        return True\n",
    "    if name == \"requirements.txt\":  # treat as TEXT so overview/installation finds it\n",
    "        return True\n",
    "\n",
    "    # default to CODE (safer for repos)\n",
    "    return False\n",
    "\n",
    "# --- main saver ------------------------------------------------------------\n",
    "\n",
    "def save_to_faiss_split_by_ext(\n",
    "    chunks: List[Document],\n",
    "    base_dir: str = \"docs_index\",\n",
    "    model: str = \"text-embedding-3-small\",  # or \"text-embedding-3-large\"\n",
    "    min_chars: int = 30,\n",
    "    max_chars: int = 10000,\n",
    "):\n",
    "    # filter once\n",
    "    docs = [d for d in chunks if d and d.page_content and min_chars <= len(d.page_content) <= max_chars]\n",
    "\n",
    "    # split\n",
    "    text_docs = [d for d in docs if _is_text(d)]\n",
    "    code_docs = [d for d in docs if not _is_text(d)]\n",
    "\n",
    "    # embedder\n",
    "    embedder = OpenAIEmbeddings(model=model)\n",
    "\n",
    "    # TEXT index\n",
    "    text_path = Path(base_dir) / \"text_index\"\n",
    "    text_path.mkdir(parents=True, exist_ok=True)\n",
    "    text_vs = FAISS.from_documents(text_docs, embedder, ids=[_stable_id(d) for d in text_docs])\n",
    "    text_vs.save_local(str(text_path))\n",
    "\n",
    "    # CODE index\n",
    "    code_path = Path(base_dir) / \"code_index\"\n",
    "    code_path.mkdir(parents=True, exist_ok=True)\n",
    "    code_vs = FAISS.from_documents(code_docs, embedder, ids=[_stable_id(d) for d in code_docs])\n",
    "    code_vs.save_local(str(code_path))\n",
    "\n",
    "    print(f\"✅ saved:\\n  text -> {text_path}  ({len(text_docs)} docs)\\n  code -> {code_path}  ({len(code_docs)} docs)\")\n",
    "    return {\"text_count\": len(text_docs), \"code_count\": len(code_docs), \"model\": model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "0823cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ saved:\n",
      "  text -> docs_index\\text_index  (16 docs)\n",
      "  code -> docs_index\\code_index  (44 docs)\n",
      "{'text_count': 16, 'code_count': 44, 'model': 'text-embedding-3-small'}\n"
     ]
    }
   ],
   "source": [
    "stats = save_to_faiss_split_by_ext(chunks, base_dir=\"docs_index\", model=\"text-embedding-3-small\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7acc457",
   "metadata": {},
   "source": [
    "# No need to run this part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "333f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-openai langchain-community faiss-cpu chromadb\n",
    "\n",
    "import math, re\n",
    "from statistics import mean\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional, Literal\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "Backend = Literal[\"faiss\", \"chroma\"]\n",
    "\n",
    "def _load_store(backend: Backend, path: str, model: str, collection: Optional[str] = None):\n",
    "    embedder = OpenAIEmbeddings(model=model)\n",
    "    if backend == \"faiss\":\n",
    "        return FAISS.load_local(path, embedder, allow_dangerous_deserialization=True), embedder\n",
    "    if backend == \"chroma\":\n",
    "        return Chroma(\n",
    "            collection_name=collection or \"default\",\n",
    "            persist_directory=path,\n",
    "            embedding_function=embedder,\n",
    "        ), embedder\n",
    "    raise ValueError(\"backend must be 'faiss' or 'chroma'\")\n",
    "\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    return [t for t in re.findall(r\"[A-Za-z0-9_]+\", text.lower()) if len(t) > 1]\n",
    "\n",
    "def _bm25_scores(query: str, docs: Dict[str, Document]) -> Dict[str, float]:\n",
    "    \"\"\"BM25 over the candidate set (good enough for re-ranking).\"\"\"\n",
    "    q_terms = _tokenize(query)\n",
    "    if not q_terms: \n",
    "        return {k: 0.0 for k in docs.keys()}\n",
    "    tok = {k: _tokenize(v.page_content or \"\") for k, v in docs.items()}\n",
    "    lengths = {k: len(v) for k, v in tok.items()}\n",
    "    N = len(tok)\n",
    "    avgdl = max(1.0, mean(lengths.values())) if lengths else 1.0\n",
    "    # document frequency\n",
    "    df = {}\n",
    "    for terms in tok.values():\n",
    "        seen = set(terms)\n",
    "        for t in seen:\n",
    "            df[t] = df.get(t, 0) + 1\n",
    "    # idf\n",
    "    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1.0) for t in set(q_terms)}\n",
    "    k1, b = 1.5, 0.75\n",
    "    scores = {}\n",
    "    for did, terms in tok.items():\n",
    "        score = 0.0\n",
    "        tf_counts = {}\n",
    "        for t in terms:\n",
    "            tf_counts[t] = tf_counts.get(t, 0) + 1\n",
    "        for qt in q_terms:\n",
    "            tf = tf_counts.get(qt, 0)\n",
    "            if tf == 0: \n",
    "                continue\n",
    "            denom = tf + k1 * (1 - b + b * (lengths[did] / avgdl))\n",
    "            score += idf.get(qt, 0.0) * (tf * (k1 + 1)) / (denom if denom else 1.0)\n",
    "        scores[did] = score\n",
    "    return scores\n",
    "\n",
    "def _minmax_norm(scores: Dict[str, float]) -> Dict[str, float]:\n",
    "    if not scores: return {}\n",
    "    vals = list(scores.values())\n",
    "    lo, hi = min(vals), max(vals)\n",
    "    if hi == lo: \n",
    "        return {k: 0.0 for k in scores}\n",
    "    return {k: (v - lo) / (hi - lo) for k, v in scores.items()}\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    *,\n",
    "    # where your indexes live:\n",
    "    backend: Backend = \"faiss\",\n",
    "    text_path: str = \"docs_index/text_index\",\n",
    "    code_path: str = \"docs_index/code_index\",\n",
    "    collection_text: Optional[str] = None,  # for Chroma\n",
    "    collection_code: Optional[str] = None,  # for Chroma\n",
    "    model: str = \"text-embedding-3-small\",\n",
    "    # retrieval sizes:\n",
    "    k_text: int = 30,\n",
    "    k_code: int = 30,\n",
    "    k_final: int = 12,\n",
    "    # weights in final score:\n",
    "    weight_vector: float = 0.7,\n",
    "    weight_bm25: float = 0.3,\n",
    "    # optional metadata filter over the merged candidates:\n",
    "    metadata_filter: Optional[callable] = None,\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Returns a list of (Document, score) after merging:\n",
    "      1) vector top-K from text index\n",
    "      2) vector top-K from code index\n",
    "      3) BM25 re-rank over the union\n",
    "    \"\"\"\n",
    "    text_vs, _ = _load_store(backend, text_path, model, collection_text)\n",
    "    code_vs, _ = _load_store(backend, code_path, model, collection_code)\n",
    "\n",
    "    # 1) vector retrieval (note: FAISS returns distance; smaller is better)\n",
    "    text_hits = text_vs.similarity_search_with_score(query, k=k_text)\n",
    "    code_hits = code_vs.similarity_search_with_score(query, k=k_code)\n",
    "\n",
    "    # unify candidates by creating synthetic IDs if missing\n",
    "    def _doc_id(d: Document) -> str:\n",
    "        # try to use original id if present\n",
    "        return d.metadata.get(\"id\") or d.metadata.get(\"_id\") or f\"{d.metadata.get('source','')}::{d.metadata.get('lines','')}::{hash(d.page_content)}\"\n",
    "\n",
    "    candidates: Dict[str, Tuple[Document, float]] = {}\n",
    "    for doc, score in text_hits + code_hits:\n",
    "        did = _doc_id(doc)\n",
    "        # convert distance-like score to similarity (higher is better)\n",
    "        sim = 1.0 / (1.0 + max(1e-9, score))\n",
    "        if (did not in candidates) or (sim > candidates[did][1]):\n",
    "            candidates[did] = (doc, sim)\n",
    "\n",
    "    # optional metadata filtering\n",
    "    if metadata_filter:\n",
    "        candidates = {k: v for k, v in candidates.items() if metadata_filter(v[0])}\n",
    "\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    # 2) BM25 over candidate set\n",
    "    cand_docs = {did: d for did, (d, _) in candidates.items()}\n",
    "    bm25 = _bm25_scores(query, cand_docs)\n",
    "\n",
    "    # 3) normalize & blend\n",
    "    vec_norm = _minmax_norm({did: sim for did, (_, sim) in candidates.items()})\n",
    "    bm_norm = _minmax_norm(bm25)\n",
    "\n",
    "    final: List[Tuple[Document, float]] = []\n",
    "    for did, (doc, _) in candidates.items():\n",
    "        score = weight_vector * vec_norm.get(did, 0.0) + weight_bm25 * bm_norm.get(did, 0.0)\n",
    "        final.append((doc, score))\n",
    "\n",
    "    final.sort(key=lambda x: x[1], reverse=True)\n",
    "    return final[:k_final]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "255ddc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\adars\\\\Documents\\\\UCD\\\\sem 3\\\\Automated Technical Documentation from Code Repositories\\\\notebooks'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7104a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-openai langchain-community faiss-cpu\n",
    "\n",
    "import re, time\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Literal, Tuple\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "Route = Literal[\"auto\", \"text\", \"code\", \"both\"]\n",
    "\n",
    "# ---- simple router ---------------------------------------------------------\n",
    "_ARCH_HINTS = re.compile(r\"\\b(arch(itecture)?|overview|design|diagram|components|topology)\\b\", re.I)\n",
    "\n",
    "def _route_query(q: str) -> Route:\n",
    "    return \"text\" if _ARCH_HINTS.search(q) else \"both\"\n",
    "\n",
    "# ---- caches ---------------------------------------------------------------\n",
    "@lru_cache(maxsize=2)\n",
    "def _get_embedder(model: str) -> OpenAIEmbeddings:\n",
    "    return OpenAIEmbeddings(model=model)\n",
    "\n",
    "@lru_cache(maxsize=8)\n",
    "def _load_faiss(path: str, model: str) -> FAISS:\n",
    "    emb = _get_embedder(model)\n",
    "    return FAISS.load_local(path, emb, allow_dangerous_deserialization=True)\n",
    "\n",
    "# ---- main search ----------------------------------------------------------\n",
    "def fast_search(\n",
    "    query: str,\n",
    "    *,\n",
    "    base_dir: str = \"docs_index\",\n",
    "    model: str = \"text-embedding-3-small\",   # or \"text-embedding-3-large\" (must match index dims)\n",
    "    route: Route = \"auto\",                   # auto|text|code|both\n",
    "    k_text: int = 8,\n",
    "    k_code: int = 12,\n",
    "    k_final: int = 20,\n",
    "    use_bm25: bool = False,                  # off by default for speed\n",
    "    types_include: Optional[set] = None,     # e.g. {\"module_docstring\",\"readme\",\"markdown\"}\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Fast retrieval:\n",
    "      - routes to the right index(es)\n",
    "      - uses MMR vector search for diversity\n",
    "      - (optional) BM25 re-rank on the small union\n",
    "    Returns [(doc, score)] with highest score first.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    if route == \"auto\":\n",
    "        route = _route_query(query)\n",
    "\n",
    "    text_hits: List[Document] = []\n",
    "    code_hits: List[Document] = []\n",
    "\n",
    "    # TEXT index\n",
    "    if route in (\"text\", \"both\"):\n",
    "        text_vs = _load_faiss(str(Path(base_dir) / \"text_index\"), model)\n",
    "        # fetch_k > k for MMR diversity but still cheap\n",
    "        text_hits = text_vs.max_marginal_relevance_search(query, k=k_text, fetch_k=min(4 * k_text, 64))\n",
    "\n",
    "    # CODE index\n",
    "    if route in (\"code\", \"both\"):\n",
    "        code_vs = _load_faiss(str(Path(base_dir) / \"code_index\"), model)\n",
    "        code_hits = code_vs.max_marginal_relevance_search(query, k=k_code, fetch_k=min(4 * k_code, 64))\n",
    "\n",
    "    # Merge + (optional) metadata filter\n",
    "    hits = text_hits + code_hits\n",
    "    if types_include:\n",
    "        hits = [d for d in hits if d.metadata.get(\"type\") in types_include]\n",
    "\n",
    "    # De-dup by a stable key\n",
    "    seen, deduped = set(), []\n",
    "    for d in hits:\n",
    "        key = (d.metadata.get(\"source\"), d.metadata.get(\"lines\"), d.page_content[:64])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(d)\n",
    "\n",
    "    # Quick vector scores for sort (convert FAISS distance-like to similarity)\n",
    "    # We'll re-embed the query once and score docs by inner product with doc vectors via FAISS again is heavy;\n",
    "    # instead, reuse similarity_search_with_score on a small candidate set: emulate by re-querying with a higher k\n",
    "    # OR simply keep MMR order. For speed, we keep MMR order unless BM25 is enabled.\n",
    "    ordered_docs = deduped\n",
    "\n",
    "    # Optional: light BM25 over the small union (simple lexical boost)\n",
    "    if use_bm25:\n",
    "        import math\n",
    "        import re as _re\n",
    "        def _tok(t): return _re.findall(r\"[A-Za-z0-9_]+\", t.lower())\n",
    "        q_terms = _tok(query)\n",
    "        df = {}\n",
    "        toks = []\n",
    "        for d in ordered_docs:\n",
    "            ts = _tok(d.page_content)\n",
    "            toks.append(ts)\n",
    "            for term in set(ts): df[term] = df.get(term, 0) + 1\n",
    "        N = max(1, len(ordered_docs))\n",
    "        idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1.0) for t in set(q_terms)}\n",
    "        k1, b = 1.5, 0.75\n",
    "        lens = [len(ts) for ts in toks]\n",
    "        avgdl = max(1, sum(lens) / len(lens))\n",
    "        scores = []\n",
    "        for d, ts, L in zip(ordered_docs, toks, lens):\n",
    "            tf = {}\n",
    "            for t in ts: tf[t] = tf.get(t, 0) + 1\n",
    "            s = 0.0\n",
    "            for t in q_terms:\n",
    "                f = tf.get(t, 0)\n",
    "                if not f: continue\n",
    "                denom = f + k1 * (1 - b + b * (L / avgdl))\n",
    "                s += idf.get(t, 0.0) * (f * (k1 + 1)) / (denom or 1.0)\n",
    "            scores.append((d, s))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        ordered_docs = [d for d, _ in scores]\n",
    "\n",
    "    # Cap results\n",
    "    ordered_docs = ordered_docs[:k_final]\n",
    "\n",
    "    # Attach a simple descending score for convenience (1..0)\n",
    "    out = [(d, 1.0 - i / max(1, len(ordered_docs))) for i, d in enumerate(ordered_docs)]\n",
    "    t1 = time.time()\n",
    "    # print(f\"[fast_search] route={route} hits={len(out)} in {(t1-t0)*1000:.1f} ms\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "357496cd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[233]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m results = \u001b[43mfast_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWe need to write system Architecture?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroute\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mboth\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mk_text\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_code\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_final\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_bm25\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#types_include={\"functiondef\",\"classdef\",\"asyncfunctiondef\",\"functiondef_docstring\"}\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[232]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mfast_search\u001b[39m\u001b[34m(query, base_dir, model, route, k_text, k_code, k_final, use_bm25, types_include)\u001b[39m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# TEXT index\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m route \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mboth\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     text_vs = \u001b[43m_load_faiss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext_index\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# fetch_k > k for MMR diversity but still cheap\u001b[39;00m\n\u001b[32m     61\u001b[39m     text_hits = text_vs.max_marginal_relevance_search(query, k=k_text, fetch_k=\u001b[38;5;28mmin\u001b[39m(\u001b[32m4\u001b[39m * k_text, \u001b[32m64\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[232]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36m_load_faiss\u001b[39m\u001b[34m(path, model)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize=\u001b[32m8\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_faiss\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, model: \u001b[38;5;28mstr\u001b[39m) -> FAISS:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     emb = \u001b[43m_get_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m FAISS.load_local(path, emb, allow_dangerous_deserialization=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[232]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36m_get_embedder\u001b[39m\u001b[34m(model)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m(maxsize=\u001b[32m2\u001b[39m)\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_embedder\u001b[39m(model: \u001b[38;5;28mstr\u001b[39m) -> OpenAIEmbeddings:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mOpenAIEmbeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:339\u001b[39m, in \u001b[36mOpenAIEmbeddings.validate_environment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    337\u001b[39m         \u001b[38;5;28mself\u001b[39m.http_async_client = httpx.AsyncClient(proxy=\u001b[38;5;28mself\u001b[39m.openai_proxy)\n\u001b[32m    338\u001b[39m     async_specific = {\u001b[33m\"\u001b[39m\u001b[33mhttp_client\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.http_async_client}\n\u001b[32m--> \u001b[39m\u001b[32m339\u001b[39m     \u001b[38;5;28mself\u001b[39m.async_client = \u001b[43mopenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAsyncOpenAI\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mclient_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43masync_specific\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m.embeddings\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\openai\\_client.py:439\u001b[39m, in \u001b[36mAsyncOpenAI.__init__\u001b[39m\u001b[34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[39m\n\u001b[32m    436\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    437\u001b[39m     base_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://api.openai.com/v1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m=\u001b[49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[38;5;28mself\u001b[39m._default_stream_cls = AsyncStream\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\openai\\_base_client.py:1367\u001b[39m, in \u001b[36mAsyncAPIClient.__init__\u001b[39m\u001b[34m(self, version, base_url, _strict_response_validation, max_retries, timeout, http_client, custom_headers, custom_query)\u001b[39m\n\u001b[32m   1353\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   1354\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid `http_client` argument; Expected an instance of `httpx.AsyncClient` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1355\u001b[39m     )\n\u001b[32m   1357\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m   1358\u001b[39m     version=version,\n\u001b[32m   1359\u001b[39m     base_url=base_url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1365\u001b[39m     _strict_response_validation=_strict_response_validation,\n\u001b[32m   1366\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1367\u001b[39m \u001b[38;5;28mself\u001b[39m._client = http_client \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mAsyncHttpxClientWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;49;00m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\openai\\_base_client.py:1296\u001b[39m, in \u001b[36m_DefaultAsyncHttpxClient.__init__\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m   1294\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mlimits\u001b[39m\u001b[33m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[32m   1295\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mfollow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1296\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\httpx\\_client.py:1402\u001b[39m, in \u001b[36mAsyncClient.__init__\u001b[39m\u001b[34m(self, auth, params, headers, cookies, verify, cert, http1, http2, proxy, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, trust_env, default_encoding)\u001b[39m\n\u001b[32m   1399\u001b[39m allow_env_proxies = trust_env \u001b[38;5;129;01mand\u001b[39;00m transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1400\u001b[39m proxy_map = \u001b[38;5;28mself\u001b[39m._get_proxy_map(proxy, allow_env_proxies)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28mself\u001b[39m._transport = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_init_transport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1404\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransport\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1410\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1412\u001b[39m \u001b[38;5;28mself\u001b[39m._mounts: \u001b[38;5;28mdict\u001b[39m[URLPattern, AsyncBaseTransport | \u001b[38;5;28;01mNone\u001b[39;00m] = {\n\u001b[32m   1413\u001b[39m     URLPattern(key): \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1424\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, proxy \u001b[38;5;129;01min\u001b[39;00m proxy_map.items()\n\u001b[32m   1425\u001b[39m }\n\u001b[32m   1426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mounts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\httpx\\_client.py:1445\u001b[39m, in \u001b[36mAsyncClient._init_transport\u001b[39m\u001b[34m(self, verify, cert, trust_env, http1, http2, limits, transport)\u001b[39m\n\u001b[32m   1442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m transport \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1443\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m transport\n\u001b[32m-> \u001b[39m\u001b[32m1445\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mAsyncHTTPTransport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1447\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1450\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1452\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\httpx\\_transports\\default.py:297\u001b[39m, in \u001b[36mAsyncHTTPTransport.__init__\u001b[39m\u001b[34m(self, verify, cert, trust_env, http1, http2, limits, proxy, uds, local_address, retries, socket_options)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhttpcore\u001b[39;00m\n\u001b[32m    296\u001b[39m proxy = Proxy(url=proxy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxy, (\u001b[38;5;28mstr\u001b[39m, URL)) \u001b[38;5;28;01melse\u001b[39;00m proxy\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m ssl_context = \u001b[43mcreate_ssl_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    300\u001b[39m     \u001b[38;5;28mself\u001b[39m._pool = httpcore.AsyncConnectionPool(\n\u001b[32m    301\u001b[39m         ssl_context=ssl_context,\n\u001b[32m    302\u001b[39m         max_connections=limits.max_connections,\n\u001b[32m   (...)\u001b[39m\u001b[32m    310\u001b[39m         socket_options=socket_options,\n\u001b[32m    311\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\adars\\Documents\\UCD\\sem 3\\Automated Technical Documentation from Code Repositories\\auto_doc_agent\\Lib\\site-packages\\httpx\\_config.py:40\u001b[39m, in \u001b[36mcreate_ssl_context\u001b[39m\u001b[34m(verify, cert, trust_env)\u001b[39m\n\u001b[32m     37\u001b[39m         ctx = ssl.create_default_context(capath=os.environ[\u001b[33m\"\u001b[39m\u001b[33mSSL_CERT_DIR\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m         \u001b[38;5;66;03m# Default case...\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m         ctx = \u001b[43mssl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_default_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcertifi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m verify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[32m     42\u001b[39m     ctx = ssl.SSLContext(ssl.PROTOCOL_TLS_CLIENT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:717\u001b[39m, in \u001b[36mcreate_default_context\u001b[39m\u001b[34m(purpose, cafile, capath, cadata)\u001b[39m\n\u001b[32m    713\u001b[39m context.verify_flags |= (_ssl.VERIFY_X509_PARTIAL_CHAIN |\n\u001b[32m    714\u001b[39m                          _ssl.VERIFY_X509_STRICT)\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cafile \u001b[38;5;129;01mor\u001b[39;00m capath \u001b[38;5;129;01mor\u001b[39;00m cadata:\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m     \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m context.verify_mode != CERT_NONE:\n\u001b[32m    719\u001b[39m     \u001b[38;5;66;03m# no explicit cafile, capath or cadata but the verify mode is\u001b[39;00m\n\u001b[32m    720\u001b[39m     \u001b[38;5;66;03m# CERT_OPTIONAL or CERT_REQUIRED. Let's try to load default system\u001b[39;00m\n\u001b[32m    721\u001b[39m     \u001b[38;5;66;03m# root CA certificates for the given purpose. This may fail silently.\u001b[39;00m\n\u001b[32m    722\u001b[39m     context.load_default_certs(purpose)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "results = fast_search(\n",
    "    \"We need to write system Architecture?\",\n",
    "    route=\"both\",\n",
    "    k_text=5, k_code=15, k_final=20,\n",
    "    use_bm25=True\n",
    "    #types_include={\"functiondef\",\"classdef\",\"asyncfunctiondef\",\"functiondef_docstring\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a162a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='e1e8261f24bd43f2b2d46eb2c678ce92', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'anonymize', 'lines': '70-79'}, page_content='def anonymize(text):\\n    response = client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that removes personal identifiers from resumes.\"},\\n        {\"role\": \"user\", \"content\": f\"Anonymize this resume:\\\\n\\\\n{text}\"}\\n        ],\\n        temperature=0\\n        )\\n    return response.choices[0].message.content.strip()'),\n",
       "  1.0),\n",
       " (Document(id='f9875e2ef72b54a1eec4fa75c04dad0e', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'page_1', 'lines': '24-35'}, page_content='def page_1():\\n    st.title(\"Welcome to StepUpYourCareer.AI\")\\n    st.markdown(\"##### Let\\'s get started with a few details.\")\\n\\n    name = st.text_input(\"Your Full Name\")\\n    email = st.text_input(\"Your Email Address\")\\n\\n    if name and email:\\n        if st.button(\"➡️ Proceed to Resume Analysis\"):\\n            st.session_state.name = name\\n            st.session_state.email = email\\n            st.session_state.page = 2'),\n",
       "  0.95),\n",
       " (Document(id='f5ce9126a1503246b41002712fe03ba3', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\README.md', 'file_ext': 'text', 'type': 'readme', 'name': 'README.md', 'lines': '1-29'}, page_content='# StepUpYourCareer.ai: Elevate Your Future\\n\\nAn AI-powered career assistant that helps students and job seekers identify **skill gaps**, receive **personalized learning roadmaps**, and connect with **industry mentors**—all from a single resume upload.\\n\\n### Link to the website: https://stepupyourcareer.streamlit.app/\\n\\n---\\n\\n## Problem\\n\\nGraduates often leave university with degrees but **lack clarity on what employers actually expect**. They spend months applying for jobs, facing rejections without knowing **what skills they’re missing** or **how to upskill efficiently**.\\n\\n---\\n\\n## Solution\\n\\n**StepUpYourCareer.ai** transforms your resume into a personalized upskilling journey.\\n\\n- **Skill Gap Analyzer**: Extracts skills from your resume and compares them to your target role\\n- **Action Plan Generator**: Recommends curated online courses and resources for each missing skill\\n- **Mentor Matching**: Clusters users and mentors using K-Means to connect you with experts in your domain\\n\\n---\\n\\n![Notes_250516_042908_1](https://github.com/user-attachments/assets/a42216a8-e04c-4335-aad7-d56a61cc873b)\\n\\n![Notes_250516_042908_4](https://github.com/user-attachments/assets/ba3b34ec-57ec-4bf5-906a-84af82342b8b)\\n\\n![Notes_250516_042908_3](https://github.com/user-attachments/assets/4b7e5f11-7923-427c-be95-cbcd7d919c5b)'),\n",
       "  0.9),\n",
       " (Document(id='149c91a5673309ad4218479aa4fed4cd', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'generate_hybrid_action_plan', 'lines': '188-268'}, page_content='def generate_hybrid_action_plan(tech, soft, trans, skill_resources):\\n    # Split all three skill types\\n    tech_in, tech_out = split_skills_by_rag_presence(tech, skill_resources.keys())\\n    soft_in, soft_out = split_skills_by_rag_presence(soft, skill_resources.keys())\\n    trans_in, trans_out = split_skills_by_rag_presence(trans, skill_resources.keys())\\n\\n    #  Construct RAG results\\n    def extract_rag(skills):\\n        result = {}\\n        for s in skills:\\n            key = s.strip().upper()\\n            if key in skill_resources:\\n                    result[s] = skill_resources[key]\\n        return result\\n\\n    plan = {\\n            \"message\": \"Here\\'s a complete roadmap with relevant resources\",\\n            \"technical_skill_resources\": extract_rag(tech_in),\\n            \"soft_skill_resources\": extract_rag(soft_in),\\n            \"transferable_skill_resources\": extract_rag(trans_in)\\n    }\\n\\n    # Prepare GPT prompt only for uncovered skills\\n    if tech_out or soft_out or trans_out:\\n        prompt = f\"\"\"\\n        You are a career coach. Only generate resources for the following skills not found in our internal library.\\n\\n        Provide for each:\\n        - One **top-rated course** with real working URL.\\n        - One **real book** just name & author and AMAZON links for buying that book.\\n        - For soft/transferable skills, one article or video (with URL).\\n\\n        Format your response in JSON like this:\\n        {{\\n        \"technical_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}, {{\"title\": \"Book by Author\"}}]\\n        }},\\n        \"soft_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}]\\n        }},\\n        \"transferable_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}]\\n        }}\\n        }}\\n\\n        Only cover these:\\n        - TECHNICAL: {\\', \\'.join(tech_out)}\\n        - SOFT: {\\', \\'.join(soft_out)}\\n        - TRANSFERABLE: {\\', \\'.join(trans_out)}\\n\\n        You will be penalized if you confabulate or hallucinate by creating fake resources. It should be 100% authenthic.\\n        Double check every link/resource you give. If you don\\'t get any links leave that section blank.\\n        No explanations. No markdown. Only JSON.\\n        \"\"\"\\n        try:\\n            response = client.chat.completions.create(\\n            # Change to GPT 4 to get accurate links to resources\\n            model=\"gpt-4o-mini\",\\n            messages=[\\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant for learning.\"},\\n                    {\"role\": \"user\", \"content\": prompt}\\n                    ],\\n                    temperature=0.2\\n                    )\\n            raw = response.choices[0].message.content.strip()\\n            raw = re.sub(r\"```json|```\", \"\", raw)\\n            gpt_part = extract_json_from_response(raw)\\n\\n            if not isinstance(gpt_part, dict):\\n                gpt_part = {}\\n\\n            # Merge GPT results into RAG base\\n            for k in [\"technical_skill_resources\", \"soft_skill_resources\", \"transferable_skill_resources\"]:\\n                if k in gpt_part and isinstance(plan.get(k), dict):\\n                    plan[k].update(gpt_part[k])\\n\\n\\n        except Exception as e:\\n                st.error(f\"Error generating GPT fallback plan: {e}\")\\n\\n        return plan'),\n",
       "  0.85),\n",
       " (Document(id='340674d673bed04d34357675a7f3b401', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 10', 'lines': 'cell_10'}, page_content='k_val = np.arange(10, 30)\\nbest_k = None\\nbest_score = -1\\nshilloutte_scores = []\\ninertias = []\\nch_indexs = []\\n\\nfor k in k_val:\\n    temp_sil_scores = []\\n    temp_ch_scores = []\\n    temp_inertias = []\\n\\n    for run in range(100):  # 100 is heavy; reduce to 5 or 10 for dev\\n        kmeans = KMeans(n_clusters=k, random_state=run, n_init=\\'auto\\')\\n        labels = kmeans.fit_predict(skills_df)\\n\\n        sil_score = silhouette_score(skills_df, labels)\\n        ch_index = calinski_harabasz_score(skills_df, labels)\\n\\n        temp_sil_scores.append(sil_score)\\n        temp_ch_scores.append(ch_index)\\n        temp_inertias.append(kmeans.inertia_)\\n\\n    # Average over all runs\\n    avg_sil = np.mean(temp_sil_scores)\\n    avg_ch = np.mean(temp_ch_scores)\\n    avg_inertia = np.mean(temp_inertias)\\n\\n    shilloutte_scores.append(avg_sil)\\n    ch_indexs.append(avg_ch)\\n    inertias.append(avg_inertia)\\n\\n    if avg_sil > best_score:\\n        best_score = avg_sil\\n        best_k = k\\n\\nprint(f\"✅ Best K = {best_k} with silhouette score = {best_score:.4f}\")\\n\\n# 📈 Plot all 3 metrics\\nplt.figure(figsize=(15, 4))\\n\\nplt.subplot(1, 3, 1)\\nplt.plot(k_val, inertias, marker=\\'o\\')\\nplt.title(\"Elbow Plot (Inertia)\")\\nplt.xlabel(\"Number of Clusters (K)\")\\nplt.ylabel(\"Inertia\")\\nplt.grid(True)\\n\\nplt.subplot(1, 3, 2)\\nplt.plot(k_val, shilloutte_scores, marker=\\'s\\', color=\\'green\\')\\nplt.title(\"Silhouette Score\")\\nplt.xlabel(\"Number of Clusters (K)\")\\nplt.ylabel(\"Score\")\\nplt.grid(True)\\n\\nplt.subplot(1, 3, 3)\\nplt.plot(k_val, ch_indexs, marker=\\'^\\', color=\\'purple\\')\\nplt.title(\"Calinski-Harabasz Index\")\\nplt.xlabel(\"Number of Clusters (K)\")\\nplt.ylabel(\"Score\")\\nplt.grid(True)\\n\\nplt.tight_layout()\\nplt.show()'),\n",
       "  0.8),\n",
       " (Document(id='4d459d86cbefefeba8c27b826986e51f', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 23', 'lines': 'cell_23'}, page_content='### Action Plan Generation (Testing for 3 Candidates)'),\n",
       "  0.75),\n",
       " (Document(id='361599efbb48a25efa5383960f6b2a6d', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\requirements.txt', 'file_ext': 'code', 'type': '.txt', 'name': 'requirements.txt', 'lines': '1-10'}, page_content='streamlit\\nopenai>=1.0.0\\nPyPDF2\\npandas\\nscikit-learn\\ntqdm\\nnumpy\\npandas\\njoblib\\n'),\n",
       "  0.7),\n",
       " (Document(id='40a32048a94fd0f251b5de7636fd29ac', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 16', 'lines': 'cell_16'}, page_content='### Clustering happeining on skills cause within each role there are still overlapping clusters, means skills are common within each roles'),\n",
       "  0.65),\n",
       " (Document(id='76c182ffd558dcd47adebce2a432d400', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 12', 'lines': 'cell_12'}, page_content='### Printing 120 job descriptions for each role (Total 600)'),\n",
       "  0.6),\n",
       " (Document(id='830acbed675a5eefafac70dce4f80bb4', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 6', 'lines': 'cell_6'}, page_content='uploaded = files.upload()\\n\\nfor fn in uploaded.keys():\\n  print(\\'User uploaded file \"{name}\" with length {length} bytes\\'.format(\\n      name=fn, length=len(uploaded[fn])))'),\n",
       "  0.55),\n",
       " (Document(id='8e974020ba5587e950e3c11d340cab03', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 5', 'lines': 'cell_5'}, page_content='from google.colab import sheets\\nsheet = sheets.InteractiveSheet(df=req_jobs_desc)'),\n",
       "  0.5),\n",
       " (Document(id='bf02d13b5588300920ec28f91afa8557', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'get_embedding', 'lines': '60-62'}, page_content='def get_embedding(text, model=\"text-embedding-ada-002\"):\\n    response = client.embeddings.create(input=[text], model=model)\\n    return response.data[0].embedding'),\n",
       "  0.44999999999999996),\n",
       " (Document(id='5d378565627e395f0438a8e6bcb88864', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 5', 'lines': 'cell_5'}, page_content='roles = [\\n    \"Artificial Intelligence\",\\n    \"Business Analyst\",\\n    \"Business Intelligence Analyst\",\\n    \"Data Analyst\",\\n    \"Machine Learning\"\\n]'),\n",
       "  0.4),\n",
       " (Document(id='e3556391f84974e0fd86730a8ebd3175', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 9', 'lines': 'cell_9'}, page_content=\"sim_resumes.to_json('sim_resume.json', orient='records', lines=False)\"),\n",
       "  0.35),\n",
       " (Document(id='d49c82c071ad65a4a3382887f4ccab47', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 2', 'lines': 'cell_2'}, page_content=\"# !pip install PyPDF2 openai\\nimport openai\\nimport json\\nimport time\\nimport re\\nfrom openai import OpenAI\\nfrom google.colab import userdata\\nimport pandas as pd\\npd.set_option('display.max_colwidth', None)\\nfrom PyPDF2 import PdfReader\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nimport numpy as np\"),\n",
       "  0.30000000000000004),\n",
       " (Document(id='a81daf8339ca751b013aba36b96dda44', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'extract_text_from_pdf', 'lines': '65-67'}, page_content='def extract_text_from_pdf(uploaded_file):\\n    reader = PdfReader(uploaded_file)\\n    return \"\\\\n\".join([page.extract_text() or \"\" for page in reader.pages])'),\n",
       "  0.25),\n",
       " (Document(id='71ecc009e0bcc141741f7e867ebfd605', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'extract_json_from_response', 'lines': '164-171'}, page_content='def extract_json_from_response(raw):\\n    match = re.search(r\"\\\\{[\\\\s\\\\S]*\\\\}\", raw)\\n    if match:\\n        try:\\n            return json.loads(match.group())\\n        except json.JSONDecodeError:\\n            return {}\\n    return {}'),\n",
       "  0.19999999999999996),\n",
       " (Document(id='b698b49036f8cb30de5c2624d9bfdf0b', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 6', 'lines': 'cell_6'}, page_content='mentors_final_data = pd.read_json(\"generated_mentors.json\")\\nmentors_final_data.set_index(\"mentor_id\", inplace=True)\\nmentors_final_data.head(5)'),\n",
       "  0.15000000000000002),\n",
       " (Document(id='31fc24533d4226a14289bce237417ed4', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 22', 'lines': 'cell_22'}, page_content='from google.colab import files\\n\\nuploaded = files.upload()\\npdf_path = list(uploaded.keys())[0]\\n\\ntarget_role = input(\"Enter your target role (e.g., \\'AI Engineer\\'): \").strip()\\n\\noutput = analyze_uploaded_resume(pdf_path, target_role)\\n\\nprint(json.dumps(output, indent=2))'),\n",
       "  0.09999999999999998),\n",
       " (Document(id='2680a4b2e3aa1fe613cc064c08cc0fc8', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'retrieve_examples', 'lines': '82-86'}, page_content='def retrieve_examples(query, embeddings, examples, k=3):\\n    query_emb = get_embedding(query)\\n    sims = sk_cosine([query_emb], embeddings)[0]\\n    top_k = sims.argsort()[-k:][::-1]\\n    return [examples[i] for i in top_k]'),\n",
       "  0.050000000000000044)]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fast_search(\n",
    "    \"We need to write system Architecture?\",\n",
    "    route=\"both\",\n",
    "    k_text=5, k_code=15, k_final=20,\n",
    "    use_bm25=True\n",
    "    #types_include={\"functiondef\",\"classdef\",\"asyncfunctiondef\",\"functiondef_docstring\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "8a588907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='bf02d13b5588300920ec28f91afa8557', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'get_embedding', 'lines': '60-62'}, page_content='def get_embedding(text, model=\"text-embedding-ada-002\"):\\n    response = client.embeddings.create(input=[text], model=model)\\n    return response.data[0].embedding'),\n",
       "  1.0),\n",
       " (Document(id='71c1f532f43a5b6ecde647d5f76e92e2', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 0', 'lines': 'cell_0'}, page_content='<a href=\"https://colab.research.google.com/github/adarshlearnngrow/StepUp-AI/blob/main/Job_Description_JD_Manupulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>'),\n",
       "  0.8333333333333334),\n",
       " (Document(id='c8c5bc244d0845d64bc7cc4f92990aa1', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 9', 'lines': 'cell_9'}, page_content='### Fitting a KMean model with range of cluster k and ran for 100 times to be certain.'),\n",
       "  0.6666666666666667),\n",
       " (Document(id='4d459d86cbefefeba8c27b826986e51f', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 23', 'lines': 'cell_23'}, page_content='### Action Plan Generation (Testing for 3 Candidates)'),\n",
       "  0.5),\n",
       " (Document(id='bdeba62b5fa1e970c4c807b1d4ed6e95', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 16', 'lines': 'cell_16'}, page_content='### File containing all common skills required for a particular role'),\n",
       "  0.33333333333333337),\n",
       " (Document(id='bf01177fb1999c14dfc987dfb49265cc', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\requirements.txt', 'file_ext': 'txt', 'type': 'txt', 'name': 'requirements.txt', 'lines': '1-9'}, page_content='streamlit\\nopenai>=1.0.0\\nPyPDF2\\npandas\\nscikit-learn\\ntqdm\\nnumpy\\npandas\\njoblib'),\n",
       "  0.16666666666666663)]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = fast_search(\n",
    "    \"DAta\",\n",
    "    route=\"both\",\n",
    "    k_text=5, k_code=5, k_final=6,\n",
    "    use_bm25=True\n",
    "    #types_include={\"functiondef\",\"classdef\",\"asyncfunctiondef\",\"functiondef_docstring\"}\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174efe0",
   "metadata": {},
   "source": [
    "## Fallback option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "55285b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Initialize client once\n",
    "client = OpenAI()\n",
    "\n",
    "# 1️⃣ Define your fixed label list\n",
    "LAYOUT = [\n",
    "    \"Project Overview\",\n",
    "    \"Objective & Scope\",\n",
    "    \"System Architecture\",\n",
    "    \"Tech Stack\",\n",
    "    \"Installation & Setup\",\n",
    "    \"Usage Instructions\",\n",
    "    \"API Documentation\",\n",
    "    \"Others\"\n",
    "]\n",
    "\n",
    "# 2️⃣ Define the function schema for OpenAI function calling\n",
    "FUNCTIONS = [\n",
    "    {\n",
    "        \"name\": \"classify_chunk\",\n",
    "        \"description\": \"Assigns documentation section labels to a chunk.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sections\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": LAYOUT\n",
    "                    },\n",
    "                    \"minItems\": 1\n",
    "                },\n",
    "                \"rationale\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"1–3 sentence explanation of why those labels were chosen\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"sections\", \"rationale\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# 3️⃣ Few-shot examples (including a multi-label example)\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"code\": \"def main():\\n    load_config()\\n    train_model()\\n    save_model()\\n\",\n",
    "        \"label\": [\"System Architecture\", \"Usage Instructions\"],\n",
    "        \"rationale\": (\n",
    "            \"Defines the core training pipeline, showing both the \"\n",
    "            \"system’s structure and how to invoke it.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"@app.get('/predict')\\ndef predict(user_id: int):\\n    return model.predict(user_id)\\n\",\n",
    "        \"label\": [\"API Documentation\", \"Usage Instructions\"],\n",
    "        \"rationale\": \"Defines an API endpoint and shows how to call it.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"# pip install -r requirements.txt\\n\",\n",
    "        \"label\": [\"Installation & Setup\"],\n",
    "        \"rationale\": \"Provides installation instructions via pip.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": '\"\"\"This project automates resume analysis using GPT.\"\"\"',\n",
    "        \"label\": [\"Project Overview\"],\n",
    "        \"rationale\": \"Gives a concise summary of the project’s purpose.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"class ServiceRegistry:\\n    \\\"\\\"\\\"Manages service instances and their lifecycles\\\"\\\"\\\"\\n    def __init__(self):\\n        self._services = {}\\n\\n    def register(self, name, svc):\\n        self._services[name] = svc\\n\\n    def get(self, name):\\n        return self._services[name]\\n\",\n",
    "        \"label\": [\"System Architecture\"],\n",
    "        \"rationale\": \"Encapsulates the core registry for services, illustrating a central architectural component.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"from queue import Queue\\n\\npipeline = Queue()\\n\\ndef enqueue_task(task):\\n    pipeline.put(task)\\n\\ndef process_pipeline():\\n    while not pipeline.empty():\\n        task = pipeline.get()\\n        handle(task)\\n\",\n",
    "        \"label\": [\"System Architecture\"],\n",
    "        \"rationale\": \"Implements the main task queue and processing loop, key parts of the system’s internal architecture.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def build_system_prompt() -> str:\n",
    "    layout_list = \"\\n\".join(f\"- {lbl}\" for lbl in LAYOUT)\n",
    "    examples = \"\\n\\n\".join(\n",
    "        f\"EXAMPLE CHUNK:\\n{ex['code'].strip()}\\n\"\n",
    "        f\"EXPECTED OUTPUT:\\n\"\n",
    "        f\"{{\\\"sections\\\": {ex['label']!r}, \\\"rationale\\\": {ex['rationale']!r}}}\"\n",
    "        for ex in FEW_SHOT_EXAMPLES\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "You are an expert technical writer. Classify each code or markdown chunk into one or more documentation sections.\n",
    "\n",
    "Valid labels (pick all that apply):\n",
    "{layout_list}\n",
    "\n",
    "❌ Do NOT invent new labels.\n",
    "✅ You may assign multiple labels—return *all* that apply.\n",
    "\n",
    "Return your answer by calling the function `classify_chunk` with valid JSON.\n",
    "\n",
    "Here are a few guiding examples:\n",
    "{examples}\n",
    "\n",
    "Now classify the next chunk.\n",
    "\"\"\".strip()\n",
    "\n",
    "def multi_label_chunk(text: str, model: str = \"gpt-3.5-turbo\") -> dict:\n",
    "    \"\"\"\n",
    "    Classify a code or markdown chunk into documentation sections.\n",
    "    Returns: {\"sections\": [...], \"rationale\": \"...\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": build_system_prompt()},\n",
    "                {\"role\": \"user\",   \"content\": text.strip()}\n",
    "            ],\n",
    "            functions=FUNCTIONS,\n",
    "            function_call={\"name\": \"classify_chunk\"},\n",
    "            temperature=0.2,\n",
    "            max_tokens=300\n",
    "        )\n",
    "\n",
    "        # Extract the function_call result (attribute access, not dict)\n",
    "        call = resp.choices[0].message.function_call\n",
    "        return json.loads(call.arguments)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ classification failed:\", e)\n",
    "        return {\n",
    "            \"sections\": [\"Others\"],\n",
    "            \"rationale\": \"Fallback due to error\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5d3c6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_chunks = []\n",
    "\n",
    "for doc in chunks[-10:-1]:\n",
    "    label = multi_label_chunk(doc.page_content)\n",
    "\n",
    "    if not label or not isinstance(label, dict):\n",
    "        label = {\n",
    "            \"sections\": [\"Others\"],\n",
    "            \"rationale\": \"Fallback – no label returned\",\n",
    "            \"tags\": []\n",
    "        }\n",
    "\n",
    "    doc.metadata[\"sections\"] = label.get(\"sections\", [\"Others\"])\n",
    "    doc.metadata[\"tags\"] = label.get(\"tags\", [])\n",
    "    doc.metadata[\"rationale\"] = label.get(\"rationale\", \"No rationale given\")\n",
    "\n",
    "    labelled_chunks.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "7a695899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'extract_text_from_pdf', 'lines': '65-67', 'sections': ['Tech Stack', 'Usage Instructions'], 'tags': [], 'rationale': 'The function extracts text from a PDF file using the PyMuPDF library, indicating the tech stack in use. Additionally, it provides instructions on how to use the function by joining the extracted text from each page.'}, page_content='def extract_text_from_pdf(uploaded_file):\\n    reader = PdfReader(uploaded_file)\\n    return \"\\\\n\".join([page.extract_text() or \"\" for page in reader.pages])'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'anonymize', 'lines': '70-79', 'sections': ['API Documentation', 'Usage Instructions'], 'tags': [], 'rationale': 'Defines a function that anonymizes text using a chat completion API. The API documentation is provided through the function definition, and the usage instructions are shown in the function implementation.'}, page_content='def anonymize(text):\\n    response = client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that removes personal identifiers from resumes.\"},\\n        {\"role\": \"user\", \"content\": f\"Anonymize this resume:\\\\n\\\\n{text}\"}\\n        ],\\n        temperature=0\\n        )\\n    return response.choices[0].message.content.strip()'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'retrieve_examples', 'lines': '82-86', 'sections': ['Usage Instructions'], 'tags': [], 'rationale': 'This function retrieves examples based on a query using embeddings and cosine similarity. It is a utility function that users can call to get relevant examples.'}, page_content='def retrieve_examples(query, embeddings, examples, k=3):\\n    query_emb = get_embedding(query)\\n    sims = sk_cosine([query_emb], embeddings)[0]\\n    top_k = sims.argsort()[-k:][::-1]\\n    return [examples[i] for i in top_k]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'generate_skill_gap', 'lines': '89-129', 'sections': ['Usage Instructions'], 'tags': [], 'rationale': 'Provides a function that generates skill gap analysis based on input resume text, target role, retrieved examples, and fallback skills. The function guides the user on how to use it by providing a detailed prompt with examples, task description, and expected JSON output format.'}, page_content='def generate_skill_gap(resume_text, target_role, retrieved_examples, fallback_skills):\\n    examples_prompt = \"\\\\n\\\\n\".join([\\n    f\"Example for role {ex[\\'target_role\\']}:\\\\nResume: {ex[\\'resume_summary\\']}\\\\nSkill Gaps: tech={ex[\\'technical_skill_gap\\']}, soft={ex[\\'soft_skill_gap\\']}, transferable={ex[\\'transferable_skills\\']}\"\\n    for ex in retrieved_examples\\n        ])\\n    expected_tech = fallback_skills.get(\"technical_skills\", [])\\n    expected_soft = fallback_skills.get(\"soft_skills\", [])\\n\\n    prompt = f\"\"\"\\n        You are a highly experienced career advisor. Based on examples and required skills, identify skill gaps.\\n\\n        ## EXAMPLES\\n        {examples_prompt}\\n\\n        ## TASK:\\n\\n        Analyze the new resume below and return only JSON:\\n        {{\\n        \"technical_skill_gaps\": [list of missing technical skills],\\n        \"soft_skill_gaps\": [list of missing soft skills],\\n        \"transferable_skills\": [skills from the resume that help bridge gaps]\\n        }}\\n\\n        No explanation, no markdown, just clean JSON.\\n\\n        Target Role: {target_role}\\n        Required Technical Skills: {\\', \\'.join(expected_tech)}\\n        Required Soft Skills: {\\', \\'.join(expected_soft)}\\n        Resume:\\n        {resume_text}\\n        \"\"\"\\n\\n    response = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant for skill gap analysis.\"},\\n        {\"role\": \"user\", \"content\": prompt}],\\n        temperature=0.0\\n        )\\n    raw = response.choices[0].message.content.strip()\\n    raw = re.sub(r\"```json|```\", \"\", raw)\\n    return json.loads(raw)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'get_skill_priorities_from_gpt', 'lines': '131-160', 'sections': ['API Documentation', 'Others'], 'tags': [], 'rationale': 'This function defines an API endpoint for getting skill priorities from GPT and processes the response to return a JSON mapping of skills to importance scores. The function also includes handling for JSON decoding errors.'}, page_content='def get_skill_priorities_from_gpt(skills, role):\\n    prompt = f\"\"\"\\n    You are a career advisor. For the target role \\'{role}\\', prioritize the following skills based on the 80/20 (Pareto) principle.\\n\\n    Skills:\\n    {\\', \\'.join(skills)}\\n\\n    Return JSON mapping each skill to an importance score from 1 to 100 (higher means more important). Output should look like:\\n    {{\\n      \"Skill1\": 95,\\n      \"Skill2\": 90,\\n      ...\\n    }}\\n\\n    No explanation. No markdown. Only JSON.\\n    \"\"\"\\n    response = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that ranks skills by importance.\"},\\n            {\"role\": \"user\", \"content\": prompt}\\n        ],\\n        temperature=0.2\\n    )\\n    raw = response.choices[0].message.content.strip()\\n    raw = re.sub(r\"```json|```\", \"\", raw)\\n    try:\\n        return json.loads(raw)\\n    except json.JSONDecodeError:\\n        return {}'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'extract_json_from_response', 'lines': '164-171', 'sections': ['Tech Stack', 'Usage Instructions'], 'tags': [], 'rationale': 'The function involves JSON parsing, indicating the tech stack includes JSON manipulation. It also provides instructions on how to use the function.'}, page_content='def extract_json_from_response(raw):\\n    match = re.search(r\"\\\\{[\\\\s\\\\S]*\\\\}\", raw)\\n    if match:\\n        try:\\n            return json.loads(match.group())\\n        except json.JSONDecodeError:\\n            return {}\\n    return {}'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'load_skill_resources', 'lines': '173-175', 'sections': ['Others'], 'tags': [], 'rationale': 'This chunk loads skill resources from a JSON file, which is not directly related to any specific documentation section like project overview, system architecture, or usage instructions.'}, page_content='def load_skill_resources():\\n    with open(\"/mount/src/stepupyourcareer.ai/StepUpAI/skill_resource_mapping.json\", \"r\") as f:\\n        return json.load(f)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'split_skills_by_rag_presence', 'lines': '178-186', 'sections': ['Objective & Scope'], 'tags': [], 'rationale': 'This function is responsible for splitting a list of skills based on their presence in a specified set of keys, indicating the objective and scope of the functionality.'}, page_content='def split_skills_by_rag_presence(skills, rag_skill_keys):\\n    present = []\\n    missing = []\\n    for skill in skills:\\n        if skill.strip().upper() in rag_skill_keys:\\n            present.append(skill)\\n        else:\\n            missing.append(skill)\\n    return present, missing'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5f09yr6y\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'generate_hybrid_action_plan', 'lines': '188-268', 'sections': ['System Architecture', 'Usage Instructions'], 'tags': [], 'rationale': 'Defines a function that generates a hybrid action plan by splitting skills, constructing RAG results, preparing a GPT prompt for uncovered skills, and merging GPT results into the base plan. Additionally, it includes error handling for generating the GPT fallback plan.'}, page_content='def generate_hybrid_action_plan(tech, soft, trans, skill_resources):\\n    # Split all three skill types\\n    tech_in, tech_out = split_skills_by_rag_presence(tech, skill_resources.keys())\\n    soft_in, soft_out = split_skills_by_rag_presence(soft, skill_resources.keys())\\n    trans_in, trans_out = split_skills_by_rag_presence(trans, skill_resources.keys())\\n\\n    #  Construct RAG results\\n    def extract_rag(skills):\\n        result = {}\\n        for s in skills:\\n            key = s.strip().upper()\\n            if key in skill_resources:\\n                    result[s] = skill_resources[key]\\n        return result\\n\\n    plan = {\\n            \"message\": \"Here\\'s a complete roadmap with relevant resources\",\\n            \"technical_skill_resources\": extract_rag(tech_in),\\n            \"soft_skill_resources\": extract_rag(soft_in),\\n            \"transferable_skill_resources\": extract_rag(trans_in)\\n    }\\n\\n    # Prepare GPT prompt only for uncovered skills\\n    if tech_out or soft_out or trans_out:\\n        prompt = f\"\"\"\\n        You are a career coach. Only generate resources for the following skills not found in our internal library.\\n\\n        Provide for each:\\n        - One **top-rated course** with real working URL.\\n        - One **real book** just name & author and AMAZON links for buying that book.\\n        - For soft/transferable skills, one article or video (with URL).\\n\\n        Format your response in JSON like this:\\n        {{\\n        \"technical_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}, {{\"title\": \"Book by Author\"}}]\\n        }},\\n        \"soft_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}]\\n        }},\\n        \"transferable_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}]\\n        }}\\n        }}\\n\\n        Only cover these:\\n        - TECHNICAL: {\\', \\'.join(tech_out)}\\n        - SOFT: {\\', \\'.join(soft_out)}\\n        - TRANSFERABLE: {\\', \\'.join(trans_out)}\\n\\n        You will be penalized if you confabulate or hallucinate by creating fake resources. It should be 100% authenthic.\\n        Double check every link/resource you give. If you don\\'t get any links leave that section blank.\\n        No explanations. No markdown. Only JSON.\\n        \"\"\"\\n        try:\\n            response = client.chat.completions.create(\\n            # Change to GPT 4 to get accurate links to resources\\n            model=\"gpt-4o-mini\",\\n            messages=[\\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant for learning.\"},\\n                    {\"role\": \"user\", \"content\": prompt}\\n                    ],\\n                    temperature=0.2\\n                    )\\n            raw = response.choices[0].message.content.strip()\\n            raw = re.sub(r\"```json|```\", \"\", raw)\\n            gpt_part = extract_json_from_response(raw)\\n\\n            if not isinstance(gpt_part, dict):\\n                gpt_part = {}\\n\\n            # Merge GPT results into RAG base\\n            for k in [\"technical_skill_resources\", \"soft_skill_resources\", \"transferable_skill_resources\"]:\\n                if k in gpt_part and isinstance(plan.get(k), dict):\\n                    plan[k].update(gpt_part[k])\\n\\n\\n        except Exception as e:\\n                st.error(f\"Error generating GPT fallback plan: {e}\")\\n\\n        return plan')]"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tech Stack', 'Usage Instructions']\n",
      "['API Documentation', 'Usage Instructions']\n",
      "['Usage Instructions']\n",
      "['Usage Instructions']\n",
      "['API Documentation', 'Others']\n",
      "['Tech Stack', 'Usage Instructions']\n",
      "['Others']\n",
      "['Objective & Scope']\n",
      "['System Architecture', 'Usage Instructions']\n"
     ]
    }
   ],
   "source": [
    "for doc in labelled_chunks:\n",
    "    print(doc.metadata[\"sections\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbcad65",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f6aae",
   "metadata": {},
   "source": [
    "# Agent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d3e4e5",
   "metadata": {},
   "source": [
    "Lang Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "851ae9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = (\n",
    "    \"Allowed sources:\\n\"\n",
    "    \"1) Retrieved repository content.\\n\"\n",
    "    \"2) Logical inferences ONLY if repo lacks it.\\n\"\n",
    "    \"3) Optional contextual/literature knowledge (must be marked).\\n\\n\"\n",
    "    \"Citation rules (MUST end every sentence with exactly one):\\n\"\n",
    "    \"- Summarising/rephrasing repo info → [the <file_name>:<line_number>]\\n\"\n",
    "    \"- New detail logically deduced → (Inferred from LLM based on repository content) \"\n",
    "    \"[the <file_name>:<line_number>, or multiple file names comma-separated]\\n\"\n",
    "    \"- External/contextual knowledge → (Included from contextual knowledge, not from repository)\\n\"\n",
    "    \"- Missing info → (Information not available in repository)\\n\\n\"\n",
    "    \"Important:\\n\"\n",
    "    \"- Do NOT mark summarised repo content as inferred.\\n\"\n",
    "    \"- Use ONLY base filenames present in CONTEXT (no directories).\\n\"\n",
    "    \"- No punctuation after the closing bracket/parenthesis.\\n\"\n",
    "    \"- Mermaid code blocks are allowed without tags inside; add a one-line caption after the block with a proper tag.\\n\"\n",
    "    \"- Be concise and factual.\"\n",
    ")\n",
    "\n",
    "examples = (\n",
    "    \"Examples:\\n\"\n",
    "    \"The project exposes a command-line interface for simulations [the README.md:1-80]\\n\"\n",
    "    \"The GameController mediates between the Dealer and the Gambler \"\n",
    "    \"(Inferred from LLM based on repository content) [the game_controller.py, configuration.py]\\n\"\n",
    "    \"Integration with external monitoring is not described (Information not available in repository)\\n\"\n",
    "    \"FastAPI is a popular Python web framework (Included from contextual knowledge, not from repository)\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "12867a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# knobs you can tweak\n",
    "PER_FILE_CAP = 2                          # max chunks per file (except overrides below)\n",
    "ALLOW_OVERRIDE = {\"README.md\": 3, \"requirements.txt\": 1}\n",
    "MAX_CONTEXT_CHARS = 12_000                # keep context reasonable for the writer\n",
    "SNIPPET_CHARS = 1000                      # trim each chunk\n",
    "\n",
    "from collections import defaultdict\n",
    "import re\n",
    "\n",
    "# knobs\n",
    "PER_FILE_CAP = 2\n",
    "MAX_CONTEXT_CHARS = 12_000\n",
    "SNIPPET_CHARS = 900\n",
    "\n",
    "# simple boost for “high-signal” code files/chunks\n",
    "_CODE_NAME_BOOST = re.compile(r\"(controller|orchestrator|router|route|handler|service|pipeline|workflow|app|main|setup|config|settings)\", re.I)\n",
    "_DOC_TYPES = {\"module_docstring\",\"classdef_docstring\",\"functiondef_docstring\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea49ee",
   "metadata": {},
   "source": [
    "## First langgraph without Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# section_runner.py\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Literal, TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "BASE_DIR    = \"docs_index\"                 # text_index/ and code_index/ live here\n",
    "EMBED_MODEL = \"text-embedding-3-small\"     # must match what you built FAISS with\n",
    "GEN_MODEL   = \"gpt-4o-mini\"                # your chat model\n",
    "\n",
    "@dataclass\n",
    "class SectionSpec:\n",
    "    name: str\n",
    "    query: str\n",
    "    route: Literal[\"text\", \"both\"] = \"text\"   # \"text\" or \"both (text+code)\"\n",
    "    k_text: int = 5\n",
    "    k_code: int = 15\n",
    "    guidance: str = \"\"                        # optional writing hints\n",
    "    additional_context: str = \"\"              # optional additional context\n",
    "\n",
    "class State(TypedDict, total=False):\n",
    "    spec: SectionSpec\n",
    "    context: str\n",
    "    draft: str\n",
    "    out_path: str\n",
    "'''\n",
    "def _retrieve(spec: SectionSpec) -> str:\n",
    "    \"\"\"Simple, per-section retrieval with optional code.\"\"\"\n",
    "    emb = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "    parts: List[str] = []\n",
    "\n",
    "    # text index (always)\n",
    "    text_vs = FAISS.load_local(f\"{BASE_DIR}/text_index\", emb, allow_dangerous_deserialization=True)\n",
    "    t_hits = text_vs.max_marginal_relevance_search(spec.query, k=spec.k_text, fetch_k=min(64, 4*spec.k_text))\n",
    "    for d in t_hits:\n",
    "        src = Path(d.metadata.get(\"source\",\"\")).name\n",
    "        loc = d.metadata.get(\"lines\",\"\")\n",
    "        parts.append(f\"[{src}:{loc}] {d.page_content[:1200]}\")\n",
    "\n",
    "    # code index (optional)\n",
    "    if spec.route == \"both\":\n",
    "        try:\n",
    "            code_vs = FAISS.load_local(f\"{BASE_DIR}/code_index\", emb, allow_dangerous_deserialization=True)\n",
    "            c_hits = code_vs.max_marginal_relevance_search(spec.query, k=spec.k_code, fetch_k=min(64, 4*spec.k_code))\n",
    "            # de-dup by first 64 chars\n",
    "            seen = {p[:64] for p in parts}\n",
    "            for d in c_hits:\n",
    "                snippet = d.page_content[:1200]\n",
    "                if snippet[:64] in seen: \n",
    "                    continue\n",
    "                seen.add(snippet[:64])\n",
    "                src = Path(d.metadata.get(\"source\",\"\")).name\n",
    "                loc = d.metadata.get(\"lines\",\"\")\n",
    "                parts.append(f\"[{src}:{loc}] {snippet}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return \"\\n\\n\".join(parts)\n",
    "'''\n",
    "\n",
    "def _score_code_hit(d) -> int:\n",
    "    src = Path(d.metadata.get(\"source\",\"\")).name\n",
    "    t   = (d.metadata.get(\"type\") or \"\").lower()\n",
    "    s = 0\n",
    "    if src.endswith(\".py\"): s += 1\n",
    "    if _CODE_NAME_BOOST.search(src): s += 2\n",
    "    if t in _DOC_TYPES: s += 3          # prefer docstrings (high info density)\n",
    "    return s\n",
    "\n",
    "def _retrieve(spec: SectionSpec) -> str:\n",
    "    \"\"\"Code-first retrieval (when route='both'), per-file cap, char budget, strict [the file:lines] tags.\"\"\"\n",
    "    emb = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "    parts: List[str] = []\n",
    "    per_file = defaultdict(int)\n",
    "    seen = set()               # (src, lines)\n",
    "    total = 0\n",
    "\n",
    "    def cap_for(src: str) -> int:\n",
    "        if src == \"README.md\": return 1\n",
    "        if src == \"requirements.txt\": return 1\n",
    "        # allow code files a bit more room\n",
    "        if src.endswith(\".py\"): return 3\n",
    "        return PER_FILE_CAP\n",
    "\n",
    "    def try_add(d):\n",
    "        nonlocal total\n",
    "        src = Path(d.metadata.get(\"source\",\"\")).name\n",
    "        loc = d.metadata.get(\"lines\",\"\")\n",
    "        key = (src, loc)\n",
    "        if key in seen or per_file[src] >= cap_for(src): \n",
    "            return\n",
    "        snippet = (d.page_content or \"\")[:SNIPPET_CHARS]\n",
    "        entry = f\"[the {src}:{loc}] {snippet}\"\n",
    "        if total + len(entry) > MAX_CONTEXT_CHARS:\n",
    "            return\n",
    "        parts.append(entry)\n",
    "        seen.add(key)\n",
    "        per_file[src] += 1\n",
    "        total += len(entry)\n",
    "\n",
    "    # load stores\n",
    "    text_vs = FAISS.load_local(f\"{BASE_DIR}/text_index\", emb, allow_dangerous_deserialization=True)\n",
    "\n",
    "    # --- CODE FIRST (only if asked for both) ---\n",
    "    if spec.route == \"both\":\n",
    "        try:\n",
    "            code_vs = FAISS.load_local(f\"{BASE_DIR}/code_index\", emb, allow_dangerous_deserialization=True)\n",
    "            c_hits = code_vs.max_marginal_relevance_search(spec.query, k=spec.k_code, fetch_k=min(64, 4*spec.k_code))\n",
    "            # prioritize by our simple score\n",
    "            c_hits.sort(key=_score_code_hit, reverse=True)\n",
    "            for d in c_hits:\n",
    "                try_add(d)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # --- TEXT afterwards (small number to frame purpose) ---\n",
    "    t_hits = text_vs.max_marginal_relevance_search(spec.query, k=spec.k_text, fetch_k=min(64, 4*spec.k_text))\n",
    "    for d in t_hits:\n",
    "        try_add(d)\n",
    "\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----- LangGraph nodes -----\n",
    "def n_retrieve(state: State) -> State:\n",
    "    ctx = _retrieve(state[\"spec\"])\n",
    "    Path(\"debug\").mkdir(exist_ok=True)\n",
    "    Path(\"debug/context.txt\").write_text(ctx, encoding=\"utf-8\")\n",
    "    return {\"context\": ctx}\n",
    "\n",
    "def n_write(state: State) -> State:\n",
    "    spec = state[\"spec\"]\n",
    "    \n",
    "    llm = ChatOpenAI(model=GEN_MODEL, temperature=0)\n",
    "    sys = (\n",
    "        \"You are a senior technical writer. Use ONLY the provided CONTEXT block (and additional_context if present) \"\n",
    "        \"and follow the citation rules exactly.\"\n",
    "    )\n",
    "\n",
    "    usr = (\n",
    "        f\"SECTION: {spec.name}\\n\"\n",
    "        f\"GOAL: {spec.query}\\n\"\n",
    "        f\"GUIDANCE: {spec.guidance}\\n\\n\"\n",
    "        f\"RULES:\\n{rules}\\n\\n{examples}\\n\"\n",
    "        f\"CONTEXT:\\n{state['context']}\\n\\n\"\n",
    "        \"CONTEXT (snippets already include tags like [the <file>:<lines>] so you can reuse filenames):\\n\"\n",
    "        f\"{state['context']}\\n\\n\"\n",
    "        f\"ADDITIONAL CONTEXT:\\n{getattr(spec, 'additional_context', '')}\\n\\n\"\n",
    "        f\"Write the '{spec.name}' section in clear Markdown.\"\n",
    "    )\n",
    "    md = llm.invoke([{\"role\":\"system\",\"content\":sys},{\"role\":\"user\",\"content\":usr}]).content.strip()\n",
    "    return {\"draft\": md}\n",
    "\n",
    "def n_save(state: State) -> State:\n",
    "    spec = state[\"spec\"]\n",
    "    out = f\"# {spec.name}\\n\\n{state['draft']}\\n\"\n",
    "    Path(\"docs\").mkdir(exist_ok=True)\n",
    "    fname = spec.name.lower().replace(\"&\",\"and\").replace(\" \", \"_\") + \".md\"\n",
    "    path = f\"docs/{fname}\"\n",
    "    Path(path).write_text(out, encoding=\"utf-8\")\n",
    "    return {\"out_path\": path}\n",
    "\n",
    "def build_graph():\n",
    "    g = StateGraph(State)\n",
    "    g.add_node(\"retrieve\", n_retrieve)\n",
    "    g.add_node(\"write\", n_write)\n",
    "    g.add_node(\"judge\", n_judge)\n",
    "    g.add_node(\"revise\", n_revise)\n",
    "    g.add_node(\"human\", n_human_review)  # if you support human mode\n",
    "    g.add_node(\"save\", n_save)\n",
    "\n",
    "    g.set_entry_point(\"retrieve\")\n",
    "    g.add_edge(\"retrieve\", \"write\")\n",
    "    g.add_edge(\"write\", \"judge\")\n",
    "    g.add_conditional_edges(\"judge\", decide_pass_or_revise, {\n",
    "        \"revise\": \"revise\",\n",
    "        \"human\": \"human\",\n",
    "        \"save\": \"save\",\n",
    "    })\n",
    "    g.add_edge(\"revise\", \"judge\")   # ← loop back to judge\n",
    "    g.add_conditional_edges(\"human\", decide_pass_or_revise, {\"revise\":\"revise\",\"save\":\"save\"})\n",
    "    return g.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d28a401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: docs/overview.md\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = build_graph()\n",
    "\n",
    "    \"\"\"\n",
    "    # Example: Overview (text only)\n",
    "    overview = SectionSpec(\n",
    "        name=\"Overview\",\n",
    "        query=\"High-level purpose, business value, scope, key features, main modules/components, audience and benefits.\",\n",
    "        route=\"both\",\n",
    "        k_text=5,\n",
    "        guidance=\"2–3 short paragraphs; avoid code-level details.\"\n",
    "    )\n",
    "   \"\"\"\n",
    "    # Example: Objective & Scope (text only, your layout)\n",
    "    obj_scope = SectionSpec(\n",
    "        name=\"Objective & Scope\",\n",
    "        query=\"Project goals/objectives and scope or limitations as described in README and docstrings.\",\n",
    "        route=\"both\",\n",
    "        k_text=12,\n",
    "        guidance=\"Include '### Goals' bullets and '### Out of Scope' bullets.\"\n",
    "    )\n",
    "\n",
    "    print(\"Wrote:\", app.invoke({\"spec\": overview})[\"out_path\"])\n",
    "    # Uncomment to run this one too:\n",
    "    # print(\"Wrote:\", app.invoke({\"spec\": obj_scope})[\"out_path\"])\n",
    "    \n",
    "    # Uncomment to run this one too:\n",
    "    # print(\"Wrote:\", app.invoke({\"spec\": obj_scope})[\"out_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "30068903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: docs/system_architecture.md\n"
     ]
    }
   ],
   "source": [
    "app = build_graph()\n",
    "architecture = SectionSpec(\n",
    "        name=\"System Architecture\",\n",
    "        query=\"Architecture overview of the project: high-level system architecture and component responsibilities,\",\n",
    "        route=\"both\",\n",
    "        k_text=10,\n",
    "        k_code=20,\n",
    "        guidance=\"Focus on the bigger as well as smaller picture.\",\n",
    "        additional_context=\"\"\"\"\n",
    "You are helping write the **System Architecture** section for a technical project.\n",
    "\n",
    "You will be given:\n",
    "- Project name & brief description — What the system does.\n",
    "- Key goals — What it is designed to achieve.\n",
    "- Key technologies — Languages, frameworks, tools, services.\n",
    "- Any special constraints — e.g., latency, security, budget.\n",
    "- Retrieved repository content tagged as architecture, diagrams, component descriptions, configuration files, and tech stack details, requirement.txt.\n",
    "\n",
    "Output format (only include what is available or inferred):\n",
    "\n",
    "1. **System Architecture Diagram (Mermaid)**\n",
    "   - Use `flowchart TD` or `graph LR`.\n",
    "   - Include the flow of the main applicaiton.\n",
    "   - Mark the title as Infered from the code.\n",
    "   - Mark missing elements as (Information not available in repository).\n",
    "\n",
    "2. **Key Components Table**\n",
    "   - Columns: Component | Responsibility | Tech/Options | Notes\n",
    "   - Include component which are acutally used, not everything.\n",
    "   - Only what is in the repo or inferred. Just take this from requirement.txt. Don't assume anything.\n",
    "\n",
    "3. **Data Flow Steps**\n",
    "   - Numbered list from input → processing → output.\n",
    "   - Mark missing or inferred steps clearly per rules.\n",
    "\n",
    "4. **Deployment View**\n",
    "   - Tell the entire thing for eg. Local dev setup, staging, production topology etc.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "5. **Scalability & Reliability**\n",
    "   - Only repo data or clearly marked inference.\n",
    "\n",
    "6. **Security & Compliance**\n",
    "   - Authentication, authorization, data protection, logging.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "7. **Trade-offs & Alternatives**\n",
    "   - Key design choices with pros/cons.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "8. **Assumptions & Constraints**\n",
    "   - Supported use cases, limits, boundaries.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "9. **Risks & Mitigations**\n",
    "   - Technical and operational risks with prevention/recovery strategies.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "10. **Observability & Quality**\n",
    "    - Metrics, tracing, alerts, testing approach.\n",
    "\n",
    "11. **Future Extensions**\n",
    "    - Possible evolutions, integrations, optimizations.\n",
    "    - Mark inferred items clearly per rules.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Wrote:\", app.invoke({\"spec\": architecture})[\"out_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0081b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: docs/technologies_used.md\n"
     ]
    }
   ],
   "source": [
    "app = build_graph()\n",
    "technologies = SectionSpec(\n",
    "        name=\"Technologies Used\",\n",
    "        query=\"Installation prerequisites and versions\",\n",
    "        route=\"both\",\n",
    "        k_text=5,\n",
    "        k_code=5,\n",
    "        guidance=\"\"\"\n",
    "        Just list the technologies used in a way like\n",
    "        Languages: Python, JavaScript\n",
    "        Frameworks: Flask, React\n",
    "        Packages: NumPy, Pandas\n",
    "        \"\"\",\n",
    "        additional_context=\"\"\n",
    ")\n",
    "\n",
    "#print(\"Retreive\", app.invoke({\"spec\": technologies}, print_mode=\"debug\"))\n",
    "print(\"Wrote:\", app.invoke({\"spec\": technologies})[\"out_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = build_graph()\n",
    "user_guide = SectionSpec(\n",
    "        name=\"Technologies Used\",\n",
    "        query=\"\",\n",
    "        route=\"both\",\n",
    "        k_text=5,\n",
    "        k_code=5,\n",
    "        guidance=\"\"\"\n",
    "        Just list the technologies used in a way like\n",
    "        Languages: Python, JavaScript\n",
    "        Frameworks: Flask, React\n",
    "        Packages: NumPy, Pandas\n",
    "        \"\"\",\n",
    "        additional_context=\"\"\n",
    ")\n",
    "\n",
    "#print(\"Retreive\", app.invoke({\"spec\": technologies}, print_mode=\"debug\"))\n",
    "print(\"Wrote:\", app.invoke({\"spec\": technologies})[\"out_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fe9f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "537f57e8",
   "metadata": {},
   "source": [
    "# Agent with judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "6d923371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict, total=False):\n",
    "    spec: SectionSpec\n",
    "    context: str\n",
    "    draft: str\n",
    "    out_path: str\n",
    "    # --- NEW: review plumbing ---\n",
    "    review_mode: Literal[\"none\", \"llm\", \"human\"]  # how to review before save\n",
    "    _judge: str            # JSON string from n_judge\n",
    "    _human_notes: str      # one-line notes from human review (optional)\n",
    "    retries: int           # how many revise→judge loops have run\n",
    "    max_retries: int       # cap to avoid infinite loop (e.g., 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SectionSpec:\n",
    "    name: str\n",
    "    query: str\n",
    "    route: Literal[\"text\", \"both\"] = \"text\"   # \"text\" or \"both (text+code)\"\n",
    "    k_text: int = 5\n",
    "    k_code: int = 15\n",
    "    guidance: str = \"\"                        # optional writing hints\n",
    "    additional_context: str = \"\"              # optional additional context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a081a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Guidance for the writer: strict citation rules + tiny examples ---\n",
    "rules = (\n",
    "    \"Allowed sources:\\n\"\n",
    "    \"1) Retrieved repository content.\\n\"\n",
    "    \"2) Logical inferences ONLY when repo doesn’t state the info.\\n\"\n",
    "    \"3) Optional contextual/literature knowledge (must be marked).\\n\\n\"\n",
    "    \"Citation rules (MUST end every sentence with exactly one):\\n\"\n",
    "    \"- Summarising/rephrasing repo info → [the <file_name>:<line_number>, or multiple file names comma-separated]\\n\"\n",
    "    \"- New detail logically deduced → (Inferred from LLM based on repository content) \"\n",
    "    \"[the <file_name>:<line_number>, or multiple file names comma-separated]\\n\"\n",
    "    \"- External/contextual knowledge → (Included from contextual knowledge, not from repository)\\n\"\n",
    "    \"- Missing info → (Information not available in repository)\\n\\n\"\n",
    "    \"Important:\\n\"\n",
    "    \"- Do NOT mark summarised repo content as inferred.\\n\"\n",
    "    \"- Use ONLY base filenames present in CONTEXT (no directories).\\n\"\n",
    "    \"- No punctuation after the closing bracket/parenthesis.\\n\"\n",
    "    \"- Mermaid code blocks can be untagged, but add a one-line caption WITH a tag after the block.\\n\"\n",
    "    \"- Be concise and factual.\"\n",
    ")\n",
    "\n",
    "examples = (\n",
    "    \"Examples:\\n\"\n",
    "    \"The project exposes a command-line interface for simulations [the README.md:1-80]\\n\"\n",
    "    \"The central controller orchestrates gameplay between dealer and gambler \"\n",
    "    \"(Inferred from LLM based on repository content) [the game_controller.py, configuration.py]\\n\"\n",
    "    \"External monitoring integration is not described (Information not available in repository)\\n\"\n",
    "    \"FastAPI is a Python web framework (Included from contextual knowledge, not from repository)\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "def n_write(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Generate a single documentation section from the retrieved CONTEXT.\n",
    "    The prompt enforces one-tag-per-sentence citation rules.\n",
    "    \"\"\"\n",
    "    spec = state[\"spec\"]\n",
    "    llm = ChatOpenAI(model=GEN_MODEL, temperature=0)\n",
    "\n",
    "    sys = (\n",
    "        \"You are a senior engineer who has wrote this code.\"\n",
    "        \"Now you are writing an industry level technical documentation.\" \n",
    "        \"Use ONLY the provided CONTEXT (and additional_context if present) \"\n",
    "        \"and follow the citation rules exactly. Also write the section professionally. No use of\"\n",
    "        \"likely, maybe, etc. Be concise and factual.\"\n",
    "    )\n",
    "\n",
    "    usr = (\n",
    "        f\"SECTION: {spec.name}\\n\"\n",
    "        f\"GOAL: {spec.query}\\n\"\n",
    "        f\"GUIDANCE: {spec.guidance}\\n\\n\"\n",
    "        f\"RULES:\\n{rules}\\n\\n{examples}\\n\"\n",
    "        \"CONTEXT (snippets already include tags like [the <file>:<lines>] so you can reuse filenames):\\n\"\n",
    "        f\"{state['context']}\\n\\n\"\n",
    "        f\"ADDITIONAL CONTEXT (optional):\\n{getattr(spec, 'additional_context', '')}\\n\\n\"\n",
    "        f\"Write the '{spec.name}' section in clear Markdown.\"\n",
    "    )\n",
    "\n",
    "    md = llm.invoke([{\"role\": \"system\", \"content\": sys},\n",
    "                     {\"role\": \"user\", \"content\": usr}]).content.strip()\n",
    "    return {\"draft\": md}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "464a5727",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_judge(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Grade the DRAFT strictly against CONTEXT.\n",
    "    Returns _judge as a JSON string with keys:\n",
    "      factual, cites_ok, hallucinated, unsupported_claims[], missing_but_expected[], score (0..1), notes\n",
    "    \"\"\"\n",
    "    spec, ctx, draft = state[\"spec\"], state[\"context\"], state[\"draft\"]\n",
    "    judge_llm = ChatOpenAI(model='gpt-4o-mini', temperature=0)\n",
    "\n",
    "    sys = (\n",
    "        \"You are a strict technical reviewer. Judge ONLY using CONTEXT. \"\n",
    "        \"Return STRICT JSON matching the schema. Looks for any contradictions, missing information, or unsupported claims, also if citation doesn't make sense.\"\n",
    "        \"For eg: The application can scale by deploying multiple instances of the Streamlit app to handle increased user load (Inferred from LLM based on repository content) [the requirements.txt:1-10].\"\n",
    "        \"If the claim is not supported by the context, return a score of 0 and a note explaining why.\"\n",
    "    )\n",
    "    usr = (\n",
    "        'Schema:\\n'\n",
    "        '{\"factual\":bool,\"cites_ok\":bool,\"hallucinated\":bool,'\n",
    "        '\"unsupported_claims\":[string],\"missing_but_expected\":[string],'\n",
    "        '\"score\":number,\"notes\":string, }\\n\\n'\n",
    "        f\"SECTION: {spec.name}\\n\\nCONTEXT:\\n{ctx}\\n\\nDRAFT:\\n{draft}\"\n",
    "    )\n",
    "\n",
    "    out = judge_llm.invoke([{\"role\": \"system\", \"content\": sys},\n",
    "                            {\"role\": \"user\", \"content\": usr}]).content\n",
    "\n",
    "    # Ensure valid JSON; if not, create a failing verdict so we trigger a revise pass\n",
    "    try:\n",
    "        json.loads(out)\n",
    "    except Exception:\n",
    "        out = ('{\"factual\": false, \"cites_ok\": false, \"hallucinated\": true, '\n",
    "               '\"unsupported_claims\":[\"Non-JSON judge output\"], '\n",
    "               '\"missing_but_expected\":[], \"score\": 0.0, \"notes\":\"Judge did not return JSON\"}')\n",
    "\n",
    "    Path(\"debug\").mkdir(exist_ok=True)\n",
    "    Path(\"debug/judge.json\").write_text(out, encoding=\"utf-8\")\n",
    "    return {\"_judge\": out}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fa30b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_human_review(state: State) -> State:\n",
    "    \"\"\"\n",
    "    Print the draft and ask a human for approval/notes in the console.\n",
    "    If notes are provided, we will run one revise pass.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- DRAFT PREVIEW ----------------------------------\\n\")\n",
    "    print(state[\"draft\"])\n",
    "    print(\"\\n----------------------------------------------------\")\n",
    "    ans = input(\"Approve this section? [y/N]: \").strip().lower()\n",
    "    notes = \"\"\n",
    "    if ans not in {\"y\", \"yes\"}:\n",
    "        print(\"Enter revision notes (single line; optional):\")\n",
    "        notes = input(\"> \").strip()\n",
    "    return {\"_human_notes\": notes or \"\"}\n",
    "\n",
    "\n",
    "def n_revise(state: State) -> State:\n",
    "    \"\"\"\n",
    "    One-shot auto-revision using either judge notes or human notes.\n",
    "    Keeps the SAME CONTEXT to avoid moving targets.\n",
    "    \"\"\"\n",
    "    spec, ctx, draft = state[\"spec\"], state[\"context\"], state[\"draft\"]\n",
    "    notes = state.get(\"_human_notes\", \"\")\n",
    "\n",
    "    # If no human notes, synthesize notes from judge JSON\n",
    "    if not notes:\n",
    "        try:\n",
    "            data = json.loads(state.get(\"_judge\", \"\") or \"{}\")\n",
    "        except Exception:\n",
    "            data = {}\n",
    "        issues = \"\\n\".join((data.get(\"unsupported_claims\") or []))\n",
    "        notes = (data.get(\"notes\", \"\") + (\"\\n\" + issues if issues else \"\")).strip()\n",
    "\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "    sys = \"Revise DRAFT to align strictly with CONTEXT. Remove/qualify unsupported claims. Add/adjust citations. Keep it concise.\"\n",
    "    usr = (\n",
    "        f\"SECTION: {spec.name}\\n\\nNOTES:\\n{notes}\\n\\n\"\n",
    "        f\"CONTEXT:\\n{ctx}\\n\\nDRAFT:\\n{draft}\\n\\n\"\n",
    "        \"Return the improved Markdown only.\"\n",
    "    )\n",
    "    fixed = llm.invoke([{\"role\": \"system\", \"content\": sys},\n",
    "                        {\"role\": \"user\", \"content\": usr}]).content.strip()\n",
    "    # increment retry counter\n",
    "    return {\"draft\": fixed, \"retries\": state.get(\"retries\", 0) + 1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e80bf1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_pass_or_revise(state: State):\n",
    "    \"\"\"\n",
    "    Route based on judge result (and human notes if present).\n",
    "    - If human left notes → revise once.\n",
    "    - If judge flags problems or low score → revise up to max_retries.\n",
    "    - Otherwise → save.\n",
    "    \"\"\"\n",
    "    # Human path: if notes exist, do exactly one revise pass\n",
    "    if state.get(\"_human_notes\"):\n",
    "        return \"revise\"\n",
    "\n",
    "    # LLM path\n",
    "    try:\n",
    "        data = json.loads(state.get(\"_judge\", \"\") or \"{}\")\n",
    "    except Exception:\n",
    "        return \"revise\"  # malformed judge → try revise once\n",
    "\n",
    "    score = float(data.get(\"score\", 0))\n",
    "    bad = (not data.get(\"factual\", True)) or data.get(\"hallucinated\", False) or (not data.get(\"cites_ok\", True))\n",
    "\n",
    "    if (not bad) and score >= 0.75:\n",
    "        return \"save\"\n",
    "\n",
    "    if state.get(\"retries\", 0) < state.get(\"max_retries\", 2):\n",
    "        return \"revise\"\n",
    "\n",
    "    # Out of retries; if you want to force manual review, return \"human\" here\n",
    "    return \"save\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac89146",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6b584",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_save(state: State) -> State:\n",
    "    spec = state[\"spec\"]\n",
    "    out = f\"# {spec.name}\\n\\n{state['draft']}\\n\"\n",
    "    Path(\"docs\").mkdir(exist_ok=True)\n",
    "    fname = spec.name.lower().replace(\"&\",\"and\").replace(\" \", \"_\") + \".md\"\n",
    "    path = f\"docs/{fname}\"\n",
    "    Path(path).write_text(out, encoding=\"utf-8\")\n",
    "    return {\"out_path\": path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "789f3c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph():\n",
    "    g = StateGraph(State)\n",
    "    g.add_node(\"retrieve\", n_retrieve)\n",
    "    g.add_node(\"write\", n_write)\n",
    "    g.add_node(\"judge\", n_judge)\n",
    "    g.add_node(\"revise\", n_revise)\n",
    "    g.add_node(\"save\", n_save)\n",
    "\n",
    "    g.set_entry_point(\"retrieve\")\n",
    "    g.add_edge(\"retrieve\", \"write\")\n",
    "    g.add_conditional_edges(\"write\", route_after_write, {\n",
    "        \"judge\": \"judge\",\n",
    "        \"save\":  \"save\",\n",
    "    })\n",
    "    g.add_edge(\"revise\", \"judge\")\n",
    "    g.add_conditional_edges(\"judge\", decide_pass_or_revise, {\n",
    "        \"revise\": \"revise\",\n",
    "        \"save\":   \"save\",\n",
    "    })\n",
    "    return g.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "b6c84379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000020DAC5CE9C0>"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6a60eb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: docs/system_architecture.md\n"
     ]
    }
   ],
   "source": [
    "app = build_graph()\n",
    "architecture = SectionSpec(\n",
    "        name=\"System Architecture\",\n",
    "        query=\"Architecture overview of the project: high-level system architecture and component responsibilities\",\n",
    "        route=\"both\",\n",
    "        k_text=10,\n",
    "        k_code=20,\n",
    "        guidance=\"Focus on the bigger as well as smaller picture.\",\n",
    "        additional_context=\"\"\"\"\n",
    "You are helping write the **System Architecture** section for a technical project.\n",
    "\n",
    "You will be given:\n",
    "- Project name & brief description — What the system does.\n",
    "- Key goals — What it is designed to achieve.\n",
    "- Key technologies — Languages, frameworks, tools, services.\n",
    "- Any special constraints — e.g., latency, security, budget.\n",
    "- Retrieved repository content tagged as architecture, diagrams, component descriptions, configuration files, and tech stack details, requirement.txt.\n",
    "\n",
    "Output format (only include what is available or inferred):\n",
    "\n",
    "1. **System Architecture Diagram (Mermaid)**\n",
    "   - Use `flowchart TD` or `graph LR`.\n",
    "   - Include the flow of the main applicaiton.\n",
    "   - Mark the title as Infered from the code.\n",
    "   - Mark missing elements as (Information not available in repository).\n",
    "\n",
    "2. **Key Components Table**\n",
    "   - Columns: Component | Responsibility | Technology | Evidence\n",
    "   - Write proper technology with package, frameworks, modules, etc.\n",
    "   - Include component which are acutally used.\n",
    "   - Only what is in the repo or inferred. Don't assume anything.\n",
    "\n",
    "3. **Detailed Explanation**\n",
    "   - Be very detailed about each step techincally. Details like what technique used for example clustering, RAG, few-shot-prompting etc, \n",
    "   also if you can you can write in pointwise manner. If you think there is less information about some step, write less information about this but dont skips any step.\n",
    "   - Explain important python functions too.\n",
    "   - Also explain the technical method used for example clustering, RAG, few-shot-prompting etc.\n",
    "   - What kind of data is used for training, validation, testing etc. If json, then show a sample json, only if available. No guessing.\n",
    "   - Mark missing or inferred steps clearly per rules.\n",
    "\n",
    "4. **Deployment View**\n",
    "   - Tell the entire thing for eg. Local dev setup, staging, production topology etc.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "5. **Scalability & Reliability**\n",
    "   - Only repo data or clearly marked inference.\n",
    "\n",
    "6. **Security & Compliance**\n",
    "   - Authentication, authorization, data protection, logging.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "7. **Trade-offs & Alternatives**\n",
    "   - Key design choices with pros/cons.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "8. **Assumptions & Constraints**\n",
    "   - Supported use cases, limits, boundaries.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "9. **Risks & Mitigations**\n",
    "   - Technical and operational risks with prevention/recovery strategies.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "10. **Observability & Quality**\n",
    "    - Metrics, tracing, alerts, testing approach.\n",
    "\n",
    "11. **Future Extensions**\n",
    "    - Possible evolutions, integrations, optimizations.\n",
    "    - Mark inferred items clearly per rules.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Wrote:\", app.invoke({\"spec\": architecture})[\"out_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bc2068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b366ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = build_graph()\n",
    "technologies = SectionSpec(\n",
    "        name=\"Technologies Used\",\n",
    "        query=\"Installation prerequisites and versions\",\n",
    "        route=\"both\",\n",
    "        k_text=5,\n",
    "        k_code=5,\n",
    "        guidance=\"\"\"\n",
    "        Just list the technologies used in a way like\n",
    "        Languages: Python, JavaScript\n",
    "        Frameworks: Flask, React\n",
    "        Packages: NumPy, Pandas\n",
    "        \"\"\",\n",
    "        additional_context=\"\"\n",
    ")\n",
    "\n",
    "#print(\"Retreive\", app.invoke({\"spec\": technologies}, print_mode=\"debug\"))\n",
    "print(\"Wrote:\", app.invoke({\"spec\": technologies})[\"out_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f72e64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_doc_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
