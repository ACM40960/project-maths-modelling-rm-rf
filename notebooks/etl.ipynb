{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15060132",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import ast\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import sys\n",
    "import nbformat\n",
    "from git import Repo, GitCommandError\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from docx import Document as DocxDocument\n",
    "from itertools import chain\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70891df4",
   "metadata": {},
   "source": [
    "# Reading the repo and chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6eee836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_low_value_file(filepath):\n",
    "    low_value_exts = [\n",
    "        '.css', '.min.js', '.json', '.svg', '.csv', '.xlsx', '.xls',\n",
    "        '.log', '.lock', '.pyc', '.pyo', '.pyd', '.class', '.jar', '.war',\n",
    "        '.o', '.obj', '.dll', '.exe', '.so', '.a', '.db', '.sqlite', '.sqlite3',\n",
    "        '.bak', '.tmp', '.ico', '.icns', '.pdf', '.docx', '.pptx',\n",
    "        '.7z', '.zip', '.tar', '.gz', '.rar', '.iml'\n",
    "    ]\n",
    "\n",
    "    low_value_files = [\n",
    "        'readme.md', 'license', '.gitignore', '.gitattributes', 'post-update.sample',\n",
    "        'fsmonitor-watchman.sample', 'pre-commit', 'pre-push', 'commit-msg',\n",
    "        'tags', 'head', 'config', 'description', 'index', '.editorconfig',\n",
    "        '.prettierrc', '.eslintrc', '.gitmodules', '.mailmap', '.clang-format',\n",
    "        'pipfile.lock', 'yarn.lock', 'package-lock.json', '.env', '.env.example', '.npmrc',\n",
    "        'update.sample'\n",
    "    ]\n",
    "\n",
    "    low_value_dirs = {\n",
    "        '.git', '.vscode', '.idea', '__pycache__',\n",
    "        'node_modules', 'dist', 'build', '.pytest_cache'\n",
    "    }\n",
    "\n",
    "    filepath_str = str(filepath).lower()\n",
    "    parts = set(Path(filepath).parts)\n",
    "\n",
    "    return (\n",
    "        Path(filepath).suffix.lower() in low_value_exts or\n",
    "        os.path.basename(filepath).lower() in low_value_files or\n",
    "        any(d in parts for d in low_value_dirs) or\n",
    "        'mock' in filepath_str\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1a2ba5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile, shutil, os, stat\n",
    "from pathlib import Path\n",
    "from git import Repo\n",
    "\n",
    "def _on_rm_error(func, path, exc_info):\n",
    "    try:\n",
    "        os.chmod(path, stat.S_IWRITE)\n",
    "    except Exception:\n",
    "        pass\n",
    "    func(path)\n",
    "\n",
    "def clone_repo(repo_url, clone_path=None) -> str:\n",
    "    # Use a fresh temp dir by default to avoid collisions\n",
    "    if clone_path is None:\n",
    "        clone_path = tempfile.mkdtemp(prefix=\"repo_\")\n",
    "    cp = Path(clone_path)\n",
    "\n",
    "    if cp.exists():\n",
    "        shutil.rmtree(cp, onerror=_on_rm_error)\n",
    "\n",
    "    Repo.clone_from(repo_url, str(cp), depth=1)\n",
    "    return str(cp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chunks(repo_path: str, index_dir: str = \"docs_index\") -> list[Document]:\n",
    "    chunks = []\n",
    "    repo_path = Path(repo_path)\n",
    "\n",
    "    # 1. README files\n",
    "    for readme_name in (\"README.md\", \"README.rst\", \"README.txt\"):\n",
    "        readme_path = repo_path / readme_name\n",
    "        if readme_path.exists():\n",
    "            content = readme_path.read_text(encoding=\"utf-8\").strip()\n",
    "            if 50 < len(content) < 5000:\n",
    "                chunks.append(Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": str(readme_path),\n",
    "                        \"file_ext\": \"text\",\n",
    "                        \"type\": \"readme\",\n",
    "                        \"name\": readme_name,\n",
    "                        \"lines\": f\"1-{content.count(chr(10)) + 1}\"\n",
    "                    }\n",
    "                ))\n",
    "            break\n",
    "\n",
    "    # 2. Other Markdown files\n",
    "    # 2. Other text files (Markdown/RST/TXT)\n",
    "    for text_path in chain(\n",
    "        repo_path.rglob(\"*.md\"),\n",
    "        repo_path.rglob(\"*.rst\"),\n",
    "        repo_path.rglob(\"*.txt\"),\n",
    "    ):\n",
    "        if text_path.name.lower() == \"readme.md\":\n",
    "            continue\n",
    "        try:\n",
    "            content = text_path.read_text(encoding=\"utf-8\").strip()\n",
    "            if 50 < len(content) < 5000:\n",
    "                ext = text_path.suffix.lower().lstrip(\".\") or \"txt\"  # <- suffix as type/ext\n",
    "                chunks.append(Document(\n",
    "                    page_content=content,\n",
    "                    metadata={\n",
    "                        \"source\": str(text_path),\n",
    "                        \"file_ext\": ext,     # was \"markdown\"\n",
    "                        \"type\": ext,         # was \"markdown\" ‚Äî now suffix-based\n",
    "                        \"name\": text_path.name,\n",
    "                        \"lines\": f\"1-{content.count(chr(10)) + 1}\",\n",
    "                    }\n",
    "                ))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # 3. Code and misc files\n",
    "    for filepath in repo_path.rglob(\"*.*\"):\n",
    "        if is_low_value_file(filepath):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            suffix = filepath.suffix.lower()\n",
    "\n",
    "            # 3a. Python files\n",
    "            if suffix == \".py\":\n",
    "                code = filepath.read_text(encoding=\"utf-8\")\n",
    "                tree = ast.parse(code)\n",
    "\n",
    "                mod_doc = ast.get_docstring(tree)\n",
    "                if mod_doc and 50 < len(mod_doc) < 5000:\n",
    "                    chunks.append(Document(\n",
    "                        page_content=mod_doc,\n",
    "                        metadata={\n",
    "                            \"source\": str(filepath),\n",
    "                            \"file_ext\": \"code\",\n",
    "                            \"type\": \"module_docstring\",\n",
    "                            \"name\": filepath.name,\n",
    "                            \"lines\": f\"1-{code.count(chr(10)) + 1}\"\n",
    "                        }\n",
    "                    ))\n",
    "\n",
    "                for node in tree.body:\n",
    "                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):\n",
    "                        content = ast.get_source_segment(code, node)\n",
    "                        if content and 50 < len(content) < 5000:\n",
    "                            chunks.append(Document(\n",
    "                                page_content=content,\n",
    "                                metadata={\n",
    "                                    \"source\": str(filepath),\n",
    "                                    \"file_ext\": \"code\",\n",
    "                                    \"type\": type(node).__name__.lower(),\n",
    "                                    \"name\": node.name,\n",
    "                                    \"lines\": f\"{node.lineno}-{getattr(node, 'end_lineno', node.lineno)}\"\n",
    "                                }\n",
    "                            ))\n",
    "\n",
    "                        doc = ast.get_docstring(node)\n",
    "                        if doc and 50 < len(doc) < 5000:\n",
    "                            chunks.append(Document(\n",
    "                                page_content=doc,\n",
    "                                metadata={\n",
    "                                    \"source\": str(filepath),\n",
    "                                    \"file_ext\": \"code\",\n",
    "                                    \"type\": f\"{type(node).__name__.lower()}_docstring\",\n",
    "                                    \"name\": node.name,\n",
    "                                    \"lines\": f\"{node.lineno}-{getattr(node, 'end_lineno', node.lineno)}\"\n",
    "                                }\n",
    "                            ))\n",
    "\n",
    "            # 3b. Notebooks\n",
    "            elif suffix == \".ipynb\":\n",
    "                nb = nbformat.read(filepath, as_version=4)\n",
    "                for i, cell in enumerate(nb.cells):\n",
    "                    if cell.cell_type in (\"markdown\", \"code\"):\n",
    "                        content = cell.source.strip()\n",
    "                        if 50 < len(content) < 5000:\n",
    "                            chunks.append(Document(\n",
    "                                page_content=content,\n",
    "                                metadata={\n",
    "                                    \"source\": str(filepath),\n",
    "                                    \"file_ext\": \"code\",\n",
    "                                    \"type\": f\"{cell.cell_type}_cell\",\n",
    "                                    \"name\": f\"{filepath.name} - cell {i}\",\n",
    "                                    \"lines\": f\"cell_{i}\"\n",
    "                                }\n",
    "                            ))\n",
    "\n",
    "            # 3c. Other code/text files\n",
    "            else:\n",
    "                code = filepath.read_text(encoding=\"utf-8\")\n",
    "                if 50 < len(code) < 5000:\n",
    "                    chunks.append(Document(\n",
    "                        page_content=code,\n",
    "                        metadata={\n",
    "                            \"source\": str(filepath),\n",
    "                            \"file_ext\": \"code\",\n",
    "                            \"type\": suffix,\n",
    "                            \"name\": filepath.name,\n",
    "                            \"lines\": f\"1-{code.count(chr(10)) + 1}\"\n",
    "                        }\n",
    "                    ))\n",
    "\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Optional FAISS index creation\n",
    "    # embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
    "    # vectordb = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    # vectordb.save_local(index_dir)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7682d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/adarshlearnngrow/StepUpYourCareer.AI/\"\n",
    "repo_path = clone_repo(repo_url)\n",
    "\n",
    "# 1Ô∏è‚É£ Build the FAISS DB from your repo\n",
    "chunks = extract_all_chunks(repo_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4b4013d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\README.md', 'file_ext': 'text', 'type': 'readme', 'name': 'README.md', 'lines': '1-29'}, page_content='# StepUpYourCareer.ai: Elevate Your Future\\n\\nAn AI-powered career assistant that helps students and job seekers identify **skill gaps**, receive **personalized learning roadmaps**, and connect with **industry mentors**‚Äîall from a single resume upload.\\n\\n### Link to the website: https://stepupyourcareer.streamlit.app/\\n\\n---\\n\\n## Problem\\n\\nGraduates often leave university with degrees but **lack clarity on what employers actually expect**. They spend months applying for jobs, facing rejections without knowing **what skills they‚Äôre missing** or **how to upskill efficiently**.\\n\\n---\\n\\n## Solution\\n\\n**StepUpYourCareer.ai** transforms your resume into a personalized upskilling journey.\\n\\n- **Skill Gap Analyzer**: Extracts skills from your resume and compares them to your target role\\n- **Action Plan Generator**: Recommends curated online courses and resources for each missing skill\\n- **Mentor Matching**: Clusters users and mentors using K-Means to connect you with experts in your domain\\n\\n---\\n\\n![Notes_250516_042908_1](https://github.com/user-attachments/assets/a42216a8-e04c-4335-aad7-d56a61cc873b)\\n\\n![Notes_250516_042908_4](https://github.com/user-attachments/assets/ba3b34ec-57ec-4bf5-906a-84af82342b8b)\\n\\n![Notes_250516_042908_3](https://github.com/user-attachments/assets/4b7e5f11-7923-427c-be95-cbcd7d919c5b)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\requirements.txt', 'file_ext': 'txt', 'type': 'txt', 'name': 'requirements.txt', 'lines': '1-9'}, page_content='streamlit\\nopenai>=1.0.0\\nPyPDF2\\npandas\\nscikit-learn\\ntqdm\\nnumpy\\npandas\\njoblib'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 0', 'lines': 'cell_0'}, page_content='<a href=\"https://colab.research.google.com/github/adarshlearnngrow/StepUpYourCareer.AI/blob/Clustering/ClusteringMentorModelTraining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 2', 'lines': 'cell_2'}, page_content=\"from openai import OpenAI\\nimport json\\nimport time\\nfrom google.colab import userdata\\nimport pandas as pd\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.cluster import KMeans\\nfrom sklearn.metrics import calinski_harabasz_score\\nfrom sklearn.metrics import silhouette_score\\nimport joblib\\nfrom sklearn.manifold import TSNE\\nimport matplotlib.patches as mpatches\\nimport os\\npd.set_option('display.max_colwidth', None)\"),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 3', 'lines': 'cell_3'}, page_content='client = OpenAI(api_key=userdata.get(\"Open_AI_API_KEY\"))'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 4', 'lines': 'cell_4'}, page_content='## Prompt to generate Mentors with skills of experties'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 5', 'lines': 'cell_5'}, page_content='def generate_mentor_prompt(role, tech_skills, n=10):\\n    prompt = f\"\"\"\\nYou are an assistant helping to create a database of expert mentors.\\n\\nGenerate {n} mentor profiles for the role: \"{role}\".\\n\\nEach mentor should have:\\n- A realistic name\\n- A short professional bio (1‚Äì2 sentences)\\n- A list of 5‚Äì7 technical skills (from this list but make sure it the wording are same or atleast close to it, because we will be doing predictions on it later):\\n  {\\', \\'.join(tech_skills)}\\n\\nReturn the output in this JSON format:\\n[\\n  {{\\n    \"name\": \"Mentor Name\",\\n    \"bio\": \"Short mentor bio.\",\\n    \"linkedin_id\": \"Mentor LinkedIn ID\",\\n    \"technical_skills\": [\"skill1\", \"skill2\", ...]\\n  }},\\n  ...\\n]\\n\"\"\"\\n    return prompt\\n\\n# Load role + skill data\\nwith open(\"/content/sample_data/role_skills.json\") as f:\\n    roles_data = json.load(f)\\n\\n# Collection\\nall_mentors = []\\nmentor_counter = 1\\n\\n# Generate mentors for each role\\nfor role_info in roles_data:\\n    role = role_info[\"role\"]\\n    tech_skills = role_info[\"technical_skills\"]\\n\\n    prompt = generate_mentor_prompt(role, tech_skills, n=25)\\n\\n    try:\\n        response = client.chat.completions.create(\\n            model=\"gpt-3.5-turbo\",\\n            messages=[{\"role\": \"user\", \"content\": prompt}],\\n            temperature=0.7\\n        )\\n\\n        raw_output = response.choices[0].message.content.strip()\\n        mentors = json.loads(raw_output)\\n\\n        for mentor in mentors:\\n            mentor[\"mentor_id\"] = f\"M{mentor_counter:04d}\"\\n            mentor[\"role\"] = role\\n            mentor_counter += 1\\n\\n        all_mentors.extend(mentors)\\n        print(f\"Added {len(mentors)} mentors for role: {role}\")\\n\\n    except Exception as e:\\n        print(f\"Error processing role {role}: {e}\")\\n\\n# ‚úÖ Save to JSON\\nwith open(\"generated_mentors.json\", \"w\") as f:\\n    json.dump(all_mentors, f, indent=2)\\n\\nprint(f\"\\\\n‚úÖ All mentor profiles saved to \\'generated_mentors.json\\' ({len(all_mentors)} total).\")'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 6', 'lines': 'cell_6'}, page_content='mentors_final_data = pd.read_json(\"generated_mentors.json\")\\nmentors_final_data.set_index(\"mentor_id\", inplace=True)\\nmentors_final_data.head(5)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 8', 'lines': 'cell_8'}, page_content='mlb = MultiLabelBinarizer()\\nskills_matrix = mlb.fit_transform(mentors_final_data[\\'technical_skills\\'])\\njoblib.dump(mlb, \"mlb.joblib\")\\nskills_df = pd.DataFrame(skills_matrix, columns=mlb.classes_, index=mentors_final_data.index)\\nskills_df.head(5)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 9', 'lines': 'cell_9'}, page_content='### Fitting a KMean model with range of cluster k and ran for 100 times to be certain.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 10', 'lines': 'cell_10'}, page_content='k_val = np.arange(10, 30)\\nbest_k = None\\nbest_score = -1\\nshilloutte_scores = []\\ninertias = []\\nch_indexs = []\\n\\nfor k in k_val:\\n    temp_sil_scores = []\\n    temp_ch_scores = []\\n    temp_inertias = []\\n\\n    for run in range(100):  # 100 is heavy; reduce to 5 or 10 for dev\\n        kmeans = KMeans(n_clusters=k, random_state=run, n_init=\\'auto\\')\\n        labels = kmeans.fit_predict(skills_df)\\n\\n        sil_score = silhouette_score(skills_df, labels)\\n        ch_index = calinski_harabasz_score(skills_df, labels)\\n\\n        temp_sil_scores.append(sil_score)\\n        temp_ch_scores.append(ch_index)\\n        temp_inertias.append(kmeans.inertia_)\\n\\n    # Average over all runs\\n    avg_sil = np.mean(temp_sil_scores)\\n    avg_ch = np.mean(temp_ch_scores)\\n    avg_inertia = np.mean(temp_inertias)\\n\\n    shilloutte_scores.append(avg_sil)\\n    ch_indexs.append(avg_ch)\\n    inertias.append(avg_inertia)\\n\\n    if avg_sil > best_score:\\n        best_score = avg_sil\\n        best_k = k\\n\\nprint(f\"‚úÖ Best K = {best_k} with silhouette score = {best_score:.4f}\")\\n\\n# üìà Plot all 3 metrics\\nplt.figure(figsize=(15, 4))\\n\\nplt.subplot(1, 3, 1)\\nplt.plot(k_val, inertias, marker=\\'o\\')\\nplt.title(\"Elbow Plot (Inertia)\")\\nplt.xlabel(\"Number of Clusters (K)\")\\nplt.ylabel(\"Inertia\")\\nplt.grid(True)\\n\\nplt.subplot(1, 3, 2)\\nplt.plot(k_val, shilloutte_scores, marker=\\'s\\', color=\\'green\\')\\nplt.title(\"Silhouette Score\")\\nplt.xlabel(\"Number of Clusters (K)\")\\nplt.ylabel(\"Score\")\\nplt.grid(True)\\n\\nplt.subplot(1, 3, 3)\\nplt.plot(k_val, ch_indexs, marker=\\'^\\', color=\\'purple\\')\\nplt.title(\"Calinski-Harabasz Index\")\\nplt.xlabel(\"Number of Clusters (K)\")\\nplt.ylabel(\"Score\")\\nplt.grid(True)\\n\\nplt.tight_layout()\\nplt.show()'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 12', 'lines': 'cell_12'}, page_content='best_k = 29\\nkmeans_final = KMeans(n_clusters=best_k, random_state=42, n_init=\\'auto\\')\\nmentors_final_data[\"cluster\"] = kmeans_final.fit_predict(skills_df)\\n\\n# Mount Google Drive\\nfrom google.colab import drive\\ndrive.mount(\\'/content/drive\\')\\n\\n\\n# Define path to save\\nsave_path = \"/content/drive/My Drive/saved_models\"\\nos.makedirs(save_path, exist_ok=True)\\n\\n# Save your scikit-learn KMeans model\\njoblib.dump(mlb, os.path.join(save_path,\"fitted_vectorizer.pkl\"))\\njoblib.dump(kmeans_final, os.path.join(save_path, \"mentor_clustering_model.pkl\"))\\n\\nprint(\"KMeans model saved to Google Drive!\")'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 14', 'lines': 'cell_14'}, page_content='### Using TSNE to see if the clustering algorithm is grouping based on skills or based on roles'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 15', 'lines': 'cell_15'}, page_content='tsne = TSNE(n_components=2, perplexity=30, random_state=42)\\ntsne_result = tsne.fit_transform(skills_df.values)\\n\\nplt.figure(figsize=(16, 6))  # one wide figure with 2 subplots\\n\\n# Subplot 1 ‚Äî using cluster labels\\nplt.subplot(1, 2, 1)\\nplt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=mentors_final_data[\"cluster\"], cmap=\\'tab10\\')\\nplt.title(\"Mentor Clusters by KMeans\")\\nplt.xlabel(\"TSNE Dimension 1\")\\nplt.ylabel(\"TSNE Dimension 2\")\\n#plt.(label=\\'Cluster\\')\\nplt.grid(True)\\n\\nrole_mapping = {role: idx for idx, role in enumerate(mentors_final_data[\"role\"].unique())}\\nrole_labels = mentors_final_data[\"role\"].map(role_mapping).values\\nplt.subplot(1, 2, 2)\\nplt.scatter(tsne_result[:, 0], tsne_result[:, 1], c=role_labels, cmap=\\'tab10\\')\\nplt.title(\"Mentor Labels by Target Role\")\\nplt.xlabel(\"TSNE Dimension 1\")\\nplt.ylabel(\"TSNE Dimension 2\")\\n#plt.colorbar(label=\\'Role Label\\')\\nplt.grid(True)\\n\\nplt.tight_layout()\\nplt.show()'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 16', 'lines': 'cell_16'}, page_content='### Clustering happeining on skills cause within each role there are still overlapping clusters, means skills are common within each roles'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 18', 'lines': 'cell_18'}, page_content='#predicting on a student\\n\\n# Load student skill gap\\nstudent_data = pd.read_json(\"skill_gap_analysis.json\")\\nskill_gap = student_data.iloc[np.random.randint(1)][\\'technical_skill_gap\\']\\n\\nskill_gap = [s.strip() for s in skill_gap.split(\",\")]\\nskill_gap\\n\\n# Ensure mlb is already fitted on your mentor skill data\\nmlb.transform([skill_gap])'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 19', 'lines': 'cell_19'}, page_content='prediction = kmeans_final.predict(mlb.transform([skill_gap]))'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 21', 'lines': 'cell_21'}, page_content='mentors_final_data[mentors_final_data[\"cluster\"] == prediction[0]][[\\'name\\', \\'bio\\',\\'linkedin_id\\', \\'technical_skills\\']]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 0', 'lines': 'cell_0'}, page_content='<a href=\"https://colab.research.google.com/github/adarshlearnngrow/StepUp-AI/blob/main/Job_Description_JD_Manupulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 2', 'lines': 'cell_2'}, page_content='from google.colab import files\\n\\nuploaded = files.upload()\\n\\nfor fn in uploaded.keys():\\n  print(\\'User uploaded file \"{name}\" with length {length} bytes\\'.format(\\n      name=fn, length=len(uploaded[fn])))'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 3', 'lines': 'cell_3'}, page_content='jobs_desc = pd.read_csv(\"JobsDatasetProcessed.csv\")\\nreq_jobs_desc = jobs_desc.query(\"Query in [\\'Artificial Intelligence\\', \\'Business Analyst\\', \\'Business Intelligence Analyst\\', \\'Data Analyst\\', \\'Machine Learning\\']\")\\nreq_jobs_desc = req_jobs_desc[[\\'Query\\', \\'Description\\']]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 4', 'lines': 'cell_4'}, page_content=\"req_jobs_desc.loc[req_jobs_desc['Query'] == 'Artificial Intelligence', 'Query'] = 'AI Engineer'\\nreq_jobs_desc\"),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 5', 'lines': 'cell_5'}, page_content='from google.colab import sheets\\nsheet = sheets.InteractiveSheet(df=req_jobs_desc)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 6', 'lines': 'cell_6'}, page_content='uploaded = files.upload()\\n\\nfor fn in uploaded.keys():\\n  print(\\'User uploaded file \"{name}\" with length {length} bytes\\'.format(\\n      name=fn, length=len(uploaded[fn])))'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 7', 'lines': 'cell_7'}, page_content='sim_resumes = pd.read_json(\"all_roles_student_resumes.json\")'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 8', 'lines': 'cell_8'}, page_content=\"sim_resumes.loc[sim_resumes['target_role'] == 'Artificial Intelligence', 'target_role'] = 'AI Engineer'\"),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 9', 'lines': 'cell_9'}, page_content=\"sim_resumes.to_json('sim_resume.json', orient='records', lines=False)\"),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 0', 'lines': 'cell_0'}, page_content='<a href=\"https://colab.research.google.com/github/adarshlearnngrow/StepUp-AI/blob/main/Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 2', 'lines': 'cell_2'}, page_content=\"# !pip install PyPDF2 openai\\nimport openai\\nimport json\\nimport time\\nimport re\\nfrom openai import OpenAI\\nfrom google.colab import userdata\\nimport pandas as pd\\npd.set_option('display.max_colwidth', None)\\nfrom PyPDF2 import PdfReader\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nimport numpy as np\"),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 3', 'lines': 'cell_3'}, page_content='client = OpenAI(api_key=userdata.get(\"openai_key\"))'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 5', 'lines': 'cell_5'}, page_content='roles = [\\n    \"Artificial Intelligence\",\\n    \"Business Analyst\",\\n    \"Business Intelligence Analyst\",\\n    \"Data Analyst\",\\n    \"Machine Learning\"\\n]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 7', 'lines': 'cell_7'}, page_content='def generate_student_resume(role: str):\\n    prompt = f\"\"\"\\nYou are generating a **realistic and anonymized resume** for a computer science student or recent graduate (0‚Äì2 years of experience) applying for a role as a {role}.\\n\\nReturn strictly valid JSON in the following format:\\n{{\\n  \"name\": \"Candidate_<ID>\",\\n  \"education\": \"<Degree, Masked University (e.g., \\'Top Engineering Institute\\'), Graduation Years>\",\\n  \"work_experience\": [\\n    {{\\n      \"role\": \"<Internship or research title>\",\\n      \"company\": \"<Realistic but anonymized organization (e.g., \\'NeuroTech AI\\', \\'InsightSoft Labs\\')>\",\\n      \"duration\": \"<Month Year ‚Äì Month Year>\",\\n      \"description\": \"Describe the responsibilities in a detailed, narrative style. Mention the purpose of the project, specific tasks completed, tools/technologies used, and measurable impact. Be authentic ‚Äî include dataset sizes, model names, APIs, or product features if relevant.\"\\n    }},\\n    {{\\n      \"role\": \"<Optional second experience (lab assistant, part-time dev, open-source contributor)>\",\\n      \"company\": \"<Another anonymized organization (e.g., \\'Top Tech University - DataLab\\')>\",\\n      \"duration\": \"<Month Year ‚Äì Month Year>\",\\n      \"description\": \"Follow the same format: outcome-focused, rich in tech and context. Avoid vague phrasing.\"\\n    }}\\n  ],\\n  \"personal_projects\": [\\n    {{\\n      \"title\": \"<Unique, interesting project title>\",\\n      \"description\": \"Explain the project‚Äôs motivation (e.g. coursework, curiosity, hackathon), what problem it solved, what tech was used, and the result. Include metrics if applicable. Avoid generic projects like \\'movie recommender\\'.\"\\n    }},\\n    {{\\n      \"title\": \"<Second project (can be unrelated to the role)>\",\\n      \"description\": \"Still provide depth. For example, a photography app, social impact tool, or game with real features. Mention full-stack tools or libraries used.\"\\n    }}\\n  ],\\n   \"technical_skills\": [\"<Realistic tech skills: some aligned, some adjacent>\"],\\n   \"soft_skills\": [\"<Soft skills like teamwork, leadership, communication, etc.>\"]\\n}}\\n\\nSTRICT INSTRUCTIONS:\\n- **No markdown**, **no explanations**, only valid, well-formatted JSON.\\n- **Do not reuse phrases** like ‚Äúworked with a team‚Äù or ‚Äúimproved accuracy.‚Äù\\n- **Never use placeholder names** like Tech_Startup_123. Instead, use fictional but plausible names like ‚ÄúCortexAI‚Äù or ‚ÄúNuvem Data.‚Äù\\n- Use **varied and creative projects** ‚Äî not all should be recommendation systems!\\n- Projects can be from coursework, hackathons, personal exploration, or clubs\\n- Leave **some skill gaps** compared to what a real job would require\\n- Descriptions must sound like they were written by a strong student applying to a competitive job ‚Äî technically sharp, reflective, and specific.\\n- You may simulate imperfections (e.g., slightly over-descriptive soft skills or overuse of buzzwords) occasionally for realism, but keep overall quality high.\\n\"\"\"\\n\\n    response = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[{\"role\": \"user\", \"content\": prompt}],\\n        temperature=0.8\\n    )\\n    return response.choices[0].message.content.strip()\\n\\nresumes = []\\n\\nfor role in roles:\\n    print(f\"Generating resumes for role: {role}\")\\n    for i in range(10):\\n        try:\\n            resume_json = generate_student_resume(role=role)\\n            resume_dict = json.loads(resume_json)\\n            resume_dict[\"target_role\"] = role\\n            resumes.append(resume_dict)\\n            print(f\"Resume {i+1} for {role}\")\\n            time.sleep(1)\\n        except Exception as e:\\n            print(f\"Error at resume {i+1} for {role}: {e}\")\\n\\n# Save all resumes to a JSON file\\nwith open(\"all_roles_student_resumes.json\", \"w\", encoding=\"utf-8\") as f:\\n    json.dump(resumes, f, indent=2, ensure_ascii=False)\\n\\nprint(\"All resumes saved to all_roles_student_resumes.json\")'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 9', 'lines': 'cell_9'}, page_content='with open(\"all_roles_student_resumes.json\", \"r\") as f:\\n    resumes = json.load(f)\\n\\njobs_df = pd.read_csv(\"req_job_desc.csv\")'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 11', 'lines': 'cell_11'}, page_content='def summarize_resume(resume):\\n    tech_skills = \", \".join(resume.get(\"technical_skills\", []))\\n    soft_skills = \", \".join(resume.get(\"soft_skills\", []))\\n    parts = []\\n    if resume.get(\"education\"):\\n        parts.append(f\"Education: {resume[\\'education\\']}\")\\n    if resume.get(\"work_experience\"):\\n        roles = [we.get(\"role\") for we in resume[\"work_experience\"] if we.get(\"role\")]\\n        if roles:\\n            parts.append(\"Experience roles: \" + \", \".join(roles))\\n    if tech_skills:\\n        parts.append(\"Technical Skills: \" + tech_skills)\\n    if soft_skills:\\n        parts.append(\"Soft Skills: \" + soft_skills)\\n    return \" | \".join(parts)\\n\\ndef summarize_job(job):\\n    title = job.get(\"Job Title\", \"Job\")\\n    it_skills = job.get(\"IT Skills\", \"\")\\n    soft_skills = job.get(\"Soft Skills\", \"\")\\n    return f\"Job Title: {title} | Required Technical Skills: {it_skills} | Required Soft Skills: {soft_skills}\"'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 12', 'lines': 'cell_12'}, page_content='### Printing 120 job descriptions for each role (Total 600)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 14', 'lines': 'cell_14'}, page_content='### Extracting common required technical and soft skills required for each role.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 15', 'lines': 'cell_15'}, page_content='jobs_df = pd.read_csv(\"req_job_desc.csv\")\\n\\n# Group by target role\\ngrouped_roles = jobs_df.groupby(\"Query\")\\n\\ndef extract_skills_from_descriptions(role, descriptions):\\n    combined_text = \"\\\\n\\\\n---\\\\n\\\\n\".join(descriptions)\\n\\n    prompt = f\"\"\"\\n      You are a highly skilled AI career advisor. You are given a set of job descriptions for the role of \"{role}\".\\n\\n      Your task is to extract the **common transferable skills** required across these jobs ‚Äî categorize them as:\\n      1. Technical Skills\\n      2. Soft Skills\\n\\n      Avoid any industry-specific or domain-specific skills (e.g., \"healthcare compliance\", \"lab equipment use\", \"insurance claim processing\"). Focus only on **transferable, job-agnostic skills**.\\n\\n      Output must be in strict JSON format like this:\\n      {{\\n        \"role\": \"{role}\",\\n        \"technical_skills\": [\"skill1\", \"skill2\", ...],\\n        \"soft_skills\": [\"skillA\", \"skillB\", ...]\\n      }}\\n\\n      Here are the job descriptions:\\n      {combined_text}\\n      \"\"\"\\n\\n    try:\\n        response = client.chat.completions.create(\\n            model=\"gpt-4o-mini\",\\n            messages=[\\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant for extracting transferable skills.\"},\\n                {\"role\": \"user\", \"content\": prompt}\\n            ],\\n            temperature=0.2\\n        )\\n\\n        raw_output = response.choices[0].message.content.strip()\\n        if raw_output.startswith(\"```json\"):\\n            raw_output = raw_output[7:]\\n        if raw_output.endswith(\"```\"):\\n            raw_output = raw_output[:-3]\\n\\n        return json.loads(raw_output)\\n\\n    except Exception as e:\\n        print(f\"Error for role \\'{role}\\': {e}\")\\n        return {\\n            \"role\": role,\\n            \"technical_skills\": [],\\n            \"soft_skills\": []\\n        }\\n\\n# Run for all roles\\nresults = []\\nfor role, group in grouped_roles:\\n    print(f\"Extracting skills for role: {role}\")\\n    descriptions = group[\"Description\"].dropna().tolist()\\n    if not descriptions:\\n        continue\\n    skill_summary = extract_skills_from_descriptions(role, descriptions)\\n    results.append(skill_summary)\\n    time.sleep(1)\\n\\n# Save to files\\ndf_out = pd.DataFrame(results)\\ndf_out.to_csv(\"role_transferable_skills.csv\", index=False)\\nwith open(\"role_transferable_skills.json\", \"w\") as f:\\n    json.dump(results, f, indent=2)\\n\\nprint(\"\\\\n Skill extraction complete. Files saved: role_transferable_skills.csv and .json\")'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 16', 'lines': 'cell_16'}, page_content='### File containing all common skills required for a particular role'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 17', 'lines': 'cell_17'}, page_content='with open(\"role_skills.json\", \"r\") as f:\\n    resumes_data = json.load(f)\\n\\nprint(json.dumps(resumes_data, indent=2))'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 18', 'lines': 'cell_18'}, page_content='### Identifying skill gaps in the simulated resumes based on desired role'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 19', 'lines': 'cell_19'}, page_content='# Load Files\\nwith open(\"sim_resume.json\", \"r\") as f:\\n    resumes = [json.loads(line) for line in f]\\n\\nwith open(\"role_skills.json\", \"r\") as f:\\n    raw_roles = json.load(f)\\nrole_skills = {r[\"role\"]: r for r in raw_roles}\\n\\n# Helper Functions\\ndef summarize_resume(resume):\\n    \"\"\"Summarize fields from a student resume.\"\"\"\\n    summary = []\\n    if \"target_role\" in resume:\\n        summary.append(f\"Target Role: {resume[\\'target_role\\']}\")\\n    if \"education\" in resume:\\n        summary.append(f\"Education: {resume[\\'education\\']}\")\\n    if \"work_experience\" in resume:\\n        jobs = [w[\"role\"] for w in resume[\"work_experience\"] if \"role\" in w]\\n        if jobs:\\n            summary.append(\"Experience: \" + \", \".join(jobs))\\n    if \"technical_skills\" in resume:\\n        summary.append(\"Technical Skills: \" + \", \".join(resume[\"technical_skills\"]))\\n    if \"soft_skills\" in resume:\\n        summary.append(\"Soft Skills: \" + \", \".join(resume[\"soft_skills\"]))\\n    return \" | \".join(summary)\\n\\ndef generate_skill_gap(resume_text, expected_tech, expected_soft):\\n    \"\"\"Query OpenAI to get skill gaps and transferable skills.\"\"\"\\n    prompt = f\"\"\"\\nYou are a career advisor helping students identify skill gaps.\\n\\nCompare the student\\'s resume with the required skills below. Do 3 things:\\n\\n1. Identify missing **technical skills** from the list that are not in the resume.\\n2. Identify missing **soft skills** from the list.\\n3. Suggest **transferable skills** from the resume that could help the student learn the missing technical skills.\\n\\nReturn valid JSON with this structure:\\n{{\\n  \"technical_skill_gaps\": [...],\\n  \"soft_skill_gaps\": [...],\\n  \"transferable_skills\": [...]\\n}}\\n\\nIf a category is empty, return an empty list.\\nDo NOT explain or repeat anything. Just return clean JSON.\\n\\n---\\nResume:\\n{resume_text}\\n\\nRequired Technical Skills: {\", \".join(expected_tech)}\\nRequired Soft Skills: {\", \".join(expected_soft)}\\n\"\"\"\\n\\n    try:\\n        response = client.chat.completions.create(\\n            model=\"gpt-4o-mini\",\\n            messages=[\\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant for skill gap analysis.\"},\\n                {\"role\": \"user\", \"content\": prompt}\\n            ],\\n            temperature=0\\n        )\\n        result = response.choices[0].message.content.strip()\\n\\n        if result.startswith(\"```json\"):\\n            result = result[7:]\\n        if result.endswith(\"```\"):\\n            result = result[:-3]\\n\\n        parsed = json.loads(result)\\n        return (\\n            parsed.get(\"technical_skill_gaps\", []),\\n            parsed.get(\"soft_skill_gaps\", []),\\n            parsed.get(\"transferable_skills\", [])\\n        )\\n    except Exception as e:\\n        print(\"Error:\", e)\\n        return [], [], []\\n\\n# Run Analysis\\n\\nrecords = []\\n\\nfor idx, resume in enumerate(resumes):\\n    name = resume.get(\"name\", f\"Candidate_{idx}\")\\n    role = resume.get(\"target_role\", \"\").strip()\\n\\n    if not role or role not in role_skills:\\n        print(f\"[{name}] Skipped ‚Äî no target role or unknown role.\")\\n        continue\\n\\n    resume_summary = summarize_resume(resume)\\n    expected = role_skills[role]\\n    tech_skills = expected[\"technical_skills\"]\\n    soft_skills = expected[\"soft_skills\"]\\n\\n    print(f\"[{name}] ‚Üí Analyzing role: {role}\")\\n    tech_gap, soft_gap, transferable = generate_skill_gap(resume_summary, tech_skills, soft_skills)\\n\\n    records.append({\\n        \"name\": name,\\n        \"target_role\": role,\\n        \"resume_summary\": resume_summary,\\n        \"technical_skill_gap\": \", \".join(tech_gap),\\n        \"soft_skill_gap\": \", \".join(soft_gap),\\n        \"transferable_skills\": \", \".join(transferable)\\n    })\\n    time.sleep(1)\\n\\n# Save Output\\ndf = pd.DataFrame(records)\\ndf.to_csv(\"skill_gap_analysis.csv\", index=False)\\ndf.to_json(\"skill_gap_analysis.json\", orient=\"records\", indent=2)\\nprint(f\"\\\\n Done! Analyzed {len(records)} resumes and saved to CSV and JSON.\")'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 20', 'lines': 'cell_20'}, page_content='### Skill Gap Analysis Functionality - allows user to upload their resume'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 21', 'lines': 'cell_21'}, page_content='with open(\"skill_gap_analysis.json\", \"r\") as f:\\n    examples = json.load(f)\\n\\ndef get_embedding(text, model=\"text-embedding-ada-002\"):\\n    response = client.embeddings.create(input=[text], model=model)\\n    return response.data[0].embedding\\n\\nexample_texts = [\\n    f\"Resume: {ex[\\'resume_summary\\']} | Role: {ex[\\'target_role\\']}\" for ex in examples\\n]\\nexample_embeddings = [get_embedding(text) for text in example_texts]\\n\\nwith open(\"role_skills.json\", \"r\") as f:\\n    role_skills_list = json.load(f)\\n\\n# Convert list ‚Üí dict: { role_name: {technical_skills: [...], soft_skills: [...]} }\\nrole_skills = {\\n    entry[\"role\"]: {\\n        \"technical_skills\": entry.get(\"technical_skills\", []),\\n        \"soft_skills\": entry.get(\"soft_skills\", [])\\n    }\\n    for entry in role_skills_list if \"role\" in entry\\n}\\n\\n# Anonymize resume text\\ndef anonymize(text):\\n    return client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that removes personal identifiers from resumes.\"},\\n            {\"role\": \"user\", \"content\": f\"Anonymize this resume:\\\\n\\\\n{text}\"}\\n        ],\\n        temperature=0\\n    ).choices[0].message.content.strip()\\n\\ndef extract_resume_text(pdf_path):\\n    reader = PdfReader(pdf_path)\\n    text = \"\\\\n\".join([page.extract_text() or \"\" for page in reader.pages])\\n    return text.strip()\\n\\ndef retrieve_examples(resume_text, target_role, k=3):\\n    query = f\"Resume: {resume_text} | Role: {target_role}\"\\n    query_embedding = get_embedding(query)\\n    sims = cosine_similarity([query_embedding], example_embeddings)[0]\\n    top_k_indices = sims.argsort()[-k:][::-1]\\n    return [examples[i] for i in top_k_indices]\\n\\ndef generate_skill_gap(resume_text, target_role, retrieved_examples, fallback_skills):\\n    examples_prompt = \"\\\\n\\\\n\".join([\\n        f\"Example for role {ex[\\'target_role\\']}:\\\\nResume: {ex[\\'resume_summary\\']}\\\\nSkill Gaps: tech={ex[\\'technical_skill_gap\\']}, soft={ex[\\'soft_skill_gap\\']}, transferable={ex[\\'transferable_skills\\']}\"\\n        for ex in retrieved_examples\\n    ])\\n\\n    expected_tech = fallback_skills.get(\"technical_skills\", [])\\n    expected_soft = fallback_skills.get(\"soft_skills\", [])\\n\\n    full_prompt = f\"\"\"\\nYou are a highly experienced career advisor. Below are real examples of skill gap analysis between student resumes and job requirements.\\n\\n## EXAMPLES\\n{examples_prompt}\\n\\n## TASK:\\n\\nNow, perform the same analysis on the following new resume.\\n\\n1. Read the student‚Äôs resume.\\n2. Compare it to the required skills.\\n3. Return:\\n   - Skills the student is missing (from the required lists).\\n   - Transferable skills the student has that help bridge gaps.\\n\\n## REQUIRED FORMAT ‚Äî Output valid JSON:\\n\\n{{\\n  \"technical_skill_gaps\": [list of missing technical skills],\\n  \"soft_skill_gaps\": [list of missing soft skills],\\n  \"transferable_skills\": [skills from the resume that can help learn missing ones]\\n}}\\n\\n‚ùå Do not explain anything. Do not repeat resume or skills. Just return the JSON.\\n‚ùå If no gaps, return empty lists.\\n\\n---\\n\\nüéØ Target Role: {target_role}\\nüìå Required Technical Skills: {\", \".join(expected_tech)}\\nüìå Required Soft Skills: {\", \".join(expected_soft)}\\n\\nüìÑ Resume:\\n{resume_text}\\n\"\"\"\\n\\n\\n    response = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant for skill gap analysis.\"},\\n            {\"role\": \"user\", \"content\": full_prompt}\\n        ],\\n        temperature=0.0\\n    )\\n    raw = response.choices[0].message.content.strip()\\n    if raw.startswith(\"```json\"):\\n        raw = raw[7:]\\n    if raw.endswith(\"```\"):\\n        raw = raw[:-3]\\n    return json.loads(raw)\\n\\n# === Full pipeline function ===\\ndef analyze_uploaded_resume(pdf_path, target_role):\\n    resume_text = extract_resume_text(pdf_path)\\n    anonymized_resume = anonymize(resume_text)\\n    fallback_skills = role_skills.get(target_role, {})\\n    retrieved = retrieve_examples(anonymized_resume, target_role)\\n    result = generate_skill_gap(anonymized_resume, target_role, retrieved, fallback_skills)\\n    return {\\n        \"resume_text\": anonymized_resume,\\n        \"target_role\": target_role,\\n        **result\\n    }'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 22', 'lines': 'cell_22'}, page_content='from google.colab import files\\n\\nuploaded = files.upload()\\npdf_path = list(uploaded.keys())[0]\\n\\ntarget_role = input(\"Enter your target role (e.g., \\'AI Engineer\\'): \").strip()\\n\\noutput = analyze_uploaded_resume(pdf_path, target_role)\\n\\nprint(json.dumps(output, indent=2))'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 23', 'lines': 'cell_23'}, page_content='### Action Plan Generation (Testing for 3 Candidates)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 24', 'lines': 'cell_24'}, page_content='df = pd.read_csv(\"skill_gap_analysis.csv\").head(3)\\n\\ndef extract_json_from_response(raw):\\n    \"\"\"Safely extract JSON object from LLM response using regex.\"\"\"\\n    match = re.search(r\"\\\\{[\\\\s\\\\S]*\\\\}\", raw)\\n    if match:\\n        try:\\n            return json.loads(match.group())\\n        except json.JSONDecodeError:\\n            return {}\\n    return {}\\n\\ndef generate_action_plan(tech_skills, soft_skills, transferable_skills):\\n    prompt = f\"\"\"\\nYou are a top-tier career advisor and learning coach. For each skill listed, recommend 2‚Äì3 diverse and high-quality, **REAL** learning resources with links.\\n\\nThe resources must be: genuinely helpful for mastering the skill\\n\\nResources may include:\\n- Online courses\\n- Popular Books with amazon links\\n- Practice Platforms (e.g. LeetCode, HackerRank)\\n- GitHub repos or open-source projects\\n- YouTube Tutorials\\n- Blogs, articles, or documentation etc-\\n\\nEnsure resources are **practical**, well-reviewed, REAL and up-to-date.\\n\\nRespond ONLY in valid JSON:\\n\\n{{\\n  \"technical_skill_resources\": {{\\n    \"Skill 1\": [\"Resource A\", \"Resource B\"],\\n    ...\\n  }},\\n  \"soft_skill_resources\": {{\\n    \"Skill 1\": [\"Resource A\", \"Resource B\"],\\n    ...\\n  }},\\n  \"transferable_skill_resources\": {{\\n    \"Skill 1\": [\"Resource A\", \"Resource B\"]\\n  }}\\n}}\\n\\nNo explanations. No markdown. No extra text. Just the JSON object. DO NOT make up fake links. Use only real, high-quality resources.\\n\\n---\\n\\nTechnical Skills Gap: {\\', \\'.join(tech_skills)}\\nSoft Skills Gap: {\\', \\'.join(soft_skills)}\\nTransferable Skills: {\\', \\'.join(transferable_skills)}\\n\"\"\"\\n\\n    try:\\n        response = client.chat.completions.create(\\n            model=\"gpt-4o-mini\",\\n            messages=[\\n                {\"role\": \"system\", \"content\": \"You are a helpful assistant for job upskilling.\"},\\n                {\"role\": \"user\", \"content\": prompt}\\n            ],\\n            temperature=0.2\\n        )\\n        raw_output = response.choices[0].message.content.strip()\\n        return extract_json_from_response(raw_output)\\n    except Exception as e:\\n        print(\"API Error:\", e)\\n        return {}\\n\\nresults = []\\n\\nfor idx, row in df.iterrows():\\n    name = row.get(\"name\", f\"Candidate_{idx}\")\\n    print(f\"[{name}] ‚Üí Generating action plan...\")\\n\\n    tech = [s.strip() for s in str(row.get(\"technical_skill_gap\", \"\")).split(\",\") if s.strip()]\\n    soft = [s.strip() for s in str(row.get(\"soft_skill_gap\", \"\")).split(\",\") if s.strip()]\\n    trans = [s.strip() for s in str(row.get(\"transferable_skills\", \"\")).split(\",\") if s.strip()]\\n\\n    if not tech and not soft and not trans:\\n        print(\"   ‚Üí No skill gaps found. Skipping.\")\\n        continue\\n\\n    plan = generate_action_plan(tech, soft, trans)\\n\\n    results.append({\\n        \"name\": name,\\n        \"target_role\": row.get(\"target_role\", \"\"),\\n        \"technical_skill_gap\": tech,\\n        \"soft_skill_gap\": soft,\\n        \"transferable_skills\": trans,\\n        \"action_plan\": plan\\n    })\\n\\n    time.sleep(1)\\n\\nwith open(\"test_action_plan_output.json\", \"w\") as f:\\n    json.dump(results, f, indent=2)\\n\\nprint(f\"\\\\n Test complete! Results saved to \\'test_action_plan_output.json\\'\")'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'page_1', 'lines': '24-35'}, page_content='def page_1():\\n    st.title(\"Welcome to StepUpYourCareer.AI\")\\n    st.markdown(\"##### Let\\'s get started with a few details.\")\\n\\n    name = st.text_input(\"Your Full Name\")\\n    email = st.text_input(\"Your Email Address\")\\n\\n    if name and email:\\n        if st.button(\"‚û°Ô∏è Proceed to Resume Analysis\"):\\n            st.session_state.name = name\\n            st.session_state.email = email\\n            st.session_state.page = 2'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'load_examples', 'lines': '37-44'}, page_content='def load_examples():\\n            with open(\"/mount/src/stepupyourcareer.ai/StepUpAI/skill_gap_analysis.json\", \"r\") as f:\\n                examples = json.load(f)\\n            example_texts = [\\n                f\"Resume: {ex[\\'resume_summary\\']} | Role: {ex[\\'target_role\\']}\" for ex in examples\\n            ]\\n            embeddings = [get_embedding(text) for text in example_texts]\\n            return examples, embeddings'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'load_role_skills', 'lines': '48-57'}, page_content='def load_role_skills():\\n    with open(\"/mount/src/stepupyourcareer.ai/StepUpAI/role_skills.json\", \"r\") as f:\\n        role_skills_list = json.load(f)\\n        return {\\n            entry[\"role\"]: {\\n                \"technical_skills\": entry.get(\"technical_skills\", []),\\n                \"soft_skills\": entry.get(\"soft_skills\", [])\\n                }\\n            for entry in role_skills_list if \"role\" in entry\\n            }'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'get_embedding', 'lines': '60-62'}, page_content='def get_embedding(text, model=\"text-embedding-ada-002\"):\\n    response = client.embeddings.create(input=[text], model=model)\\n    return response.data[0].embedding'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'extract_text_from_pdf', 'lines': '65-67'}, page_content='def extract_text_from_pdf(uploaded_file):\\n    reader = PdfReader(uploaded_file)\\n    return \"\\\\n\".join([page.extract_text() or \"\" for page in reader.pages])'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'anonymize', 'lines': '70-79'}, page_content='def anonymize(text):\\n    response = client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that removes personal identifiers from resumes.\"},\\n        {\"role\": \"user\", \"content\": f\"Anonymize this resume:\\\\n\\\\n{text}\"}\\n        ],\\n        temperature=0\\n        )\\n    return response.choices[0].message.content.strip()'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'retrieve_examples', 'lines': '82-86'}, page_content='def retrieve_examples(query, embeddings, examples, k=3):\\n    query_emb = get_embedding(query)\\n    sims = sk_cosine([query_emb], embeddings)[0]\\n    top_k = sims.argsort()[-k:][::-1]\\n    return [examples[i] for i in top_k]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'generate_skill_gap', 'lines': '89-129'}, page_content='def generate_skill_gap(resume_text, target_role, retrieved_examples, fallback_skills):\\n    examples_prompt = \"\\\\n\\\\n\".join([\\n    f\"Example for role {ex[\\'target_role\\']}:\\\\nResume: {ex[\\'resume_summary\\']}\\\\nSkill Gaps: tech={ex[\\'technical_skill_gap\\']}, soft={ex[\\'soft_skill_gap\\']}, transferable={ex[\\'transferable_skills\\']}\"\\n    for ex in retrieved_examples\\n        ])\\n    expected_tech = fallback_skills.get(\"technical_skills\", [])\\n    expected_soft = fallback_skills.get(\"soft_skills\", [])\\n\\n    prompt = f\"\"\"\\n        You are a highly experienced career advisor. Based on examples and required skills, identify skill gaps.\\n\\n        ## EXAMPLES\\n        {examples_prompt}\\n\\n        ## TASK:\\n\\n        Analyze the new resume below and return only JSON:\\n        {{\\n        \"technical_skill_gaps\": [list of missing technical skills],\\n        \"soft_skill_gaps\": [list of missing soft skills],\\n        \"transferable_skills\": [skills from the resume that help bridge gaps]\\n        }}\\n\\n        No explanation, no markdown, just clean JSON.\\n\\n        Target Role: {target_role}\\n        Required Technical Skills: {\\', \\'.join(expected_tech)}\\n        Required Soft Skills: {\\', \\'.join(expected_soft)}\\n        Resume:\\n        {resume_text}\\n        \"\"\"\\n\\n    response = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant for skill gap analysis.\"},\\n        {\"role\": \"user\", \"content\": prompt}],\\n        temperature=0.0\\n        )\\n    raw = response.choices[0].message.content.strip()\\n    raw = re.sub(r\"```json|```\", \"\", raw)\\n    return json.loads(raw)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'get_skill_priorities_from_gpt', 'lines': '131-160'}, page_content='def get_skill_priorities_from_gpt(skills, role):\\n    prompt = f\"\"\"\\n    You are a career advisor. For the target role \\'{role}\\', prioritize the following skills based on the 80/20 (Pareto) principle.\\n\\n    Skills:\\n    {\\', \\'.join(skills)}\\n\\n    Return JSON mapping each skill to an importance score from 1 to 100 (higher means more important). Output should look like:\\n    {{\\n      \"Skill1\": 95,\\n      \"Skill2\": 90,\\n      ...\\n    }}\\n\\n    No explanation. No markdown. Only JSON.\\n    \"\"\"\\n    response = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that ranks skills by importance.\"},\\n            {\"role\": \"user\", \"content\": prompt}\\n        ],\\n        temperature=0.2\\n    )\\n    raw = response.choices[0].message.content.strip()\\n    raw = re.sub(r\"```json|```\", \"\", raw)\\n    try:\\n        return json.loads(raw)\\n    except json.JSONDecodeError:\\n        return {}'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'extract_json_from_response', 'lines': '164-171'}, page_content='def extract_json_from_response(raw):\\n    match = re.search(r\"\\\\{[\\\\s\\\\S]*\\\\}\", raw)\\n    if match:\\n        try:\\n            return json.loads(match.group())\\n        except json.JSONDecodeError:\\n            return {}\\n    return {}'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'load_skill_resources', 'lines': '173-175'}, page_content='def load_skill_resources():\\n    with open(\"/mount/src/stepupyourcareer.ai/StepUpAI/skill_resource_mapping.json\", \"r\") as f:\\n        return json.load(f)'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'split_skills_by_rag_presence', 'lines': '178-186'}, page_content='def split_skills_by_rag_presence(skills, rag_skill_keys):\\n    present = []\\n    missing = []\\n    for skill in skills:\\n        if skill.strip().upper() in rag_skill_keys:\\n            present.append(skill)\\n        else:\\n            missing.append(skill)\\n    return present, missing'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'generate_hybrid_action_plan', 'lines': '188-268'}, page_content='def generate_hybrid_action_plan(tech, soft, trans, skill_resources):\\n    # Split all three skill types\\n    tech_in, tech_out = split_skills_by_rag_presence(tech, skill_resources.keys())\\n    soft_in, soft_out = split_skills_by_rag_presence(soft, skill_resources.keys())\\n    trans_in, trans_out = split_skills_by_rag_presence(trans, skill_resources.keys())\\n\\n    #  Construct RAG results\\n    def extract_rag(skills):\\n        result = {}\\n        for s in skills:\\n            key = s.strip().upper()\\n            if key in skill_resources:\\n                    result[s] = skill_resources[key]\\n        return result\\n\\n    plan = {\\n            \"message\": \"Here\\'s a complete roadmap with relevant resources\",\\n            \"technical_skill_resources\": extract_rag(tech_in),\\n            \"soft_skill_resources\": extract_rag(soft_in),\\n            \"transferable_skill_resources\": extract_rag(trans_in)\\n    }\\n\\n    # Prepare GPT prompt only for uncovered skills\\n    if tech_out or soft_out or trans_out:\\n        prompt = f\"\"\"\\n        You are a career coach. Only generate resources for the following skills not found in our internal library.\\n\\n        Provide for each:\\n        - One **top-rated course** with real working URL.\\n        - One **real book** just name & author and AMAZON links for buying that book.\\n        - For soft/transferable skills, one article or video (with URL).\\n\\n        Format your response in JSON like this:\\n        {{\\n        \"technical_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}, {{\"title\": \"Book by Author\"}}]\\n        }},\\n        \"soft_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}]\\n        }},\\n        \"transferable_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}]\\n        }}\\n        }}\\n\\n        Only cover these:\\n        - TECHNICAL: {\\', \\'.join(tech_out)}\\n        - SOFT: {\\', \\'.join(soft_out)}\\n        - TRANSFERABLE: {\\', \\'.join(trans_out)}\\n\\n        You will be penalized if you confabulate or hallucinate by creating fake resources. It should be 100% authenthic.\\n        Double check every link/resource you give. If you don\\'t get any links leave that section blank.\\n        No explanations. No markdown. Only JSON.\\n        \"\"\"\\n        try:\\n            response = client.chat.completions.create(\\n            # Change to GPT 4 to get accurate links to resources\\n            model=\"gpt-4o-mini\",\\n            messages=[\\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant for learning.\"},\\n                    {\"role\": \"user\", \"content\": prompt}\\n                    ],\\n                    temperature=0.2\\n                    )\\n            raw = response.choices[0].message.content.strip()\\n            raw = re.sub(r\"```json|```\", \"\", raw)\\n            gpt_part = extract_json_from_response(raw)\\n\\n            if not isinstance(gpt_part, dict):\\n                gpt_part = {}\\n\\n            # Merge GPT results into RAG base\\n            for k in [\"technical_skill_resources\", \"soft_skill_resources\", \"transferable_skill_resources\"]:\\n                if k in gpt_part and isinstance(plan.get(k), dict):\\n                    plan[k].update(gpt_part[k])\\n\\n\\n        except Exception as e:\\n                st.error(f\"Error generating GPT fallback plan: {e}\")\\n\\n        return plan'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\requirements.txt', 'file_ext': 'code', 'type': '.txt', 'name': 'requirements.txt', 'lines': '1-10'}, page_content='streamlit\\nopenai>=1.0.0\\nPyPDF2\\npandas\\nscikit-learn\\ntqdm\\nnumpy\\npandas\\njoblib\\n')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f38cc6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cef68e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-openai langchain-community faiss-cpu\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "import hashlib, json, os\n",
    "\n",
    "from langchain_core.documents import Document  # or: from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# --- helpers ---------------------------------------------------------------\n",
    "\n",
    "def _stable_id(doc: Document) -> str:\n",
    "    meta_json = json.dumps(doc.metadata or {}, sort_keys=True, ensure_ascii=False)\n",
    "    return hashlib.md5((doc.page_content + \"||\" + meta_json).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def _norm_ext(ext: str) -> str:\n",
    "    ext = (ext or \"\").lower().lstrip(\".\")\n",
    "    return ext\n",
    "\n",
    "def _guess_ext(doc: Document) -> str:\n",
    "    # 1) try metadata.file_ext\n",
    "    ext = _norm_ext(str(doc.metadata.get(\"file_ext\", \"\")))\n",
    "    # some pipelines put \"code\" there; treat as unknown\n",
    "    if ext in {\"\", \"code\"}:\n",
    "        # 2) try source path\n",
    "        src = str(doc.metadata.get(\"source\", \"\"))\n",
    "        if src:\n",
    "            ext = _norm_ext(Path(src).suffix)\n",
    "    # 3) fall back to name\n",
    "    if not ext and doc.metadata.get(\"name\"):\n",
    "        ext = _norm_ext(Path(str(doc.metadata[\"name\"])).suffix)\n",
    "    return ext\n",
    "\n",
    "TEXT_EXTS = {\"md\", \"rst\", \"txt\"}                       # pure prose\n",
    "CONFIG_EXTS = {\"yml\", \"yaml\", \"toml\", \"ini\", \"cfg\"}    # config (we'll treat as CODE for retrieval)\n",
    "NB_EXTS = {\"ipynb\"}                                    # notebooks\n",
    "CODE_EXTS = {\"py\", \"js\", \"ts\", \"java\", \"go\", \"cpp\", \"c\", \"cs\", \"rb\", \"php\"}\n",
    "\n",
    "def _is_text(doc: Document) -> bool:\n",
    "    \"\"\"Decide TEXT vs CODE using both 'type' and real extension.\"\"\"\n",
    "    t = str(doc.metadata.get(\"type\", \"\")).lower()\n",
    "    ext = _guess_ext(doc)\n",
    "\n",
    "    # type-first rules\n",
    "    if \"docstring\" in t or t in {\"readme\", \"markdown\", \"inline_comment\", \"markdown_cell\", \"module_docstring\"}:\n",
    "        return True\n",
    "    if t in {\"functiondef\", \"asyncfunctiondef\", \"classdef\", \"code\", \"ipynb_cell\"}:\n",
    "        return False\n",
    "\n",
    "    # extension fallbacks\n",
    "    if ext in TEXT_EXTS:\n",
    "        return True\n",
    "    if ext in CODE_EXTS | CONFIG_EXTS | NB_EXTS:\n",
    "        return False\n",
    "\n",
    "    # special names\n",
    "    name = str(doc.metadata.get(\"name\", \"\")).lower()\n",
    "    if name in {\"readme.md\", \"readme.rst\", \"readme.txt\", \"license\", \"license.txt\"}:\n",
    "        return True\n",
    "    if name == \"requirements.txt\":  # treat as TEXT so overview/installation finds it\n",
    "        return True\n",
    "\n",
    "    # default to CODE (safer for repos)\n",
    "    return False\n",
    "\n",
    "# --- main saver ------------------------------------------------------------\n",
    "\n",
    "def save_to_faiss_split_by_ext(\n",
    "    chunks: List[Document],\n",
    "    base_dir: str = \"docs_index\",\n",
    "    model: str = \"text-embedding-3-small\",  # or \"text-embedding-3-large\"\n",
    "    min_chars: int = 30,\n",
    "    max_chars: int = 10000,\n",
    "):\n",
    "    # filter once\n",
    "    docs = [d for d in chunks if d and d.page_content and min_chars <= len(d.page_content) <= max_chars]\n",
    "\n",
    "    # split\n",
    "    text_docs = [d for d in docs if _is_text(d)]\n",
    "    code_docs = [d for d in docs if not _is_text(d)]\n",
    "\n",
    "    # embedder\n",
    "    embedder = OpenAIEmbeddings(model=model)\n",
    "\n",
    "    # TEXT index\n",
    "    text_path = Path(base_dir) / \"text_index\"\n",
    "    text_path.mkdir(parents=True, exist_ok=True)\n",
    "    text_vs = FAISS.from_documents(text_docs, embedder, ids=[_stable_id(d) for d in text_docs])\n",
    "    text_vs.save_local(str(text_path))\n",
    "\n",
    "    # CODE index\n",
    "    code_path = Path(base_dir) / \"code_index\"\n",
    "    code_path.mkdir(parents=True, exist_ok=True)\n",
    "    code_vs = FAISS.from_documents(code_docs, embedder, ids=[_stable_id(d) for d in code_docs])\n",
    "    code_vs.save_local(str(code_path))\n",
    "\n",
    "    print(f\"‚úÖ saved:\\n  text -> {text_path}  ({len(text_docs)} docs)\\n  code -> {code_path}  ({len(code_docs)} docs)\")\n",
    "    return {\"text_count\": len(text_docs), \"code_count\": len(code_docs), \"model\": model}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0823cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ saved:\n",
      "  text -> docs_index\\text_index  (16 docs)\n",
      "  code -> docs_index\\code_index  (44 docs)\n",
      "{'text_count': 16, 'code_count': 44, 'model': 'text-embedding-3-small'}\n"
     ]
    }
   ],
   "source": [
    "stats = save_to_faiss_split_by_ext(chunks, base_dir=\"docs_index\", model=\"text-embedding-3-small\")\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "333f5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-openai langchain-community faiss-cpu chromadb\n",
    "\n",
    "import math, re\n",
    "from statistics import mean\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional, Literal\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "Backend = Literal[\"faiss\", \"chroma\"]\n",
    "\n",
    "def _load_store(backend: Backend, path: str, model: str, collection: Optional[str] = None):\n",
    "    embedder = OpenAIEmbeddings(model=model)\n",
    "    if backend == \"faiss\":\n",
    "        return FAISS.load_local(path, embedder, allow_dangerous_deserialization=True), embedder\n",
    "    if backend == \"chroma\":\n",
    "        return Chroma(\n",
    "            collection_name=collection or \"default\",\n",
    "            persist_directory=path,\n",
    "            embedding_function=embedder,\n",
    "        ), embedder\n",
    "    raise ValueError(\"backend must be 'faiss' or 'chroma'\")\n",
    "\n",
    "def _tokenize(text: str) -> List[str]:\n",
    "    return [t for t in re.findall(r\"[A-Za-z0-9_]+\", text.lower()) if len(t) > 1]\n",
    "\n",
    "def _bm25_scores(query: str, docs: Dict[str, Document]) -> Dict[str, float]:\n",
    "    \"\"\"BM25 over the candidate set (good enough for re-ranking).\"\"\"\n",
    "    q_terms = _tokenize(query)\n",
    "    if not q_terms: \n",
    "        return {k: 0.0 for k in docs.keys()}\n",
    "    tok = {k: _tokenize(v.page_content or \"\") for k, v in docs.items()}\n",
    "    lengths = {k: len(v) for k, v in tok.items()}\n",
    "    N = len(tok)\n",
    "    avgdl = max(1.0, mean(lengths.values())) if lengths else 1.0\n",
    "    # document frequency\n",
    "    df = {}\n",
    "    for terms in tok.values():\n",
    "        seen = set(terms)\n",
    "        for t in seen:\n",
    "            df[t] = df.get(t, 0) + 1\n",
    "    # idf\n",
    "    idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1.0) for t in set(q_terms)}\n",
    "    k1, b = 1.5, 0.75\n",
    "    scores = {}\n",
    "    for did, terms in tok.items():\n",
    "        score = 0.0\n",
    "        tf_counts = {}\n",
    "        for t in terms:\n",
    "            tf_counts[t] = tf_counts.get(t, 0) + 1\n",
    "        for qt in q_terms:\n",
    "            tf = tf_counts.get(qt, 0)\n",
    "            if tf == 0: \n",
    "                continue\n",
    "            denom = tf + k1 * (1 - b + b * (lengths[did] / avgdl))\n",
    "            score += idf.get(qt, 0.0) * (tf * (k1 + 1)) / (denom if denom else 1.0)\n",
    "        scores[did] = score\n",
    "    return scores\n",
    "\n",
    "def _minmax_norm(scores: Dict[str, float]) -> Dict[str, float]:\n",
    "    if not scores: return {}\n",
    "    vals = list(scores.values())\n",
    "    lo, hi = min(vals), max(vals)\n",
    "    if hi == lo: \n",
    "        return {k: 0.0 for k in scores}\n",
    "    return {k: (v - lo) / (hi - lo) for k, v in scores.items()}\n",
    "\n",
    "def hybrid_search(\n",
    "    query: str,\n",
    "    *,\n",
    "    # where your indexes live:\n",
    "    backend: Backend = \"faiss\",\n",
    "    text_path: str = \"docs_index/text_index\",\n",
    "    code_path: str = \"docs_index/code_index\",\n",
    "    collection_text: Optional[str] = None,  # for Chroma\n",
    "    collection_code: Optional[str] = None,  # for Chroma\n",
    "    model: str = \"text-embedding-3-small\",\n",
    "    # retrieval sizes:\n",
    "    k_text: int = 30,\n",
    "    k_code: int = 30,\n",
    "    k_final: int = 12,\n",
    "    # weights in final score:\n",
    "    weight_vector: float = 0.7,\n",
    "    weight_bm25: float = 0.3,\n",
    "    # optional metadata filter over the merged candidates:\n",
    "    metadata_filter: Optional[callable] = None,\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Returns a list of (Document, score) after merging:\n",
    "      1) vector top-K from text index\n",
    "      2) vector top-K from code index\n",
    "      3) BM25 re-rank over the union\n",
    "    \"\"\"\n",
    "    text_vs, _ = _load_store(backend, text_path, model, collection_text)\n",
    "    code_vs, _ = _load_store(backend, code_path, model, collection_code)\n",
    "\n",
    "    # 1) vector retrieval (note: FAISS returns distance; smaller is better)\n",
    "    text_hits = text_vs.similarity_search_with_score(query, k=k_text)\n",
    "    code_hits = code_vs.similarity_search_with_score(query, k=k_code)\n",
    "\n",
    "    # unify candidates by creating synthetic IDs if missing\n",
    "    def _doc_id(d: Document) -> str:\n",
    "        # try to use original id if present\n",
    "        return d.metadata.get(\"id\") or d.metadata.get(\"_id\") or f\"{d.metadata.get('source','')}::{d.metadata.get('lines','')}::{hash(d.page_content)}\"\n",
    "\n",
    "    candidates: Dict[str, Tuple[Document, float]] = {}\n",
    "    for doc, score in text_hits + code_hits:\n",
    "        did = _doc_id(doc)\n",
    "        # convert distance-like score to similarity (higher is better)\n",
    "        sim = 1.0 / (1.0 + max(1e-9, score))\n",
    "        if (did not in candidates) or (sim > candidates[did][1]):\n",
    "            candidates[did] = (doc, sim)\n",
    "\n",
    "    # optional metadata filtering\n",
    "    if metadata_filter:\n",
    "        candidates = {k: v for k, v in candidates.items() if metadata_filter(v[0])}\n",
    "\n",
    "    if not candidates:\n",
    "        return []\n",
    "\n",
    "    # 2) BM25 over candidate set\n",
    "    cand_docs = {did: d for did, (d, _) in candidates.items()}\n",
    "    bm25 = _bm25_scores(query, cand_docs)\n",
    "\n",
    "    # 3) normalize & blend\n",
    "    vec_norm = _minmax_norm({did: sim for did, (_, sim) in candidates.items()})\n",
    "    bm_norm = _minmax_norm(bm25)\n",
    "\n",
    "    final: List[Tuple[Document, float]] = []\n",
    "    for did, (doc, _) in candidates.items():\n",
    "        score = weight_vector * vec_norm.get(did, 0.0) + weight_bm25 * bm_norm.get(did, 0.0)\n",
    "        final.append((doc, score))\n",
    "\n",
    "    final.sort(key=lambda x: x[1], reverse=True)\n",
    "    return final[:k_final]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "255ddc12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\adars\\\\Documents\\\\UCD\\\\sem 3\\\\Automated Technical Documentation from Code Repositories\\\\notebooks'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e877cdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01  score=0.908  functiondef  C:\\Users\\adars\\AppData\\Local\\Temp\\repo_5xlubq3o\\StepUpAI\\app.py:188-268\n",
      "02  score=0.699  functiondef  C:\\Users\\adars\\AppData\\Local\\Temp\\repo_5xlubq3o\\StepUpAI\\app.py:24-35\n",
      "03  score=0.361  functiondef  C:\\Users\\adars\\AppData\\Local\\Temp\\repo_5xlubq3o\\StepUpAI\\app.py:131-160\n",
      "04  score=0.242  functiondef  C:\\Users\\adars\\AppData\\Local\\Temp\\repo_5xlubq3o\\StepUpAI\\app.py:70-79\n",
      "05  score=0.192  functiondef  C:\\Users\\adars\\AppData\\Local\\Temp\\repo_5xlubq3o\\StepUpAI\\app.py:89-129\n",
      "06  score=0.136  functiondef  C:\\Users\\adars\\AppData\\Local\\Temp\\repo_5xlubq3o\\StepUpAI\\app.py:173-175\n",
      "07  score=0.115  functiondef  C:\\Users\\adars\\AppData\\Local\\Temp\\repo_5xlubq3o\\StepUpAI\\app.py:37-44\n",
      "08  score=0.000  functiondef  C:\\Users\\adars\\AppData\\Local\\Temp\\repo_5xlubq3o\\StepUpAI\\app.py:60-62\n"
     ]
    }
   ],
   "source": [
    "results = hybrid_search(\n",
    "    \"We need to write system Architecture?\",\n",
    "    backend=\"faiss\",\n",
    "    text_path=\"docs_index/text_index\",\n",
    "    code_path=\"docs_index/code_index\",\n",
    "    model=\"text-embedding-3-small\",   # or \"text-embedding-3-large\"\n",
    "    k_text=30, k_code=30, k_final=12,\n",
    "    # e.g., only functions/classes:\n",
    "    metadata_filter=lambda d: d.metadata.get(\"type\") in {\"functiondef\",\"classdef\",\"asyncfunctiondef\"}\n",
    ")\n",
    "\n",
    "for i, (doc, score) in enumerate(results, 1):\n",
    "    print(f\"{i:02d}  score={score:.3f}  {doc.metadata.get('type')}  {doc.metadata.get('source')}:{doc.metadata.get('lines')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31d27d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='57fcc19e78979d4dddf2194ae943f650', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5xlubq3o\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'generate_hybrid_action_plan', 'lines': '188-268'}, page_content='def generate_hybrid_action_plan(tech, soft, trans, skill_resources):\\n    # Split all three skill types\\n    tech_in, tech_out = split_skills_by_rag_presence(tech, skill_resources.keys())\\n    soft_in, soft_out = split_skills_by_rag_presence(soft, skill_resources.keys())\\n    trans_in, trans_out = split_skills_by_rag_presence(trans, skill_resources.keys())\\n\\n    #  Construct RAG results\\n    def extract_rag(skills):\\n        result = {}\\n        for s in skills:\\n            key = s.strip().upper()\\n            if key in skill_resources:\\n                    result[s] = skill_resources[key]\\n        return result\\n\\n    plan = {\\n            \"message\": \"Here\\'s a complete roadmap with relevant resources\",\\n            \"technical_skill_resources\": extract_rag(tech_in),\\n            \"soft_skill_resources\": extract_rag(soft_in),\\n            \"transferable_skill_resources\": extract_rag(trans_in)\\n    }\\n\\n    # Prepare GPT prompt only for uncovered skills\\n    if tech_out or soft_out or trans_out:\\n        prompt = f\"\"\"\\n        You are a career coach. Only generate resources for the following skills not found in our internal library.\\n\\n        Provide for each:\\n        - One **top-rated course** with real working URL.\\n        - One **real book** just name & author and AMAZON links for buying that book.\\n        - For soft/transferable skills, one article or video (with URL).\\n\\n        Format your response in JSON like this:\\n        {{\\n        \"technical_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}, {{\"title\": \"Book by Author\"}}]\\n        }},\\n        \"soft_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}]\\n        }},\\n        \"transferable_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}]\\n        }}\\n        }}\\n\\n        Only cover these:\\n        - TECHNICAL: {\\', \\'.join(tech_out)}\\n        - SOFT: {\\', \\'.join(soft_out)}\\n        - TRANSFERABLE: {\\', \\'.join(trans_out)}\\n\\n        You will be penalized if you confabulate or hallucinate by creating fake resources. It should be 100% authenthic.\\n        Double check every link/resource you give. If you don\\'t get any links leave that section blank.\\n        No explanations. No markdown. Only JSON.\\n        \"\"\"\\n        try:\\n            response = client.chat.completions.create(\\n            # Change to GPT 4 to get accurate links to resources\\n            model=\"gpt-4o-mini\",\\n            messages=[\\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant for learning.\"},\\n                    {\"role\": \"user\", \"content\": prompt}\\n                    ],\\n                    temperature=0.2\\n                    )\\n            raw = response.choices[0].message.content.strip()\\n            raw = re.sub(r\"```json|```\", \"\", raw)\\n            gpt_part = extract_json_from_response(raw)\\n\\n            if not isinstance(gpt_part, dict):\\n                gpt_part = {}\\n\\n            # Merge GPT results into RAG base\\n            for k in [\"technical_skill_resources\", \"soft_skill_resources\", \"transferable_skill_resources\"]:\\n                if k in gpt_part and isinstance(plan.get(k), dict):\\n                    plan[k].update(gpt_part[k])\\n\\n\\n        except Exception as e:\\n                st.error(f\"Error generating GPT fallback plan: {e}\")\\n\\n        return plan'),\n",
       "  np.float32(0.90848595)),\n",
       " (Document(id='5882ded6b913fc19c3e5e2ef67d93402', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5xlubq3o\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'page_1', 'lines': '24-35'}, page_content='def page_1():\\n    st.title(\"Welcome to StepUpYourCareer.AI\")\\n    st.markdown(\"##### Let\\'s get started with a few details.\")\\n\\n    name = st.text_input(\"Your Full Name\")\\n    email = st.text_input(\"Your Email Address\")\\n\\n    if name and email:\\n        if st.button(\"‚û°Ô∏è Proceed to Resume Analysis\"):\\n            st.session_state.name = name\\n            st.session_state.email = email\\n            st.session_state.page = 2'),\n",
       "  np.float32(0.6987705)),\n",
       " (Document(id='025a8166ebe7ea5a94ce036a8eaee83f', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5xlubq3o\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'get_skill_priorities_from_gpt', 'lines': '131-160'}, page_content='def get_skill_priorities_from_gpt(skills, role):\\n    prompt = f\"\"\"\\n    You are a career advisor. For the target role \\'{role}\\', prioritize the following skills based on the 80/20 (Pareto) principle.\\n\\n    Skills:\\n    {\\', \\'.join(skills)}\\n\\n    Return JSON mapping each skill to an importance score from 1 to 100 (higher means more important). Output should look like:\\n    {{\\n      \"Skill1\": 95,\\n      \"Skill2\": 90,\\n      ...\\n    }}\\n\\n    No explanation. No markdown. Only JSON.\\n    \"\"\"\\n    response = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[\\n            {\"role\": \"system\", \"content\": \"You are a helpful assistant that ranks skills by importance.\"},\\n            {\"role\": \"user\", \"content\": prompt}\\n        ],\\n        temperature=0.2\\n    )\\n    raw = response.choices[0].message.content.strip()\\n    raw = re.sub(r\"```json|```\", \"\", raw)\\n    try:\\n        return json.loads(raw)\\n    except json.JSONDecodeError:\\n        return {}'),\n",
       "  np.float32(0.3614008)),\n",
       " (Document(id='27954e5d438569fd9384ff7cf6196ec7', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5xlubq3o\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'anonymize', 'lines': '70-79'}, page_content='def anonymize(text):\\n    response = client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that removes personal identifiers from resumes.\"},\\n        {\"role\": \"user\", \"content\": f\"Anonymize this resume:\\\\n\\\\n{text}\"}\\n        ],\\n        temperature=0\\n        )\\n    return response.choices[0].message.content.strip()'),\n",
       "  np.float32(0.2416063)),\n",
       " (Document(id='0390998282de7617508590ceec2edf77', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5xlubq3o\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'generate_skill_gap', 'lines': '89-129'}, page_content='def generate_skill_gap(resume_text, target_role, retrieved_examples, fallback_skills):\\n    examples_prompt = \"\\\\n\\\\n\".join([\\n    f\"Example for role {ex[\\'target_role\\']}:\\\\nResume: {ex[\\'resume_summary\\']}\\\\nSkill Gaps: tech={ex[\\'technical_skill_gap\\']}, soft={ex[\\'soft_skill_gap\\']}, transferable={ex[\\'transferable_skills\\']}\"\\n    for ex in retrieved_examples\\n        ])\\n    expected_tech = fallback_skills.get(\"technical_skills\", [])\\n    expected_soft = fallback_skills.get(\"soft_skills\", [])\\n\\n    prompt = f\"\"\"\\n        You are a highly experienced career advisor. Based on examples and required skills, identify skill gaps.\\n\\n        ## EXAMPLES\\n        {examples_prompt}\\n\\n        ## TASK:\\n\\n        Analyze the new resume below and return only JSON:\\n        {{\\n        \"technical_skill_gaps\": [list of missing technical skills],\\n        \"soft_skill_gaps\": [list of missing soft skills],\\n        \"transferable_skills\": [skills from the resume that help bridge gaps]\\n        }}\\n\\n        No explanation, no markdown, just clean JSON.\\n\\n        Target Role: {target_role}\\n        Required Technical Skills: {\\', \\'.join(expected_tech)}\\n        Required Soft Skills: {\\', \\'.join(expected_soft)}\\n        Resume:\\n        {resume_text}\\n        \"\"\"\\n\\n    response = client.chat.completions.create(\\n        model=\"gpt-4o-mini\",\\n        messages=[{\"role\": \"system\", \"content\": \"You are a helpful assistant for skill gap analysis.\"},\\n        {\"role\": \"user\", \"content\": prompt}],\\n        temperature=0.0\\n        )\\n    raw = response.choices[0].message.content.strip()\\n    raw = re.sub(r\"```json|```\", \"\", raw)\\n    return json.loads(raw)'),\n",
       "  np.float32(0.19196197)),\n",
       " (Document(id='0df89ad1e43cd3c49bb45b9066725241', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5xlubq3o\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'load_skill_resources', 'lines': '173-175'}, page_content='def load_skill_resources():\\n    with open(\"/mount/src/stepupyourcareer.ai/StepUpAI/skill_resource_mapping.json\", \"r\") as f:\\n        return json.load(f)'),\n",
       "  np.float32(0.13581377)),\n",
       " (Document(id='0c7f3db9c1019b2c9b4f4d21cc4ca607', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5xlubq3o\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'load_examples', 'lines': '37-44'}, page_content='def load_examples():\\n            with open(\"/mount/src/stepupyourcareer.ai/StepUpAI/skill_gap_analysis.json\", \"r\") as f:\\n                examples = json.load(f)\\n            example_texts = [\\n                f\"Resume: {ex[\\'resume_summary\\']} | Role: {ex[\\'target_role\\']}\" for ex in examples\\n            ]\\n            embeddings = [get_embedding(text) for text in example_texts]\\n            return examples, embeddings'),\n",
       "  np.float32(0.11499333)),\n",
       " (Document(id='a1d9d327a4fa814cbb95893741f191ce', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_5xlubq3o\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'get_embedding', 'lines': '60-62'}, page_content='def get_embedding(text, model=\"text-embedding-ada-002\"):\\n    response = client.embeddings.create(input=[text], model=model)\\n    return response.data[0].embedding'),\n",
       "  np.float32(0.0))]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7104a871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain-openai langchain-community faiss-cpu\n",
    "\n",
    "import re, time\n",
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Literal, Tuple\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "Route = Literal[\"auto\", \"text\", \"code\", \"both\"]\n",
    "\n",
    "# ---- simple router ---------------------------------------------------------\n",
    "_ARCH_HINTS = re.compile(r\"\\b(arch(itecture)?|overview|design|diagram|components|topology)\\b\", re.I)\n",
    "\n",
    "def _route_query(q: str) -> Route:\n",
    "    return \"text\" if _ARCH_HINTS.search(q) else \"both\"\n",
    "\n",
    "# ---- caches ---------------------------------------------------------------\n",
    "@lru_cache(maxsize=2)\n",
    "def _get_embedder(model: str) -> OpenAIEmbeddings:\n",
    "    return OpenAIEmbeddings(model=model)\n",
    "\n",
    "@lru_cache(maxsize=8)\n",
    "def _load_faiss(path: str, model: str) -> FAISS:\n",
    "    emb = _get_embedder(model)\n",
    "    return FAISS.load_local(path, emb, allow_dangerous_deserialization=True)\n",
    "\n",
    "# ---- main search ----------------------------------------------------------\n",
    "def fast_search(\n",
    "    query: str,\n",
    "    *,\n",
    "    base_dir: str = \"docs_index\",\n",
    "    model: str = \"text-embedding-3-small\",   # or \"text-embedding-3-large\" (must match index dims)\n",
    "    route: Route = \"auto\",                   # auto|text|code|both\n",
    "    k_text: int = 8,\n",
    "    k_code: int = 12,\n",
    "    k_final: int = 20,\n",
    "    use_bm25: bool = False,                  # off by default for speed\n",
    "    types_include: Optional[set] = None,     # e.g. {\"module_docstring\",\"readme\",\"markdown\"}\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Fast retrieval:\n",
    "      - routes to the right index(es)\n",
    "      - uses MMR vector search for diversity\n",
    "      - (optional) BM25 re-rank on the small union\n",
    "    Returns [(doc, score)] with highest score first.\n",
    "    \"\"\"\n",
    "    t0 = time.time()\n",
    "    if route == \"auto\":\n",
    "        route = _route_query(query)\n",
    "\n",
    "    text_hits: List[Document] = []\n",
    "    code_hits: List[Document] = []\n",
    "\n",
    "    # TEXT index\n",
    "    if route in (\"text\", \"both\"):\n",
    "        text_vs = _load_faiss(str(Path(base_dir) / \"text_index\"), model)\n",
    "        # fetch_k > k for MMR diversity but still cheap\n",
    "        text_hits = text_vs.max_marginal_relevance_search(query, k=k_text, fetch_k=min(4 * k_text, 64))\n",
    "\n",
    "    # CODE index\n",
    "    if route in (\"code\", \"both\"):\n",
    "        code_vs = _load_faiss(str(Path(base_dir) / \"code_index\"), model)\n",
    "        code_hits = code_vs.max_marginal_relevance_search(query, k=k_code, fetch_k=min(4 * k_code, 64))\n",
    "\n",
    "    # Merge + (optional) metadata filter\n",
    "    hits = text_hits + code_hits\n",
    "    if types_include:\n",
    "        hits = [d for d in hits if d.metadata.get(\"type\") in types_include]\n",
    "\n",
    "    # De-dup by a stable key\n",
    "    seen, deduped = set(), []\n",
    "    for d in hits:\n",
    "        key = (d.metadata.get(\"source\"), d.metadata.get(\"lines\"), d.page_content[:64])\n",
    "        if key not in seen:\n",
    "            seen.add(key)\n",
    "            deduped.append(d)\n",
    "\n",
    "    # Quick vector scores for sort (convert FAISS distance-like to similarity)\n",
    "    # We'll re-embed the query once and score docs by inner product with doc vectors via FAISS again is heavy;\n",
    "    # instead, reuse similarity_search_with_score on a small candidate set: emulate by re-querying with a higher k\n",
    "    # OR simply keep MMR order. For speed, we keep MMR order unless BM25 is enabled.\n",
    "    ordered_docs = deduped\n",
    "\n",
    "    # Optional: light BM25 over the small union (simple lexical boost)\n",
    "    if use_bm25:\n",
    "        import math\n",
    "        import re as _re\n",
    "        def _tok(t): return _re.findall(r\"[A-Za-z0-9_]+\", t.lower())\n",
    "        q_terms = _tok(query)\n",
    "        df = {}\n",
    "        toks = []\n",
    "        for d in ordered_docs:\n",
    "            ts = _tok(d.page_content)\n",
    "            toks.append(ts)\n",
    "            for term in set(ts): df[term] = df.get(term, 0) + 1\n",
    "        N = max(1, len(ordered_docs))\n",
    "        idf = {t: math.log((N - df.get(t, 0) + 0.5) / (df.get(t, 0) + 0.5) + 1.0) for t in set(q_terms)}\n",
    "        k1, b = 1.5, 0.75\n",
    "        lens = [len(ts) for ts in toks]\n",
    "        avgdl = max(1, sum(lens) / len(lens))\n",
    "        scores = []\n",
    "        for d, ts, L in zip(ordered_docs, toks, lens):\n",
    "            tf = {}\n",
    "            for t in ts: tf[t] = tf.get(t, 0) + 1\n",
    "            s = 0.0\n",
    "            for t in q_terms:\n",
    "                f = tf.get(t, 0)\n",
    "                if not f: continue\n",
    "                denom = f + k1 * (1 - b + b * (L / avgdl))\n",
    "                s += idf.get(t, 0.0) * (f * (k1 + 1)) / (denom or 1.0)\n",
    "            scores.append((d, s))\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        ordered_docs = [d for d, _ in scores]\n",
    "\n",
    "    # Cap results\n",
    "    ordered_docs = ordered_docs[:k_final]\n",
    "\n",
    "    # Attach a simple descending score for convenience (1..0)\n",
    "    out = [(d, 1.0 - i / max(1, len(ordered_docs))) for i, d in enumerate(ordered_docs)]\n",
    "    t1 = time.time()\n",
    "    # print(f\"[fast_search] route={route} hits={len(out)} in {(t1-t0)*1000:.1f} ms\")\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "357496cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fast_search(\n",
    "    \"We need to write system Architecture?\",\n",
    "    route=\"both\",\n",
    "    k_text=5, k_code=15, k_final=20,\n",
    "    use_bm25=True\n",
    "    #types_include={\"functiondef\",\"classdef\",\"asyncfunctiondef\",\"functiondef_docstring\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a162a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='78a43a24648c4ae08bacee6f459510d9', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'anonymize', 'lines': '70-79'}, page_content='def anonymize(text):\\n    response = client.chat.completions.create(\\n    model=\"gpt-4o-mini\",\\n    messages=[\\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant that removes personal identifiers from resumes.\"},\\n        {\"role\": \"user\", \"content\": f\"Anonymize this resume:\\\\n\\\\n{text}\"}\\n        ],\\n        temperature=0\\n        )\\n    return response.choices[0].message.content.strip()'),\n",
       "  1.0),\n",
       " (Document(id='4081e793c3e1332769db725a394cc6e1', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'page_1', 'lines': '24-35'}, page_content='def page_1():\\n    st.title(\"Welcome to StepUpYourCareer.AI\")\\n    st.markdown(\"##### Let\\'s get started with a few details.\")\\n\\n    name = st.text_input(\"Your Full Name\")\\n    email = st.text_input(\"Your Email Address\")\\n\\n    if name and email:\\n        if st.button(\"‚û°Ô∏è Proceed to Resume Analysis\"):\\n            st.session_state.name = name\\n            st.session_state.email = email\\n            st.session_state.page = 2'),\n",
       "  0.95),\n",
       " (Document(id='1a500757ee121082497d8e82c0210d81', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\README.md', 'file_ext': 'text', 'type': 'readme', 'name': 'README.md', 'lines': '1-29'}, page_content='# StepUpYourCareer.ai: Elevate Your Future\\n\\nAn AI-powered career assistant that helps students and job seekers identify **skill gaps**, receive **personalized learning roadmaps**, and connect with **industry mentors**‚Äîall from a single resume upload.\\n\\n### Link to the website: https://stepupyourcareer.streamlit.app/\\n\\n---\\n\\n## Problem\\n\\nGraduates often leave university with degrees but **lack clarity on what employers actually expect**. They spend months applying for jobs, facing rejections without knowing **what skills they‚Äôre missing** or **how to upskill efficiently**.\\n\\n---\\n\\n## Solution\\n\\n**StepUpYourCareer.ai** transforms your resume into a personalized upskilling journey.\\n\\n- **Skill Gap Analyzer**: Extracts skills from your resume and compares them to your target role\\n- **Action Plan Generator**: Recommends curated online courses and resources for each missing skill\\n- **Mentor Matching**: Clusters users and mentors using K-Means to connect you with experts in your domain\\n\\n---\\n\\n![Notes_250516_042908_1](https://github.com/user-attachments/assets/a42216a8-e04c-4335-aad7-d56a61cc873b)\\n\\n![Notes_250516_042908_4](https://github.com/user-attachments/assets/ba3b34ec-57ec-4bf5-906a-84af82342b8b)\\n\\n![Notes_250516_042908_3](https://github.com/user-attachments/assets/4b7e5f11-7923-427c-be95-cbcd7d919c5b)'),\n",
       "  0.9),\n",
       " (Document(id='5ecdc5e5790667ea5c6fced85cecb916', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'generate_hybrid_action_plan', 'lines': '188-268'}, page_content='def generate_hybrid_action_plan(tech, soft, trans, skill_resources):\\n    # Split all three skill types\\n    tech_in, tech_out = split_skills_by_rag_presence(tech, skill_resources.keys())\\n    soft_in, soft_out = split_skills_by_rag_presence(soft, skill_resources.keys())\\n    trans_in, trans_out = split_skills_by_rag_presence(trans, skill_resources.keys())\\n\\n    #  Construct RAG results\\n    def extract_rag(skills):\\n        result = {}\\n        for s in skills:\\n            key = s.strip().upper()\\n            if key in skill_resources:\\n                    result[s] = skill_resources[key]\\n        return result\\n\\n    plan = {\\n            \"message\": \"Here\\'s a complete roadmap with relevant resources\",\\n            \"technical_skill_resources\": extract_rag(tech_in),\\n            \"soft_skill_resources\": extract_rag(soft_in),\\n            \"transferable_skill_resources\": extract_rag(trans_in)\\n    }\\n\\n    # Prepare GPT prompt only for uncovered skills\\n    if tech_out or soft_out or trans_out:\\n        prompt = f\"\"\"\\n        You are a career coach. Only generate resources for the following skills not found in our internal library.\\n\\n        Provide for each:\\n        - One **top-rated course** with real working URL.\\n        - One **real book** just name & author and AMAZON links for buying that book.\\n        - For soft/transferable skills, one article or video (with URL).\\n\\n        Format your response in JSON like this:\\n        {{\\n        \"technical_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}, {{\"title\": \"Book by Author\"}}]\\n        }},\\n        \"soft_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}]\\n        }},\\n        \"transferable_skill_resources\": {{\\n            \"Skill\": [{{\"title\": \"...\", \"url\": \"...\" }}]\\n        }}\\n        }}\\n\\n        Only cover these:\\n        - TECHNICAL: {\\', \\'.join(tech_out)}\\n        - SOFT: {\\', \\'.join(soft_out)}\\n        - TRANSFERABLE: {\\', \\'.join(trans_out)}\\n\\n        You will be penalized if you confabulate or hallucinate by creating fake resources. It should be 100% authenthic.\\n        Double check every link/resource you give. If you don\\'t get any links leave that section blank.\\n        No explanations. No markdown. Only JSON.\\n        \"\"\"\\n        try:\\n            response = client.chat.completions.create(\\n            # Change to GPT 4 to get accurate links to resources\\n            model=\"gpt-4o-mini\",\\n            messages=[\\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant for learning.\"},\\n                    {\"role\": \"user\", \"content\": prompt}\\n                    ],\\n                    temperature=0.2\\n                    )\\n            raw = response.choices[0].message.content.strip()\\n            raw = re.sub(r\"```json|```\", \"\", raw)\\n            gpt_part = extract_json_from_response(raw)\\n\\n            if not isinstance(gpt_part, dict):\\n                gpt_part = {}\\n\\n            # Merge GPT results into RAG base\\n            for k in [\"technical_skill_resources\", \"soft_skill_resources\", \"transferable_skill_resources\"]:\\n                if k in gpt_part and isinstance(plan.get(k), dict):\\n                    plan[k].update(gpt_part[k])\\n\\n\\n        except Exception as e:\\n                st.error(f\"Error generating GPT fallback plan: {e}\")\\n\\n        return plan'),\n",
       "  0.85),\n",
       " (Document(id='e023515213994db88e91934c6225b6d3', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 10', 'lines': 'cell_10'}, page_content='k_val = np.arange(10, 30)\\nbest_k = None\\nbest_score = -1\\nshilloutte_scores = []\\ninertias = []\\nch_indexs = []\\n\\nfor k in k_val:\\n    temp_sil_scores = []\\n    temp_ch_scores = []\\n    temp_inertias = []\\n\\n    for run in range(100):  # 100 is heavy; reduce to 5 or 10 for dev\\n        kmeans = KMeans(n_clusters=k, random_state=run, n_init=\\'auto\\')\\n        labels = kmeans.fit_predict(skills_df)\\n\\n        sil_score = silhouette_score(skills_df, labels)\\n        ch_index = calinski_harabasz_score(skills_df, labels)\\n\\n        temp_sil_scores.append(sil_score)\\n        temp_ch_scores.append(ch_index)\\n        temp_inertias.append(kmeans.inertia_)\\n\\n    # Average over all runs\\n    avg_sil = np.mean(temp_sil_scores)\\n    avg_ch = np.mean(temp_ch_scores)\\n    avg_inertia = np.mean(temp_inertias)\\n\\n    shilloutte_scores.append(avg_sil)\\n    ch_indexs.append(avg_ch)\\n    inertias.append(avg_inertia)\\n\\n    if avg_sil > best_score:\\n        best_score = avg_sil\\n        best_k = k\\n\\nprint(f\"‚úÖ Best K = {best_k} with silhouette score = {best_score:.4f}\")\\n\\n# üìà Plot all 3 metrics\\nplt.figure(figsize=(15, 4))\\n\\nplt.subplot(1, 3, 1)\\nplt.plot(k_val, inertias, marker=\\'o\\')\\nplt.title(\"Elbow Plot (Inertia)\")\\nplt.xlabel(\"Number of Clusters (K)\")\\nplt.ylabel(\"Inertia\")\\nplt.grid(True)\\n\\nplt.subplot(1, 3, 2)\\nplt.plot(k_val, shilloutte_scores, marker=\\'s\\', color=\\'green\\')\\nplt.title(\"Silhouette Score\")\\nplt.xlabel(\"Number of Clusters (K)\")\\nplt.ylabel(\"Score\")\\nplt.grid(True)\\n\\nplt.subplot(1, 3, 3)\\nplt.plot(k_val, ch_indexs, marker=\\'^\\', color=\\'purple\\')\\nplt.title(\"Calinski-Harabasz Index\")\\nplt.xlabel(\"Number of Clusters (K)\")\\nplt.ylabel(\"Score\")\\nplt.grid(True)\\n\\nplt.tight_layout()\\nplt.show()'),\n",
       "  0.8),\n",
       " (Document(id='5841abfbafb745c6404206baa01da2b0', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 23', 'lines': 'cell_23'}, page_content='### Action Plan Generation (Testing for 3 Candidates)'),\n",
       "  0.75),\n",
       " (Document(id='35aea59fd408783aa610f5b1035ff731', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\requirements.txt', 'file_ext': 'code', 'type': '.txt', 'name': 'requirements.txt', 'lines': '1-10'}, page_content='streamlit\\nopenai>=1.0.0\\nPyPDF2\\npandas\\nscikit-learn\\ntqdm\\nnumpy\\npandas\\njoblib\\n'),\n",
       "  0.7),\n",
       " (Document(id='84990bc65122799dfd38649956fcd199', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 16', 'lines': 'cell_16'}, page_content='### Clustering happeining on skills cause within each role there are still overlapping clusters, means skills are common within each roles'),\n",
       "  0.65),\n",
       " (Document(id='322c5a3ae5ae125c76ba41d56abb98af', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 12', 'lines': 'cell_12'}, page_content='### Printing 120 job descriptions for each role (Total 600)'),\n",
       "  0.6),\n",
       " (Document(id='bc33039ccdc20c87bbdd441cdab439dc', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 6', 'lines': 'cell_6'}, page_content='uploaded = files.upload()\\n\\nfor fn in uploaded.keys():\\n  print(\\'User uploaded file \"{name}\" with length {length} bytes\\'.format(\\n      name=fn, length=len(uploaded[fn])))'),\n",
       "  0.55),\n",
       " (Document(id='f4da72310e5d994f7914caab19ef8597', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 5', 'lines': 'cell_5'}, page_content='from google.colab import sheets\\nsheet = sheets.InteractiveSheet(df=req_jobs_desc)'),\n",
       "  0.5),\n",
       " (Document(id='0d361c7e97726669bfefa26ab3d2216c', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'get_embedding', 'lines': '60-62'}, page_content='def get_embedding(text, model=\"text-embedding-ada-002\"):\\n    response = client.embeddings.create(input=[text], model=model)\\n    return response.data[0].embedding'),\n",
       "  0.44999999999999996),\n",
       " (Document(id='362ffea1e04339aef6b9e4b3a172cb46', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 5', 'lines': 'cell_5'}, page_content='roles = [\\n    \"Artificial Intelligence\",\\n    \"Business Analyst\",\\n    \"Business Intelligence Analyst\",\\n    \"Data Analyst\",\\n    \"Machine Learning\"\\n]'),\n",
       "  0.4),\n",
       " (Document(id='b21a1158744c12927c30eba8bc33777a', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 9', 'lines': 'cell_9'}, page_content=\"sim_resumes.to_json('sim_resume.json', orient='records', lines=False)\"),\n",
       "  0.35),\n",
       " (Document(id='5f4fbd79a7110aa4aa5081a98823624b', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 2', 'lines': 'cell_2'}, page_content=\"# !pip install PyPDF2 openai\\nimport openai\\nimport json\\nimport time\\nimport re\\nfrom openai import OpenAI\\nfrom google.colab import userdata\\nimport pandas as pd\\npd.set_option('display.max_colwidth', None)\\nfrom PyPDF2 import PdfReader\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nimport numpy as np\"),\n",
       "  0.30000000000000004),\n",
       " (Document(id='fe1c5e22b648e2079a30ef04f008ef28', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'extract_text_from_pdf', 'lines': '65-67'}, page_content='def extract_text_from_pdf(uploaded_file):\\n    reader = PdfReader(uploaded_file)\\n    return \"\\\\n\".join([page.extract_text() or \"\" for page in reader.pages])'),\n",
       "  0.25),\n",
       " (Document(id='4ec55f4ed0c9b44f532fe2eb05f8399d', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'extract_json_from_response', 'lines': '164-171'}, page_content='def extract_json_from_response(raw):\\n    match = re.search(r\"\\\\{[\\\\s\\\\S]*\\\\}\", raw)\\n    if match:\\n        try:\\n            return json.loads(match.group())\\n        except json.JSONDecodeError:\\n            return {}\\n    return {}'),\n",
       "  0.19999999999999996),\n",
       " (Document(id='19f8686e5f0f56f42b5e2f4fb4b8f072', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 6', 'lines': 'cell_6'}, page_content='mentors_final_data = pd.read_json(\"generated_mentors.json\")\\nmentors_final_data.set_index(\"mentor_id\", inplace=True)\\nmentors_final_data.head(5)'),\n",
       "  0.15000000000000002),\n",
       " (Document(id='a7f084aa210b4facb8c47d15b177ddcc', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 22', 'lines': 'cell_22'}, page_content='from google.colab import files\\n\\nuploaded = files.upload()\\npdf_path = list(uploaded.keys())[0]\\n\\ntarget_role = input(\"Enter your target role (e.g., \\'AI Engineer\\'): \").strip()\\n\\noutput = analyze_uploaded_resume(pdf_path, target_role)\\n\\nprint(json.dumps(output, indent=2))'),\n",
       "  0.09999999999999998),\n",
       " (Document(id='e589ea14f7fe6ef7ab0d9194ca24c809', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\app.py', 'file_ext': 'code', 'type': 'functiondef', 'name': 'retrieve_examples', 'lines': '82-86'}, page_content='def retrieve_examples(query, embeddings, examples, k=3):\\n    query_emb = get_embedding(query)\\n    sims = sk_cosine([query_emb], embeddings)[0]\\n    top_k = sims.argsort()[-k:][::-1]\\n    return [examples[i] for i in top_k]'),\n",
       "  0.050000000000000044)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3944f9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fast_search(\n",
    "    \"We need to write system Architecture?\",\n",
    "    route=\"both\",\n",
    "    k_text=5, k_code=15, k_final=20,\n",
    "    use_bm25=True\n",
    "    #types_include={\"functiondef\",\"classdef\",\"asyncfunctiondef\",\"functiondef_docstring\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a588907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(id='a39258abac5f898c0001fd1010c56bef', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Job_Description_JD_Manupulation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Job_Description_JD_Manupulation.ipynb - cell 0', 'lines': 'cell_0'}, page_content='<a href=\"https://colab.research.google.com/github/adarshlearnngrow/StepUp-AI/blob/main/Job_Description_JD_Manupulation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>'),\n",
       "  1.0),\n",
       " (Document(id='ac8860bec3d50a9c4affecc718108f32', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 14', 'lines': 'cell_14'}, page_content='### Extracting common required technical and soft skills required for each role.'),\n",
       "  0.8333333333333334),\n",
       " (Document(id='35aea59fd408783aa610f5b1035ff731', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\StepUpAI\\\\requirements.txt', 'file_ext': 'code', 'type': '.txt', 'name': 'requirements.txt', 'lines': '1-10'}, page_content='streamlit\\nopenai>=1.0.0\\nPyPDF2\\npandas\\nscikit-learn\\ntqdm\\nnumpy\\npandas\\njoblib\\n'),\n",
       "  0.6666666666666667),\n",
       " (Document(id='dd579da75ba82f0519339941006b8121', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 9', 'lines': 'cell_9'}, page_content='### Fitting a KMean model with range of cluster k and ran for 100 times to be certain.'),\n",
       "  0.5),\n",
       " (Document(id='5841abfbafb745c6404206baa01da2b0', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb', 'file_ext': 'code', 'type': 'markdown_cell', 'name': 'Skill_Gap_Analysis_and_Action_Plan_Generation.ipynb - cell 23', 'lines': 'cell_23'}, page_content='### Action Plan Generation (Testing for 3 Candidates)'),\n",
       "  0.33333333333333337),\n",
       " (Document(id='3d929bb5c81907be898484d2c19f5c6e', metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_uyycmlem\\\\ClusteringMentorModelTraining.ipynb', 'file_ext': 'code', 'type': 'code_cell', 'name': 'ClusteringMentorModelTraining.ipynb - cell 21', 'lines': 'cell_21'}, page_content='mentors_final_data[mentors_final_data[\"cluster\"] == prediction[0]][[\\'name\\', \\'bio\\',\\'linkedin_id\\', \\'technical_skills\\']]'),\n",
       "  0.16666666666666663)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = fast_search(\n",
    "    \"What Technologies are used in this code?\",\n",
    "    route=\"both\",\n",
    "    k_text=5, k_code=1, k_final=6,\n",
    "    use_bm25=True\n",
    "    #types_include={\"functiondef\",\"classdef\",\"asyncfunctiondef\",\"functiondef_docstring\"}\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8174efe0",
   "metadata": {},
   "source": [
    "## Fallback option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55285b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "\n",
    "# Initialize client once\n",
    "client = OpenAI()\n",
    "\n",
    "# 1Ô∏è‚É£ Define your fixed label list\n",
    "LAYOUT = [\n",
    "    \"Project Overview\",\n",
    "    \"Objective & Scope\",\n",
    "    \"System Architecture\",\n",
    "    \"Tech Stack\",\n",
    "    \"Installation & Setup\",\n",
    "    \"Usage Instructions\",\n",
    "    \"API Documentation\",\n",
    "    \"Others\"\n",
    "]\n",
    "\n",
    "# 2Ô∏è‚É£ Define the function schema for OpenAI function calling\n",
    "FUNCTIONS = [\n",
    "    {\n",
    "        \"name\": \"classify_chunk\",\n",
    "        \"description\": \"Assigns documentation section labels to a chunk.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sections\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": LAYOUT\n",
    "                    },\n",
    "                    \"minItems\": 1\n",
    "                },\n",
    "                \"rationale\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"1‚Äì3 sentence explanation of why those labels were chosen\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"sections\", \"rationale\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# 3Ô∏è‚É£ Few-shot examples (including a multi-label example)\n",
    "FEW_SHOT_EXAMPLES = [\n",
    "    {\n",
    "        \"code\": \"def main():\\n    load_config()\\n    train_model()\\n    save_model()\\n\",\n",
    "        \"label\": [\"System Architecture\", \"Usage Instructions\"],\n",
    "        \"rationale\": (\n",
    "            \"Defines the core training pipeline, showing both the \"\n",
    "            \"system‚Äôs structure and how to invoke it.\"\n",
    "        )\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"@app.get('/predict')\\ndef predict(user_id: int):\\n    return model.predict(user_id)\\n\",\n",
    "        \"label\": [\"API Documentation\", \"Usage Instructions\"],\n",
    "        \"rationale\": \"Defines an API endpoint and shows how to call it.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"# pip install -r requirements.txt\\n\",\n",
    "        \"label\": [\"Installation & Setup\"],\n",
    "        \"rationale\": \"Provides installation instructions via pip.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": '\"\"\"This project automates resume analysis using GPT.\"\"\"',\n",
    "        \"label\": [\"Project Overview\"],\n",
    "        \"rationale\": \"Gives a concise summary of the project‚Äôs purpose.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"class ServiceRegistry:\\n    \\\"\\\"\\\"Manages service instances and their lifecycles\\\"\\\"\\\"\\n    def __init__(self):\\n        self._services = {}\\n\\n    def register(self, name, svc):\\n        self._services[name] = svc\\n\\n    def get(self, name):\\n        return self._services[name]\\n\",\n",
    "        \"label\": [\"System Architecture\"],\n",
    "        \"rationale\": \"Encapsulates the core registry for services, illustrating a central architectural component.\"\n",
    "    },\n",
    "    {\n",
    "        \"code\": \"from queue import Queue\\n\\npipeline = Queue()\\n\\ndef enqueue_task(task):\\n    pipeline.put(task)\\n\\ndef process_pipeline():\\n    while not pipeline.empty():\\n        task = pipeline.get()\\n        handle(task)\\n\",\n",
    "        \"label\": [\"System Architecture\"],\n",
    "        \"rationale\": \"Implements the main task queue and processing loop, key parts of the system‚Äôs internal architecture.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "def build_system_prompt() -> str:\n",
    "    layout_list = \"\\n\".join(f\"- {lbl}\" for lbl in LAYOUT)\n",
    "    examples = \"\\n\\n\".join(\n",
    "        f\"EXAMPLE CHUNK:\\n{ex['code'].strip()}\\n\"\n",
    "        f\"EXPECTED OUTPUT:\\n\"\n",
    "        f\"{{\\\"sections\\\": {ex['label']!r}, \\\"rationale\\\": {ex['rationale']!r}}}\"\n",
    "        for ex in FEW_SHOT_EXAMPLES\n",
    "    )\n",
    "\n",
    "    return f\"\"\"\n",
    "You are an expert technical writer. Classify each code or markdown chunk into one or more documentation sections.\n",
    "\n",
    "Valid labels (pick all that apply):\n",
    "{layout_list}\n",
    "\n",
    "‚ùå Do NOT invent new labels.\n",
    "‚úÖ You may assign multiple labels‚Äîreturn *all* that apply.\n",
    "\n",
    "Return your answer by calling the function `classify_chunk` with valid JSON.\n",
    "\n",
    "Here are a few guiding examples:\n",
    "{examples}\n",
    "\n",
    "Now classify the next chunk.\n",
    "\"\"\".strip()\n",
    "\n",
    "def multi_label_chunk(text: str, model: str = \"gpt-3.5-turbo\") -> dict:\n",
    "    \"\"\"\n",
    "    Classify a code or markdown chunk into documentation sections.\n",
    "    Returns: {\"sections\": [...], \"rationale\": \"...\"}\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": build_system_prompt()},\n",
    "                {\"role\": \"user\",   \"content\": text.strip()}\n",
    "            ],\n",
    "            functions=FUNCTIONS,\n",
    "            function_call={\"name\": \"classify_chunk\"},\n",
    "            temperature=0.2,\n",
    "            max_tokens=300\n",
    "        )\n",
    "\n",
    "        # Extract the function_call result (attribute access, not dict)\n",
    "        call = resp.choices[0].message.function_call\n",
    "        return json.loads(call.arguments)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è classification failed:\", e)\n",
    "        return {\n",
    "            \"sections\": [\"Others\"],\n",
    "            \"rationale\": \"Fallback due to error\"\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d3c6972",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_chunks = []\n",
    "\n",
    "for doc in chunks[-10:-1]:\n",
    "    label = multi_label_chunk(doc.page_content)\n",
    "\n",
    "    if not label or not isinstance(label, dict):\n",
    "        label = {\n",
    "            \"sections\": [\"Others\"],\n",
    "            \"rationale\": \"Fallback ‚Äì no label returned\",\n",
    "            \"tags\": []\n",
    "        }\n",
    "\n",
    "    doc.metadata[\"sections\"] = label.get(\"sections\", [\"Others\"])\n",
    "    doc.metadata[\"tags\"] = label.get(\"tags\", [])\n",
    "    doc.metadata[\"rationale\"] = label.get(\"rationale\", \"No rationale given\")\n",
    "\n",
    "    labelled_chunks.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7a695899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_pfs6yiy6\\\\blackjack\\\\models\\\\shoe.py', 'file_ext': 'code', 'type': 'classdef', 'name': 'Shoe', 'lines': '6-44', 'sections': ['System Architecture', 'Usage Instructions'], 'tags': [], 'rationale': \"Illustrates the internal structure of the 'Shoe' class which manages decks of cards and deals them. Additionally, it provides methods for using the class to deal cards in a card game.\"}, page_content='class Shoe:\\n\\n    def __init__(self, num_decks):\\n        # Create the decks of cards.\\n        self.decks = [Deck() for _ in range(num_decks)]\\n\\n        # List to hold the cards to be dealt, in the order in which they\\'ll be dealt.\\n        self.card_pile = []\\n\\n        # Initialize with a shuffled card pile so the shoe is ready to be played.\\n        self.reset_card_pile()\\n\\n    def cards(self):\\n        \"\"\"Get all cards belonging to the shoe (through decks).\"\"\"\\n        cards = []\\n        for deck in self.decks:\\n            for card in deck.cards:\\n                cards.append(card)\\n        return cards\\n\\n    def shuffled_cards(self):\\n        \"\"\"Shuffle the shoe\\'s cards.\"\"\"\\n        cards = self.cards()\\n        random.shuffle(cards)\\n        return cards\\n\\n    def reset_card_pile(self):\\n        \"\"\"Reset the shoe\\'s card pile.\"\"\"\\n        self.card_pile = self.shuffled_cards()\\n\\n    def deal_card(self):\\n        \"\"\"Deal a card from the shoe (reshuffle if pile exhausted).\"\"\"\\n        if not self.card_pile:\\n            self.reset_card_pile()\\n        return self.card_pile.pop()\\n\\n    def deal_n_cards(self, num_cards):\\n        \"\"\"Deal a set number of cards from the shoe.\"\"\"\\n        return [self.deal_card() for _ in range(num_cards)]'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_pfs6yiy6\\\\blackjack\\\\strategies\\\\base_static_strategy.py', 'file_ext': 'code', 'type': 'classdef', 'name': 'BaseStaticStrategy', 'lines': '11-51', 'sections': ['System Architecture'], 'tags': [], 'rationale': 'Defines a base class for static strategies in a card game system, outlining methods for loading dataframes and determining actions based on hand and dealer information.'}, page_content='class BaseStaticStrategy(BaseStrategy):\\n    \"\"\"\\n    Base predetermined Strategy from which other static Strategies can be derrived.\\n    Note that concrete static Strategies must implement the required BaseStrategy methods omitted here.\\n    \"\"\"\\n\\n    def __init__(self, strategy_name):\\n        super().__init__()\\n        self.split_df = self._load_df(strategy_name, \\'split\\')\\n        self.soft_df = self._load_df(strategy_name, \\'soft\\')\\n        self.hard_df = self._load_df(strategy_name, \\'hard\\')\\n\\n    @staticmethod\\n    def _load_df(strategy_name, csv_type):\\n        \"\"\"Load a DataFrame from a CSV for determining actions.\"\"\"\\n        csv_path = f\"{DIRECTORY}/csv/{strategy_name}/{csv_type}.csv\"\\n        return read_csv(csv_path, index_col=0)\\n\\n    def get_hand_action(self, hand, options, dealer_upcard):\\n        \"\"\"Get the action to take on the hand (\\'Hit\\', \\'Stand\\', etc.)\"\"\"\\n        # Get the dealer value by which to look up the correct action\\n        column = dealer_upcard.csv_format()\\n\\n        # If splitting is an option, check if that action should be taken first.\\n        if \\'Split\\' in options.values():\\n            row = hand.cards[0].csv_format()\\n            if self.split_df.at[row, column] == \\'Yes\\':\\n                return \\'Split\\'\\n\\n        # Use the appropriate \\'soft\\' or \\'hard\\' hand DataFrame to decide which action should be taken.\\n        row = hand.final_total()\\n        if hand.is_soft():\\n            action = self.soft_df.at[row, column]\\n        else:\\n            action = self.hard_df.at[row, column]\\n\\n        # Handle the edge case where doubling is the recommended action, but the user doesn\\'t have enough money to do so.\\n        if action == \\'Double\\' and \\'Double\\' not in options.values():\\n            return \\'Hit\\'\\n        \\n        return action'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_pfs6yiy6\\\\blackjack\\\\strategies\\\\base_static_strategy.py', 'file_ext': 'code', 'type': 'classdef_docstring', 'name': 'BaseStaticStrategy', 'lines': '11-51', 'sections': ['System Architecture'], 'tags': [], 'rationale': 'Defines a base strategy that serves as a blueprint for deriving other static strategies. It outlines the core methods that concrete static strategies must implement.'}, page_content='Base predetermined Strategy from which other static Strategies can be derrived.\\nNote that concrete static Strategies must implement the required BaseStrategy methods omitted here.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_pfs6yiy6\\\\blackjack\\\\strategies\\\\base_strategy.py', 'file_ext': 'code', 'type': 'classdef', 'name': 'BaseStrategy', 'lines': '4-25', 'sections': ['System Architecture'], 'tags': [], 'rationale': \"Defines an abstract base class that outlines the methods required to be implemented by all Strategies for in-game decisions, illustrating a key component of the system's architecture.\"}, page_content='class BaseStrategy(ABC):\\n    \"\"\"Abstract base class that lays out the methods that must be implemented by all Strategies for in-game decisions.\"\"\"\\n\\n    @abstractmethod\\n    def wants_to_change_wager(self):\\n        \"\"\"Get a yes/no response (bool) for whether the gambler wants to change their auto-wager.\"\"\"\\n\\n    @abstractmethod\\n    def get_new_auto_wager(self):\\n        \"\"\"Get a new auto-wager amount (float).\"\"\"\\n\\n    @abstractmethod\\n    def get_hand_action(self, hand, options, dealer_upcard):\\n        \"\"\"Get the action to take on the hand (\\'Hit\\', \\'Stand\\', etc.)\"\"\"\\n\\n    @abstractmethod\\n    def wants_even_money(self):\\n        \"\"\"Get a yes/no response (bool) for whether a the gambler wants to take even money for a blackjack when facing an Ace.\"\"\"\\n\\n    @abstractmethod\\n    def wants_insurance(self):\\n        \"\"\"Get a yes/no response (bool) for whether a user wants to make an insurance bet when facing an Ace.\"\"\"'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_pfs6yiy6\\\\blackjack\\\\strategies\\\\base_strategy.py', 'file_ext': 'code', 'type': 'classdef_docstring', 'name': 'BaseStrategy', 'lines': '4-25', 'sections': ['System Architecture'], 'tags': [], 'rationale': \"Defines the core structure for implementing different strategies for in-game decisions, highlighting a key component of the system's architecture.\"}, page_content='Abstract base class that lays out the methods that must be implemented by all Strategies for in-game decisions.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_pfs6yiy6\\\\blackjack\\\\strategies\\\\default_static_strategy.py', 'file_ext': 'code', 'type': 'classdef', 'name': 'DefaultStaticStrategy', 'lines': '4-25', 'sections': ['System Architecture'], 'tags': [], 'rationale': 'Defines the default static strategy class for managing optimal odds in a gambling system.'}, page_content='class DefaultStaticStrategy(BaseStaticStrategy):\\n    \"\"\"Default StaticStrategy for optimal odds.\"\"\"\\n\\n    def __init__(self):\\n        super().__init__(strategy_name=\\'default\\')\\n\\n    def wants_to_change_wager(self):\\n        \"\"\"Get a yes/no response (bool) for whether the gambler wants to change their auto-wager.\"\"\"\\n        return False\\n\\n    def get_new_auto_wager(self):\\n        \"\"\"Get a new auto-wager amount (float).\"\"\"\\n        # Will not change the auto-wager in this strategy\\n        pass\\n\\n    def wants_even_money(self):\\n        \"\"\"Get a yes/no response (bool) for whether a the gambler wants to take even money for a blackjack when facing an Ace.\"\"\"\\n        return False\\n\\n    def wants_insurance(self):\\n        \"\"\"Get a yes/no response (bool) for whether a user wants to make an insurance bet when facing an Ace.\"\"\"\\n        return False'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_pfs6yiy6\\\\blackjack\\\\strategies\\\\insurance_static_strategy.py', 'file_ext': 'code', 'type': 'classdef', 'name': 'InsuranceStaticStrategy', 'lines': '4-9', 'sections': ['System Architecture'], 'tags': [], 'rationale': \"Defines a new class for a specific strategy in the system, indicating a key component of the system's architecture.\"}, page_content='class InsuranceStaticStrategy(DefaultStaticStrategy):\\n    \"\"\"Same as the optimal DefaultStaticStrategy except insurance is bought when facing a dealer Ace.\"\"\"\\n\\n    def wants_insurance(self):\\n        \"\"\"Get a yes/no response (bool) for whether a user wants to make an insurance bet when facing an Ace.\"\"\"\\n        return True'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_pfs6yiy6\\\\blackjack\\\\strategies\\\\insurance_static_strategy.py', 'file_ext': 'code', 'type': 'classdef_docstring', 'name': 'InsuranceStaticStrategy', 'lines': '4-9', 'sections': ['System Architecture'], 'tags': [], 'rationale': \"Describes a modification in the system's strategy for handling insurance when facing a dealer Ace, indicating a change in the system's behavior.\"}, page_content='Same as the optimal DefaultStaticStrategy except insurance is bought when facing a dealer Ace.'),\n",
       " Document(metadata={'source': 'C:\\\\Users\\\\adars\\\\AppData\\\\Local\\\\Temp\\\\repo_pfs6yiy6\\\\blackjack\\\\strategies\\\\user_input_strategy.py', 'file_ext': 'code', 'type': 'classdef', 'name': 'UserInputStrategy', 'lines': '7-57', 'sections': ['System Architecture', 'Usage Instructions'], 'tags': [], 'rationale': 'Defines a strategy class for making decisions via user input at a command line prompt. It includes methods for getting user input related to changing wager, getting new auto-wager amount, selecting hand actions, taking even money, and making insurance bets.'}, page_content='class UserInputStrategy(BaseStrategy):\\n    \"\"\"Strategy that makes decisions via user input at a command line prompt.\"\"\"\\n\\n    @staticmethod\\n    def wants_to_change_wager():\\n        \"\"\"Get a yes/no response (bool) for whether the gambler wants to change their auto-wager.\"\"\"\\n        # Ask if the gambler wants to cash out or change their auto-wager\\n        return get_user_input(\\n            \\'Change your auto-wager or cash out? (y/n) => \\',\\n            yes_no_response\\n        )\\n\\n    @staticmethod\\n    def get_new_auto_wager():\\n        \"\"\"Get a new auto-wager amount (float).\"\"\"\\n        return get_user_input(\\n            \\'Please enter an auto-wager amount (Enter $0 to cash out): $\\',\\n            float_response\\n        )\\n\\n    # NOTE: Dealer up card only relevant for strategy input (a real user can just see it)\\n    @staticmethod\\n    def get_hand_action(hand, options, dealer_upcard):\\n        \"\"\"\\n        Get the action to take on the hand (\\'Hit\\', \\'Stand\\', etc.).\\n        \\n        hand - GamblerHand instance\\n        options - OrderedDict of possible actions like: {\\'h\\': \\'Hit\\', \\'s\\': \\'Stand\\' ... }\\n        dealer_upcard - Card instance dealer is showing (applicable to other InputControllers)\\n        \"\"\"\\n        # Formatted options to display to the user\\n        display_options = [f\"{option} ({abbreviation})\" for abbreviation, option in options.items()]\\n\\n        # Ask what the user would like to do, given their options\\n        response = get_user_input(\\n            f\"[ Hand {hand.hand_number} ] What would you like to do? [ {\\' , \\'.join(display_options)} ] => \",\\n            partial(choice_response, choices=options.keys())\\n        )\\n\\n        # Return the user\\'s selection (\\'Hit\\', \\'Stand\\', etc.)\\n        return options[response]\\n\\n    @staticmethod\\n    def wants_even_money():\\n        \"\"\"Get a yes/no response (bool) for whether a user wants to take even money for a blackjack when facing an Ace.\"\"\"\\n        return get_user_input(\"Take even money? (y/n) => \", yes_no_response)\\n\\n    @staticmethod\\n    def wants_insurance():\\n        \"\"\"Get a yes/no response (bool) for whether a user wants to make an insurance bet when facing an Ace.\"\"\"\\n        return get_user_input(\"Insurance? (y/n) => \", yes_no_response)')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[-10:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Project Overview', 'Objective & Scope', 'Tech Stack']\n",
      "['Others']\n",
      "['Tech Stack', 'Installation & Setup']\n",
      "['Tech Stack', 'Usage Instructions']\n",
      "['Objective & Scope', 'Others']\n",
      "['Usage Instructions', 'Others']\n",
      "['Usage Instructions', 'Others']\n",
      "['Usage Instructions', 'Others']\n",
      "['Usage Instructions', 'System Architecture']\n",
      "['Others']\n",
      "['Usage Instructions', 'Installation & Setup']\n",
      "['Tech Stack', 'Others']\n",
      "['System Architecture', 'Usage Instructions']\n",
      "['Objective & Scope', 'System Architecture']\n",
      "['Usage Instructions', 'Others']\n",
      "['Usage Instructions', 'API Documentation']\n",
      "['Others']\n",
      "['Others']\n",
      "['Usage Instructions']\n",
      "['Usage Instructions']\n",
      "['Others']\n",
      "['Tech Stack']\n",
      "['Usage Instructions']\n",
      "['Others']\n",
      "['Others']\n",
      "['Usage Instructions']\n",
      "['Others']\n",
      "['Installation & Setup', 'Tech Stack']\n",
      "['Tech Stack', 'Usage Instructions']\n",
      "['Others']\n",
      "['Usage Instructions', 'API Documentation', 'Others']\n",
      "['Installation & Setup', 'Others']\n",
      "['Usage Instructions', 'Others']\n",
      "['Objective & Scope']\n",
      "['Objective & Scope', 'Others']\n",
      "['System Architecture', 'Usage Instructions']\n",
      "['Others']\n",
      "['Others']\n",
      "['Objective & Scope', 'Others']\n",
      "['Usage Instructions', 'Others']\n",
      "['Objective & Scope', 'Usage Instructions']\n",
      "['System Architecture', 'Usage Instructions', 'API Documentation']\n",
      "['Usage Instructions', 'Others']\n",
      "['Objective & Scope']\n",
      "['Usage Instructions', 'API Documentation', 'Others']\n",
      "['Usage Instructions']\n",
      "['Usage Instructions', 'Others']\n",
      "['Installation & Setup', 'Others']\n",
      "['API Documentation']\n",
      "['Usage Instructions', 'Tech Stack']\n",
      "['API Documentation']\n",
      "['Usage Instructions', 'Tech Stack']\n",
      "['Usage Instructions', 'API Documentation']\n",
      "['API Documentation']\n",
      "['System Architecture', 'Tech Stack']\n",
      "['Installation & Setup', 'Others']\n",
      "['Objective & Scope', 'Tech Stack']\n",
      "['Usage Instructions', 'Others']\n",
      "['Installation & Setup']\n"
     ]
    }
   ],
   "source": [
    "for doc in labelled_chunks:\n",
    "    print(doc.metadata[\"sections\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbcad65",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01f6aae",
   "metadata": {},
   "source": [
    "# Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdcfd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: docs/system_architecture.md\n"
     ]
    }
   ],
   "source": [
    "# arch_graph_min.py\n",
    "from pathlib import Path\n",
    "from typing import TypedDict\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "BASE_DIR    = \"docs_index\"                 # where text_index/ lives\n",
    "EMBED_MODEL = \"text-embedding-3-small\"     # must match your FAISS build\n",
    "GEN_MODEL   = \"gpt-4o-mini\"\n",
    "\n",
    "class State(TypedDict, total=False):\n",
    "    query: str           # what we‚Äôre asking retrieval\n",
    "    context: str         # formatted snippets to show the writer\n",
    "    draft: str           # LLM‚Äôs markdown output\n",
    "    out_path: str        # where we saved the section\n",
    "\n",
    "def retrieve_node(state: State) -> State:\n",
    "    \"\"\"Pull diverse text snippets about architecture from FAISS.\"\"\"\n",
    "    emb = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "    vs  = FAISS.load_local(f\"{BASE_DIR}/text_index\", emb, allow_dangerous_deserialization=True)\n",
    "    hits = vs.max_marginal_relevance_search(state[\"query\"], k=12, fetch_k=48)\n",
    "\n",
    "    parts = []\n",
    "    for d in hits:\n",
    "        src = Path(d.metadata.get(\"source\",\"\")).name\n",
    "        loc = d.metadata.get(\"lines\",\"\")\n",
    "        snippet = d.page_content[:1200]\n",
    "        parts.append(f\"[{src}:{loc}] {snippet}\")\n",
    "    return {\"context\": \"\\n\\n\".join(parts)}\n",
    "\n",
    "def write_node(state: State) -> State:\n",
    "    \"\"\"Ask the model to write the section using ONLY retrieved context.\"\"\"\n",
    "    llm = ChatOpenAI(model=GEN_MODEL, temperature=0.2)\n",
    "    msgs = [\n",
    "        {\"role\":\"system\",\"content\":\"You are a senior technical writer. Use ONLY the provided context. If unknown, say so.\"},\n",
    "        {\"role\":\"user\",\"content\": f\"Write the 'System Architecture' section in Markdown.\\n\\nCONTEXT:\\n{state['context']}\"}\n",
    "    ]\n",
    "    md = llm.invoke(msgs).content.strip()\n",
    "    return {\"draft\": md}\n",
    "\n",
    "def finish_node(state: State) -> State:\n",
    "    section = state.get(\"section_name\", \"System Architecture\")\n",
    "    out = f\"# {section}\\n\\n\" + state[\"draft\"] + \"\\n\"\n",
    "    Path(\"docs\").mkdir(exist_ok=True)\n",
    "    # make a reasonable filename from section name\n",
    "    fname = section.lower().replace(\" \", \"_\") + \".md\"\n",
    "    Path(f\"docs/{fname}\").write_text(out, encoding=\"utf-8\")\n",
    "    return {\"out_path\": f\"docs/{fname}\"}\n",
    "\n",
    "def build_graph():\n",
    "    g = StateGraph(State)\n",
    "    g.add_node(\"retrieve\", retrieve_node)\n",
    "    g.add_node(\"write\", write_node)\n",
    "    g.add_node(\"finish\", finish_node)\n",
    "\n",
    "    g.set_entry_point(\"retrieve\")\n",
    "    g.add_edge(\"retrieve\", \"write\")\n",
    "    g.add_edge(\"write\", \"finish\")\n",
    "    return g.compile()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = build_graph()\n",
    "    final = app.invoke({\n",
    "        \"section_name\": \"Overview\",\n",
    "        \"query\": (\n",
    "            \"High-level purpose of the project, key features, main modules/components, \"\n",
    "            \"primary workflows, who the project is for, and the benefits.\"\n",
    "        )\n",
    "        # (optional) you can also pass \"route\": \"both\", \"k_text\": 12, \"k_code\": 8\n",
    "        # if you already added those in a previous step\n",
    "    })\n",
    "    print(\"Wrote:\", final[\"out_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "126a4bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: docs/system_architecture.md\n"
     ]
    }
   ],
   "source": [
    "app = build_graph()\n",
    "final = app.invoke({\n",
    "    \"section_name\": \"Overview\",\n",
    "    \"query\": (\n",
    "        \"High-level purpose of the project, key features, main modules/components, \"\n",
    "        \"primary workflows, who the project is for, and the benefits.\"\n",
    "    )\n",
    "    # (optional) you can also pass \"route\": \"both\", \"k_text\": 12, \"k_code\": 8\n",
    "    # if you already added those in a previous step\n",
    "})\n",
    "print(\"Wrote:\", final[\"out_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "851ae9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rules = (\n",
    "    \"Allowed sources:\\n\"\n",
    "    \"1) Retrieved repository content.\\n\"\n",
    "    \"2) Logical inferences ONLY if repo lacks it.\\n\"\n",
    "    \"3) Optional contextual/literature knowledge (must be marked).\\n\\n\"\n",
    "    \"Citation rules (MUST end every sentence with exactly one):\\n\"\n",
    "    \"- Summarising/rephrasing repo info ‚Üí [the <file_name>:<line_number>]\\n\"\n",
    "    \"- New detail logically deduced ‚Üí (Inferred from LLM based on repository content) \"\n",
    "    \"[the <file_name>:<line_number>, or multiple file names comma-separated]\\n\"\n",
    "    \"- External/contextual knowledge ‚Üí (Included from contextual knowledge, not from repository)\\n\"\n",
    "    \"- Missing info ‚Üí (Information not available in repository)\\n\\n\"\n",
    "    \"Important:\\n\"\n",
    "    \"- Do NOT mark summarised repo content as inferred.\\n\"\n",
    "    \"- Use ONLY base filenames present in CONTEXT (no directories).\\n\"\n",
    "    \"- No punctuation after the closing bracket/parenthesis.\\n\"\n",
    "    \"- Mermaid code blocks are allowed without tags inside; add a one-line caption after the block with a proper tag.\\n\"\n",
    "    \"- Be concise and factual.\"\n",
    ")\n",
    "\n",
    "examples = (\n",
    "    \"Examples:\\n\"\n",
    "    \"The project exposes a command-line interface for simulations [the README.md:1-80]\\n\"\n",
    "    \"The GameController mediates between the Dealer and the Gambler \"\n",
    "    \"(Inferred from LLM based on repository content) [the game_controller.py, configuration.py]\\n\"\n",
    "    \"Integration with external monitoring is not described (Information not available in repository)\\n\"\n",
    "    \"FastAPI is a popular Python web framework (Included from contextual knowledge, not from repository)\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bef10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# section_runner.py\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Literal, TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "BASE_DIR    = \"docs_index\"                 # text_index/ and code_index/ live here\n",
    "EMBED_MODEL = \"text-embedding-3-small\"     # must match what you built FAISS with\n",
    "GEN_MODEL   = \"gpt-4o-mini\"                # your chat model\n",
    "\n",
    "@dataclass\n",
    "class SectionSpec:\n",
    "    name: str\n",
    "    query: str\n",
    "    route: Literal[\"text\", \"both\"] = \"text\"   # \"text\" or \"both (text+code)\"\n",
    "    k_text: int = 5\n",
    "    k_code: int = 15\n",
    "    guidance: str = \"\"                        # optional writing hints\n",
    "    additional_context: str = \"\"              # optional additional context\n",
    "\n",
    "class State(TypedDict, total=False):\n",
    "    spec: SectionSpec\n",
    "    context: str\n",
    "    draft: str\n",
    "    out_path: str\n",
    "\n",
    "def _retrieve(spec: SectionSpec) -> str:\n",
    "    \"\"\"Simple, per-section retrieval with optional code.\"\"\"\n",
    "    emb = OpenAIEmbeddings(model=EMBED_MODEL)\n",
    "    parts: List[str] = []\n",
    "\n",
    "    # text index (always)\n",
    "    text_vs = FAISS.load_local(f\"{BASE_DIR}/text_index\", emb, allow_dangerous_deserialization=True)\n",
    "    t_hits = text_vs.max_marginal_relevance_search(spec.query, k=spec.k_text, fetch_k=min(64, 4*spec.k_text))\n",
    "    for d in t_hits:\n",
    "        src = Path(d.metadata.get(\"source\",\"\")).name\n",
    "        loc = d.metadata.get(\"lines\",\"\")\n",
    "        parts.append(f\"[{src}:{loc}] {d.page_content[:1200]}\")\n",
    "\n",
    "    # code index (optional)\n",
    "    if spec.route == \"both\":\n",
    "        try:\n",
    "            code_vs = FAISS.load_local(f\"{BASE_DIR}/code_index\", emb, allow_dangerous_deserialization=True)\n",
    "            c_hits = code_vs.max_marginal_relevance_search(spec.query, k=spec.k_code, fetch_k=min(64, 4*spec.k_code))\n",
    "            # de-dup by first 64 chars\n",
    "            seen = {p[:64] for p in parts}\n",
    "            for d in c_hits:\n",
    "                snippet = d.page_content[:1200]\n",
    "                if snippet[:64] in seen: \n",
    "                    continue\n",
    "                seen.add(snippet[:64])\n",
    "                src = Path(d.metadata.get(\"source\",\"\")).name\n",
    "                loc = d.metadata.get(\"lines\",\"\")\n",
    "                parts.append(f\"[{src}:{loc}] {snippet}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return \"\\n\\n\".join(parts)\n",
    "\n",
    "# ----- LangGraph nodes -----\n",
    "def n_retrieve(state: State) -> State:\n",
    "    ctx = _retrieve(state[\"spec\"])\n",
    "    Path(\"debug\").mkdir(exist_ok=True)\n",
    "    Path(\"debug/context.txt\").write_text(ctx, encoding=\"utf-8\")\n",
    "    return {\"context\": ctx}\n",
    "\n",
    "def n_write(state: State) -> State:\n",
    "    spec = state[\"spec\"]\n",
    "    \n",
    "    llm = ChatOpenAI(model=GEN_MODEL, temperature=0)\n",
    "    sys = (\n",
    "        \"You are a senior technical writer. Use ONLY the provided CONTEXT block (and additional_context if present) \"\n",
    "        \"and follow the citation rules exactly.\"\n",
    "    )\n",
    "\n",
    "    usr = (\n",
    "        f\"SECTION: {spec.name}\\n\"\n",
    "        f\"GOAL: {spec.query}\\n\"\n",
    "        f\"GUIDANCE: {spec.guidance}\\n\\n\"\n",
    "        f\"RULES:\\n{rules}\\n\\n{examples}\\n\"\n",
    "        f\"CONTEXT:\\n{state['context']}\\n\\n\"\n",
    "        \"CONTEXT (snippets already include tags like [the <file>:<lines>] so you can reuse filenames):\\n\"\n",
    "        f\"{state['context']}\\n\\n\"\n",
    "        f\"ADDITIONAL CONTEXT:\\n{getattr(spec, 'additional_context', '')}\\n\\n\"\n",
    "        f\"Write the '{spec.name}' section in clear Markdown.\"\n",
    "    )\n",
    "    md = llm.invoke([{\"role\":\"system\",\"content\":sys},{\"role\":\"user\",\"content\":usr}]).content.strip()\n",
    "    return {\"draft\": md}\n",
    "\n",
    "def n_save(state: State) -> State:\n",
    "    spec = state[\"spec\"]\n",
    "    out = f\"# {spec.name}\\n\\n{state['draft']}\\n\"\n",
    "    Path(\"docs\").mkdir(exist_ok=True)\n",
    "    fname = spec.name.lower().replace(\"&\",\"and\").replace(\" \", \"_\") + \".md\"\n",
    "    path = f\"docs/{fname}\"\n",
    "    Path(path).write_text(out, encoding=\"utf-8\")\n",
    "    return {\"out_path\": path}\n",
    "\n",
    "def build_graph():\n",
    "    g = StateGraph(State)\n",
    "    g.add_node(\"retrieve\", n_retrieve)\n",
    "    g.add_node(\"write\", n_write)\n",
    "\n",
    "    g.add_node(\"save\", n_save)\n",
    "    g.set_entry_point(\"retrieve\")\n",
    "    #g.add_edge(\"retrieve\", END)\n",
    "    g.add_edge(\"retrieve\", \"write\")\n",
    "    g.add_edge(\"write\", \"save\")\n",
    "    return g.compile()\n",
    "\n",
    "# ----- Run one section (example) -----\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a401a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: docs/overview.md\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = build_graph()\n",
    "\n",
    "    \"\"\"\n",
    "    # Example: Overview (text only)\n",
    "    overview = SectionSpec(\n",
    "        name=\"Overview\",\n",
    "        query=\"High-level purpose, business value, scope, key features, main modules/components, audience and benefits.\",\n",
    "        route=\"both\",\n",
    "        k_text=5,\n",
    "        guidance=\"2‚Äì3 short paragraphs; avoid code-level details.\"\n",
    "    )\n",
    "   \"\"\"\n",
    "    # Example: Objective & Scope (text only, your layout)\n",
    "    obj_scope = SectionSpec(\n",
    "        name=\"Objective & Scope\",\n",
    "        query=\"Project goals/objectives and scope or limitations as described in README and docstrings.\",\n",
    "        route=\"both\",\n",
    "        k_text=12,\n",
    "        guidance=\"Include '### Goals' bullets and '### Out of Scope' bullets.\"\n",
    "    )\n",
    "\n",
    "    print(\"Wrote:\", app.invoke({\"spec\": overview})[\"out_path\"])\n",
    "    # Uncomment to run this one too:\n",
    "    # print(\"Wrote:\", app.invoke({\"spec\": obj_scope})[\"out_path\"])\n",
    "    \n",
    "    # Uncomment to run this one too:\n",
    "    # print(\"Wrote:\", app.invoke({\"spec\": obj_scope})[\"out_path\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "30068903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: docs/system_architecture.md\n"
     ]
    }
   ],
   "source": [
    "app = build_graph()\n",
    "architecture = SectionSpec(\n",
    "        name=\"System Architecture\",\n",
    "        query=\"system architecture diagram, architecture overview, components, workflow, requirement.txt\",\n",
    "        route=\"both\",\n",
    "        k_text=10,\n",
    "        k_code=25,\n",
    "        guidance=\"Focus on the bigger as well as smaller picture.\",\n",
    "        additional_context=\"\"\"\"\n",
    "You are helping write the **System Architecture** section for a technical project.\n",
    "\n",
    "You will be given:\n",
    "- Project name & brief description ‚Äî What the system does.\n",
    "- Key goals ‚Äî What it is designed to achieve.\n",
    "- Key technologies ‚Äî Languages, frameworks, tools, services.\n",
    "- Any special constraints ‚Äî e.g., latency, security, budget.\n",
    "- Retrieved repository content tagged as architecture, diagrams, component descriptions, configuration files, and tech stack details, requirement.txt.\n",
    "\n",
    "Output format (only include what is available or inferred):\n",
    "\n",
    "1. **System Architecture Diagram (Mermaid)**\n",
    "   - Use `flowchart TD` or `graph LR`.\n",
    "   - Include the flow of the main applicaiton.\n",
    "   - Mark the title as Infered from the code.\n",
    "   - Mark missing elements as (Information not available in repository).\n",
    "\n",
    "2. **Key Components Table**\n",
    "   - Columns: Component | Responsibility | Tech/Options | Notes\n",
    "   - Include component which are acutally used, not everything.\n",
    "   - Only what is in the repo or inferred. Just take this from requirement.txt. Don't assume anything.\n",
    "\n",
    "3. **Data Flow Steps**\n",
    "   - Numbered list from input ‚Üí processing ‚Üí output.\n",
    "   - Mark missing or inferred steps clearly per rules.\n",
    "\n",
    "4. **Deployment View**\n",
    "   - Tell the entire thing for eg. Local dev setup, staging, production topology etc.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "5. **Scalability & Reliability**\n",
    "   - Only repo data or clearly marked inference.\n",
    "\n",
    "6. **Security & Compliance**\n",
    "   - Authentication, authorization, data protection, logging.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "7. **Trade-offs & Alternatives**\n",
    "   - Key design choices with pros/cons.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "8. **Assumptions & Constraints**\n",
    "   - Supported use cases, limits, boundaries.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "9. **Risks & Mitigations**\n",
    "   - Technical and operational risks with prevention/recovery strategies.\n",
    "   - Mark inferred items clearly per rules.\n",
    "\n",
    "10. **Observability & Quality**\n",
    "    - Metrics, tracing, alerts, testing approach.\n",
    "\n",
    "11. **Future Extensions**\n",
    "    - Possible evolutions, integrations, optimizations.\n",
    "    - Mark inferred items clearly per rules.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Wrote:\", app.invoke({\"spec\": architecture})[\"out_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0081b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: docs/technologies_used.md\n"
     ]
    }
   ],
   "source": [
    "app = build_graph()\n",
    "technologies = SectionSpec(\n",
    "        name=\"Technologies Used\",\n",
    "        query=\"Installation prerequisites and versions\",\n",
    "        route=\"both\",\n",
    "        k_text=5,\n",
    "        k_code=5,\n",
    "        guidance=\"\"\"\n",
    "        Just list the technologies used in a way like\n",
    "        Languages: Python, JavaScript\n",
    "        Frameworks: Flask, React\n",
    "        Packages: NumPy, Pandas\n",
    "        \"\"\",\n",
    "        additional_context=\"\"\n",
    ")\n",
    "\n",
    "#print(\"Retreive\", app.invoke({\"spec\": technologies}, print_mode=\"debug\"))\n",
    "print(\"Wrote:\", app.invoke({\"spec\": technologies})[\"out_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e6a2ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auto_doc_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
